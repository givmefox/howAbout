{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import isodate\n",
    "import emoji\n",
    "from kiwipiepy import Kiwi\n",
    "from kiwipiepy.utils import Stopwords\n",
    "from krwordrank.word import KRWordRank\n",
    "from krwordrank.word import summarize_with_keywords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 환경 변수 및 변수 설정\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)  \n",
    "\n",
    "CATEGORIES = {\n",
    "    \"News & Politics\": \"25\",\n",
    "    'Music' : \"10\",\n",
    "    'Sports' : \"17\",\n",
    "    'Gaming' : \"20\",\n",
    "    'Science & Technology': \"28\"\n",
    "}\n",
    "# JSON 데이터 저장 함수\n",
    "def save_to_json(data, filename):\n",
    "    # data 폴더가 존재하지 않으면 생성\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    \n",
    "    # 파일 경로를 data 폴더 아래로 설정\n",
    "    filepath = os.path.join('data', filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    print(f\"save_to_json : 데이터 저장: {filepath}\")\n",
    "    \n",
    "    # JSON 데이터 로드 함수\n",
    "# JSON 파일을 읽어서 딕셔너리로 반환하는 함수\n",
    "def load_json(filename):\n",
    "    # 파일 경로를 data 폴더 아래로 설정\n",
    "    filepath = os.path.join('data', filename)\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "# value가 문자열배열의 배열일 때 한글자와 빈 문자열을 제외하고 문자열로 만드는 함수\n",
    "def merge_values(data):\n",
    "    \"\"\"\n",
    "    key: value에서 value가 문자열 배열의 배열일 때,\n",
    "    하나의 문자열로 변환하고 빈 문자열(\"\") 및 한 글자 단어는 제외하는 함수.\n",
    "\n",
    "    :param data: dict, key: list of lists (e.g., { \"key1\": [[\"문장1\", \"문장2\"], [\"문장3\"]] })\n",
    "    :return: dict, key: merged string (e.g., { \"key1\": \"문장1 문장2 문장3\" })\n",
    "    \"\"\"\n",
    "    merged_data = merge_values\n",
    "    for key, values in data.items():\n",
    "        if isinstance(values, list):  # 값이 리스트인지 확인\n",
    "            merged_sentence = \" \".join(\n",
    "                s for sublist in values for s in sublist if s.strip() and len(s.strip()) > 1\n",
    "            )  # 빈 문자열 & 한 글자 제외 후 공백으로 연결\n",
    "            merged_data[key] = merged_sentence  # key: 병합된 문자열 형태로 저장\n",
    "\n",
    "    return merged_data\n",
    "# title과 tags를 하나의 문장으로 합치는 함수\n",
    "def merge_clean_title_tags(data):\n",
    "    \"\"\"\n",
    "    딕셔너리에서 'title'과 'tags'를 하나의 문장으로 변환하는 함수.\n",
    "    \n",
    "    :param data: dict (유튜브 비디오 정보)\n",
    "    :return: dict (title과 tags가 합쳐진 문장)\n",
    "    \"\"\"\n",
    "    merged_data = {}\n",
    "    for category, videos in data.items():\n",
    "        merged_data[category] = []  # 카테고리별 리스트 생성\n",
    "        for video in videos:\n",
    "            title = video.get(\"title\", \"\").strip()  # title이 없으면 빈 문자열\n",
    "            tags = video.get(\"tags\", [])  # tags가 없거나 None이면 빈 리스트로 처리\n",
    "            merged = \" \".join([title] + tags)  # title과 tags 결합\n",
    "            cleaned_merged = clean_text(merged)  # 텍스트 전처리\n",
    "            merged_data[category].append(cleaned_merged)\n",
    "    \n",
    "    return merged_data\n",
    "# 텍스트 데이터를 전처리하는 함수\n",
    "def clean_text(text):\n",
    "    # HTML 태그 제거\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # &quot 제거\n",
    "    text = text.replace(\"&quot\", \"\")\n",
    "    text = text.replace(\"&lt\", \"\")\n",
    "    text = text.replace(\"&gt\", \"\")\n",
    "    # URL, 멘션, 해시태그 제거\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\S+|#\\S+', '', text)\n",
    "    # 한글, 영어, 숫자, 공백을 제외한 모든 문자 제거\n",
    "    text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", text)\n",
    "    # 앞뒤 공백 제거\n",
    "    text = text.strip()\n",
    "    # 중복 공백 제거\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    # 모든 이모지 제거\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    # 영어를 소문자로 변환\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"윤대통령|윤대통|윤통|윤\", \"윤석열\", text)\n",
    "    text = re.sub(r\"장원\", \"홍장원\", text)\n",
    "    text = re.sub(r\"계엄령\", \"계엄\", text)\n",
    "\n",
    "    return text\n",
    "# YOUTUBE API를 통해 동영상 데이터 가져오기\n",
    "def fetch_trending_videos(category_id, region_code=\"KR\", max_results=200):\n",
    "    \n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(videos) < max_results:\n",
    "        try:\n",
    "            request = youtube.videos().list(\n",
    "                part=\"snippet,statistics,contentDetails\",\n",
    "                chart=\"mostPopular\",\n",
    "                regionCode=region_code,\n",
    "                videoCategoryId=category_id,\n",
    "                maxResults=min(50, max_results - len(videos)),\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                duration = isodate.parse_duration(item[\"contentDetails\"][\"duration\"])\n",
    "                duration_in_seconds = duration.total_seconds()  #초로 바꾸기기\n",
    "\n",
    "                if duration_in_seconds > 80:  # 80초 이상의 동영상만 가져오기\n",
    "                    videos.append({\n",
    "                        \"video_id\": item[\"id\"],\n",
    "                        \"title\": item[\"snippet\"][\"title\"],\n",
    "                        \"description\": item[\"snippet\"][\"description\"],\n",
    "                        \"tags\": item[\"snippet\"].get(\"tags\", []),\n",
    "                        \"duration\": str(duration),\n",
    "                        \"view_count\": int(item[\"statistics\"].get(\"viewCount\", 0)),\n",
    "                        \"like_count\": int(item[\"statistics\"].get(\"likeCount\", 0)),\n",
    "                        \"comment_count\": int(item[\"statistics\"].get(\"commentCount\", 0)),\n",
    "                        \"category_id\": category_id,\n",
    "                    })\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching videos: {e}\")\n",
    "            time.sleep(5)  # 잠시 대기 후 다시 시도\n",
    "\n",
    "    return videos\n",
    "# 모든 동영상 데이터 가져오기기\n",
    "def write_all_videos(output_file=\"raw_video_data.json\"):\n",
    "    all_videos = {}\n",
    "\n",
    "    for category_name, category_id in CATEGORIES.items():\n",
    "        print(f\"비디오 가져올 카테고리 : {category_name}\")\n",
    "        videos = fetch_trending_videos(category_id, region_code=\"KR\", max_results=200)\n",
    "        all_videos[category_name] = videos\n",
    "        print(f\"{len(videos)}개 완료.\")\n",
    "        \n",
    "    save_to_json(all_videos, output_file)\n",
    "    print(f\"write_all_videos : 모든 비디오 데이터가 '{output_file}'에 저장되었습니다.\")\n",
    "# 비디오 댓글 가져오기 함수\n",
    "def fetch_video_comments(video_id, max_results=100):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < max_results:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=min(50, max_results - len(comments)),\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                comments.append(comment)\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                if not comments:\n",
    "                    print(f\"{video_id} - 댓글 없음\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "# 비디오 댓글 저장 함수\n",
    "def write_video_comments(input_file, output_file):\n",
    "    # JSON 데이터 로드\n",
    "    data = load_json(input_file)\n",
    "    \n",
    "    # 비디오 댓글 가져오기 및 저장\n",
    "    all_comments = {}\n",
    "    for category, videos in data.items():\n",
    "        all_comments[category] = []  # 카테고리별 리스트 생성\n",
    "\n",
    "        for video in videos:\n",
    "            video_id = video[\"video_id\"]\n",
    "            comments = fetch_video_comments(video_id, max_results=1000)\n",
    "            all_comments[category].append({\"video_id\": video_id, \"comments\": comments})\n",
    "             \n",
    "    # 결과 저장\n",
    "    save_to_json(all_comments, output_file)\n",
    "    print(f\"get_video_comments : 비디오 댓글이 '{output_file}'에 저장되었습니다.\")\n",
    "# 비디오 댓글 전처리 저장 함수\n",
    "def write_clean_text(input_file, output_file):\n",
    "    data = load_json(input_file)\n",
    "    processed_data = {} \n",
    "    \n",
    "    for category, videos in data.items():\n",
    "        processed_data[category] = []  # 카테고리별 리스트 생성\n",
    "        if isinstance(videos, list):  # 값이 리스트인지 확인\n",
    "            for video in videos:\n",
    "                video_id = video[\"video_id\"]\n",
    "                comments = video.get(\"comments\", [])\n",
    "                cleaned_comments = [clean_text(comment) for comment in comments]\n",
    "                processed_data[category].append({\"video_id\": video_id, \"comments\": cleaned_comments})\n",
    "        \n",
    "    # 총 문장 수 계산\n",
    "    total_sentences = sum(len(video[\"comments\"]) for videos in processed_data.values() for video in videos)\n",
    "    print(f\"wirte_clean : 총 문장 수 = {total_sentences}\")  \n",
    "    save_to_json(processed_data, output_file)\n",
    "# Kiwi 객체 생성 함수\n",
    "def study_extract_kiwi(input_file, output_file, train_file):\n",
    "    # Kiwi 객체 생성\n",
    "    kiwi = Kiwi()\n",
    "    # Load the cleaned video comments\n",
    "    data = load_json(input_file)\n",
    "    train_data = load_json(train_file)\n",
    "    kiwi_objects = {}\n",
    "    \n",
    "    # for category, videos in data.items():\n",
    "    #     if isinstance(videos, list):  # Check if the value is a list\n",
    "    #         train_data[category] = []  # Initialize list for each category\n",
    "    #         for video in videos:\n",
    "    #             comments = video.get(\"comments\", [])\n",
    "    #             train_data[category].extend(comments)  # Add corrected comments to train_data\n",
    "    # save_to_json(train_data, \"train_data.json\")\n",
    "    # 훈련\n",
    "    for category, comments in train_data.items():\n",
    "        kiwi = Kiwi()\n",
    "        kiwi.extract_add_words(comments, min_cnt=5, max_word_len=10, min_score=0.25, pos_score=0.5, lm_filter=True)\n",
    "        kiwi_objects[category] = kiwi\n",
    "        \n",
    "    print(\"study_kiwi : 훈련 완료\")\n",
    "    print(kiwi_objects)\n",
    "    \n",
    "    write_tokenize_nouns(input_file, output_file, kiwi_objects)\n",
    "    \n",
    "    return kiwi_objects\n",
    "# 형태소 분석 및 명사 추출 함수\n",
    "def write_tokenize_nouns(input_file, output_file, kiwi_objects):\n",
    "    \n",
    "    kiwi_stopwords = Stopwords()\n",
    "    \n",
    "    eng_stopwords = list(stopwords.words('english'))  # 영어 불용어 사전\n",
    "    \n",
    "    for stopword in eng_stopwords:\n",
    "        kiwi_stopwords.add((stopword, 'SL'))\n",
    "    \n",
    "    tokenized_nouns = {}\n",
    "    data = load_json(input_file)\n",
    "\n",
    "    for category, videos in data.items():\n",
    "        tokenized_nouns[category] = []  # 카테고리별 리스트 생성\n",
    "\n",
    "        for video in videos:\n",
    "            video_id = video[\"video_id\"]\n",
    "            comments = video.get(\"comments\", [])\n",
    "\n",
    "            # 댓글을 형태소 분석 후 명사만 추출\n",
    "            tokenized = []\n",
    "            for comment in comments:\n",
    "                tokens = kiwi_objects[category].tokenize(comment, stopwords = kiwi_stopwords )  # 카테고리별 Kiwi 객체 사용하여 형태소 분석\n",
    "                nouns = [\n",
    "                    token.form for token in tokens\n",
    "                    if token.tag in [\"NNP\", \"SL\"]  # 명사(NNP)만 추출\n",
    "                    and len(token.form) > 1  # 1글자 제외\n",
    "                ]\n",
    "                tokenized.append(nouns)\n",
    "\n",
    "            # 구조 유지\n",
    "            tokenized_nouns[category].append({\"video_id\": video_id, \"nouns\": tokenized})\n",
    "\n",
    "    save_to_json(tokenized_nouns, output_file)    \n",
    "# KR-WordRank 객체 생성 함수\n",
    "def work_krRank(input_file, output_file):\n",
    "\n",
    "    data = load_json(input_file)\n",
    "    ranked_keywords = {}  # ✅ 모든 카테고리의 결과를 저장할 딕셔너리\n",
    "\n",
    "    for category, values in data.items():\n",
    "        print(f\"🔍 카테고리: {category}\")\n",
    "        print(f\"📌 데이터 타입: {type(values)}\")\n",
    "\n",
    "        if not isinstance(values, list) or not values:\n",
    "            print(f\"⚠️ '{category}' 카테고리에서 키워드 분석을 건너뜀 (데이터 없음)\")\n",
    "            continue\n",
    "\n",
    "        # ✅ 키워드 추출 실행\n",
    "        keywords = summarize_with_keywords(values)\n",
    "        # ✅ 점수가 높은 순으로 정렬하여 상위 100개만 선택\n",
    "        sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:1000]\n",
    "        ranked_keywords[category] = sorted_keywords  # ✅ 결과 저장\n",
    "\n",
    "        # ✅ 현재 카테고리의 결과 출력\n",
    "        print(f\"✅ {category} 키워드 추출 완료\")\n",
    "\n",
    "    # ✅ 모든 카테고리의 결과를 한 번에 저장\n",
    "    save_to_json(ranked_keywords, output_file)\n",
    "    print(f\"✅ 키워드 랭킹이 '{output_file}'에 저장되었습니다.\")\n",
    "# kr-wrodrank cleaned data\n",
    "def work_krRank2(input_file):\n",
    "    min_count = 10   # 단어의 최소 출현 빈도수\n",
    "    max_length = 10  # 단어의 최대 길이\n",
    "    wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "\n",
    "    data = load_json(input_file)\n",
    "    \n",
    "    beta = 0.85    # PageRank의 감쇄 계수\n",
    "    max_iter = 10  # 반복 횟수\n",
    "    ranked_keywords = {}  # ✅ 모든 카테고리의 결과를 저장할 딕셔너리\n",
    "    noun_keywords = {}  # ✅ 명사 키워드만 저장할 딕셔너리\n",
    "    for category, videos in data.items():\n",
    "        for video in videos:\n",
    "            comments = video.get(\"comments\", [])\n",
    "            document = \" \".join(comments)  # ✅ 모든 댓글을 하나의 문서로 결합\n",
    "\n",
    "        # ✅ 데이터 상태 확인\n",
    "        print(f\"✅ 카테고리: {category}, 댓글 개수: {len(comments)}\")\n",
    "        print(f\"📌 문서 데이터 샘플: {comments[:10]}\")  # 일부 출력\n",
    "\n",
    "        # ✅ 키워드 추출 실행 (빈 리스트가 아님을 보장)\n",
    "        try:\n",
    "            keywords, rank, graph = wordrank_extractor.extract(comments, beta, max_iter)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 오류 발생: {category} 카테고리에서 키워드 추출 실패. 오류 메시지: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # 4️⃣ Kiwi를 사용해서 명사만 필터링\n",
    "        kiwi = Kiwi()\n",
    "\n",
    "        noun_keywords = {\n",
    "            word: score for word, score in keywords.items()\n",
    "            if any(token.tag.startswith(\"NNP\") for token in kiwi.tokenize(word))\n",
    "        }\n",
    "\n",
    "        sorted_noun_keywords = sorted(noun_keywords.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "        \n",
    "        ranked_keywords[category] = sorted_noun_keywords  # ✅ 결과 저장\n",
    "\n",
    "        # ✅ 현재 카테고리의 결과 출력\n",
    "        print(f\"Category: {category}\")\n",
    "        print(sorted_noun_keywords[:10])  # 상위 10개 키워드만 출력\n",
    "\n",
    "    # ✅ 모든 카테고리의 결과를 한 번에 저장\n",
    "    save_to_json(ranked_keywords, \"ranked_keywords.json\")\n",
    "    print(\"✅ work_krRank2 : 키워드 랭킹이 'ranked_keywords.json'에 저장되었습니다.\")\n",
    "# 텍스트 merge\n",
    "def merge_nouns(input_file, output_file):\n",
    "    data = load_json(input_file)\n",
    "    merged_data = {}  # 결과 저장할 딕셔너리\n",
    "\n",
    "    for category, videos in data.items():\n",
    "        category_texts = []  # 현재 카테고리의 video별 문자열 리스트\n",
    "\n",
    "        for video in videos:\n",
    "            nouns = video.get(\"nouns\", [])  # nouns 가져오기\n",
    "            \n",
    "            # ✅ video의 모든 nouns를 하나의 문자열로 변환\n",
    "            merged_text = \" \".join([\" \".join(noun_list) for noun_list in nouns])\n",
    "            \n",
    "            if merged_text:  # 비어있지 않은 경우 추가\n",
    "                category_texts.append(merged_text)\n",
    "\n",
    "        # ✅ 최종적으로 category별 str 리스트 저장\n",
    "        category_texts = [re.sub(r'\\s{2,}', ' ', text) for text in category_texts]\n",
    "        merged_data[category] = category_texts\n",
    "\n",
    "    save_to_json(merged_data, output_file)\n",
    "    print(\"✅ merge_nouns : 명사 데이터가 'merged_nouns.json'에 저장되었습니다.\")\n",
    "     \n",
    "def merge_comments(input_file, output_file):\n",
    "    data = load_json(input_file)\n",
    "    merged_data = {}  # 결과 저장할 딕셔너리\n",
    "\n",
    "    for category, videos in data.items():\n",
    "        category_comments = []  # 현재 카테고리의 video별 문자열 리스트\n",
    "\n",
    "        for video in videos:\n",
    "            comments = video.get(\"comments\", [])  # nouns 가져오기\n",
    "                \n",
    "                # ✅ video의 모든 comments를 하나의 문자열로 변환\n",
    "            merged_comments = \" \".join(comments)  \n",
    "            if merged_comments:  # 비어있지 않은 경우 추가\n",
    "                category_comments.append(merged_comments)\n",
    "\n",
    "            # ✅ 최종적으로 category별 str 리스트 저장\n",
    "            category_comments = [re.sub(r'\\s{2,}', ' ', text) for text in category_comments]\n",
    "            merged_data[category] = category_comments\n",
    "\n",
    "    save_to_json(merged_data, output_file)\n",
    "    print(\"✅ merge_nouns : 명사 데이터가 'merged_nouns.json'에 저장되었습니다.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비디오 가져올 카테고리 : News & Politics\n",
      "38개 완료.\n",
      "비디오 가져올 카테고리 : Music\n",
      "30개 완료.\n",
      "비디오 가져올 카테고리 : Sports\n",
      "9개 완료.\n",
      "비디오 가져올 카테고리 : Gaming\n",
      "71개 완료.\n",
      "비디오 가져올 카테고리 : Science & Technology\n",
      "67개 완료.\n",
      "save_to_json : 데이터 저장: data\\raw_video_data.json\n",
      "write_all_videos : 모든 비디오 데이터가 'raw_video_data.json'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "write_all_videos(\"raw_video_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fgorh0Oh4pM - 댓글 없음\n",
      "save_to_json : 데이터 저장: data\\video_comments.json\n",
      "get_video_comments : 비디오 댓글이 'video_comments.json'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "write_video_comments(\"raw_video_data.json\", \"video_comments.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wirte_clean : 총 문장 수 = 79528\n",
      "save_to_json : 데이터 저장: data\\cleaned_video_comments.json\n"
     ]
    }
   ],
   "source": [
    "write_clean_text(\"video_comments.json\", \"cleaned_video_comments.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study_kiwi : 훈련 완료\n",
      "{'News & Politics': Kiwi(num_workers=1, model_path=None, integrate_allomorph=True, load_default_dict=True, load_typo_dict=True, model_type='knlm', typos=None, typo_cost_threshold=2.5), 'Music': Kiwi(num_workers=1, model_path=None, integrate_allomorph=True, load_default_dict=True, load_typo_dict=True, model_type='knlm', typos=None, typo_cost_threshold=2.5), 'Sports': Kiwi(num_workers=1, model_path=None, integrate_allomorph=True, load_default_dict=True, load_typo_dict=True, model_type='knlm', typos=None, typo_cost_threshold=2.5), 'Gaming': Kiwi(num_workers=1, model_path=None, integrate_allomorph=True, load_default_dict=True, load_typo_dict=True, model_type='knlm', typos=None, typo_cost_threshold=2.5), 'Science & Technology': Kiwi(num_workers=1, model_path=None, integrate_allomorph=True, load_default_dict=True, load_typo_dict=True, model_type='knlm', typos=None, typo_cost_threshold=2.5)}\n",
      "save_to_json : 데이터 저장: data\\tokenized_nouns.json\n"
     ]
    }
   ],
   "source": [
    "kiwi_objects = study_extract_kiwi(\"cleaned_video_comments.json\",\"tokenized_nouns.json\", \"merged_comments.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_to_json : 데이터 저장: data\\merged_data.json\n",
      "✅ merge_nouns : 명사 데이터가 'merged_nouns.json'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "merge_nouns(\"tokenized_nouns.json\", \"merged_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 카테고리: News & Politics\n",
      "📌 데이터 타입: <class 'list'>\n",
      "✅ News & Politics 키워드 추출 완료\n",
      "🔍 카테고리: Music\n",
      "📌 데이터 타입: <class 'list'>\n",
      "✅ Music 키워드 추출 완료\n",
      "🔍 카테고리: Sports\n",
      "📌 데이터 타입: <class 'list'>\n",
      "✅ Sports 키워드 추출 완료\n",
      "🔍 카테고리: Gaming\n",
      "📌 데이터 타입: <class 'list'>\n",
      "✅ Gaming 키워드 추출 완료\n",
      "🔍 카테고리: Science & Technology\n",
      "📌 데이터 타입: <class 'list'>\n",
      "✅ Science & Technology 키워드 추출 완료\n",
      "save_to_json : 데이터 저장: data\\ranked_keywords.json\n",
      "✅ 키워드 랭킹이 'ranked_keywords.json'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "work_krRank(\"merged_data.json\", \"ranked_keywords.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_to_json : 데이터 저장: data\\title_tags.json\n"
     ]
    }
   ],
   "source": [
    "merged_data = merge_clean_title_tags(load_json(\"raw_video_data.json\"))\n",
    "\n",
    "save_to_json(merged_data, \"title_tags.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_to_json : 데이터 저장: data\\merged_comments.json\n",
      "✅ merge_nouns : 명사 데이터가 'merged_nouns.json'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "merge_comments(\"cleaned_video_comments.json\", \"merged_comments.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Okt tokenizer\n",
    "okt = Okt()\n",
    "\n",
    "# Function to extract nouns using Okt\n",
    "\n",
    "\n",
    "# Example usage\n",
    "data = load_json('title_tags.json')\n",
    "\n",
    "for category, list in data.items():\n",
    "    for item in list:\n",
    "        nouns = okt.nouns(item)\n",
    "        print(nouns)\n",
    "nouns = extract_nouns(sample_text)\n",
    "print(nouns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
