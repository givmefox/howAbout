{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import isodate\n",
    "import emoji\n",
    "from kiwipiepy import Kiwi\n",
    "from kiwipiepy.utils import Stopwords\n",
    "from krwordrank.word import KRWordRank\n",
    "from krwordrank.word import summarize_with_keywords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë° ë³€ìˆ˜ ì„¤ì •\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)  \n",
    "\n",
    "CATEGORIES = {\n",
    "    \"News & Politics\": \"25\",\n",
    "    'Music' : \"10\",\n",
    "    'Sports' : \"17\",\n",
    "    'Gaming' : \"20\",\n",
    "    'Science & Technology': \"28\"\n",
    "}\n",
    "# JSON ë°ì´í„° ì €ì¥ í•¨ìˆ˜\n",
    "def save_to_json(data, filename):\n",
    "    # data í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    \n",
    "    # íŒŒì¼ ê²½ë¡œë¥¼ data í´ë” ì•„ë˜ë¡œ ì„¤ì •\n",
    "    filepath = os.path.join('data', filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    print(f\"save_to_json : ë°ì´í„° ì €ì¥: {filepath}\")\n",
    "    \n",
    "    # JSON ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "# JSON íŒŒì¼ì„ ì½ì–´ì„œ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "def load_json(filename):\n",
    "    # íŒŒì¼ ê²½ë¡œë¥¼ data í´ë” ì•„ë˜ë¡œ ì„¤ì •\n",
    "    filepath = os.path.join('data', filename)\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "# valueê°€ ë¬¸ìì—´ë°°ì—´ì˜ ë°°ì—´ì¼ ë•Œ í•œê¸€ìì™€ ë¹ˆ ë¬¸ìì—´ì„ ì œì™¸í•˜ê³  ë¬¸ìì—´ë¡œ ë§Œë“œëŠ” í•¨ìˆ˜\n",
    "def merge_values(data):\n",
    "    \"\"\"\n",
    "    key: valueì—ì„œ valueê°€ ë¬¸ìì—´ ë°°ì—´ì˜ ë°°ì—´ì¼ ë•Œ,\n",
    "    í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ë¹ˆ ë¬¸ìì—´(\"\") ë° í•œ ê¸€ì ë‹¨ì–´ëŠ” ì œì™¸í•˜ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    :param data: dict, key: list of lists (e.g., { \"key1\": [[\"ë¬¸ì¥1\", \"ë¬¸ì¥2\"], [\"ë¬¸ì¥3\"]] })\n",
    "    :return: dict, key: merged string (e.g., { \"key1\": \"ë¬¸ì¥1 ë¬¸ì¥2 ë¬¸ì¥3\" })\n",
    "    \"\"\"\n",
    "    merged_data = merge_values\n",
    "    for key, values in data.items():\n",
    "        if isinstance(values, list):  # ê°’ì´ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸\n",
    "            merged_sentence = \" \".join(\n",
    "                s for sublist in values for s in sublist if s.strip() and len(s.strip()) > 1\n",
    "            )  # ë¹ˆ ë¬¸ìì—´ & í•œ ê¸€ì ì œì™¸ í›„ ê³µë°±ìœ¼ë¡œ ì—°ê²°\n",
    "            merged_data[key] = merged_sentence  # key: ë³‘í•©ëœ ë¬¸ìì—´ í˜•íƒœë¡œ ì €ì¥\n",
    "\n",
    "    return merged_data\n",
    "# titleê³¼ tagsë¥¼ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ í•©ì¹˜ëŠ” í•¨ìˆ˜\n",
    "def merge_clean_title_tags(data):\n",
    "    \"\"\"\n",
    "    ë”•ì…”ë„ˆë¦¬ì—ì„œ 'title'ê³¼ 'tags'ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜.\n",
    "    \n",
    "    :param data: dict (ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ì •ë³´)\n",
    "    :return: dict (titleê³¼ tagsê°€ í•©ì³ì§„ ë¬¸ì¥)\n",
    "    \"\"\"\n",
    "    merged_data = {}\n",
    "    for category, videos in data.items():\n",
    "        merged_data[category] = []  # ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "        for video in videos:\n",
    "            title = video.get(\"title\", \"\").strip()  # titleì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´\n",
    "            tags = video.get(\"tags\", [])  # tagsê°€ ì—†ê±°ë‚˜ Noneì´ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬\n",
    "            merged = \" \".join([title] + tags)  # titleê³¼ tags ê²°í•©\n",
    "            cleaned_merged = clean_text(merged)  # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "            merged_data[category].append(cleaned_merged)\n",
    "    \n",
    "    return merged_data\n",
    "# í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
    "def clean_text(text):\n",
    "    # HTML íƒœê·¸ ì œê±°\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # &quot ì œê±°\n",
    "    text = text.replace(\"&quot\", \"\")\n",
    "    text = text.replace(\"&lt\", \"\")\n",
    "    text = text.replace(\"&gt\", \"\")\n",
    "    # URL, ë©˜ì…˜, í•´ì‹œíƒœê·¸ ì œê±°\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\S+|#\\S+', '', text)\n",
    "    # í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì ì œê±°\n",
    "    text = re.sub(r\"[^ê°€-í£a-zA-Z0-9\\s]\", \"\", text)\n",
    "    # ì•ë’¤ ê³µë°± ì œê±°\n",
    "    text = text.strip()\n",
    "    # ì¤‘ë³µ ê³µë°± ì œê±°\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    # ëª¨ë“  ì´ëª¨ì§€ ì œê±°\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    # ì˜ì–´ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"ìœ¤ëŒ€í†µë ¹|ìœ¤ëŒ€í†µ|ìœ¤í†µ|ìœ¤\", \"ìœ¤ì„ì—´\", text)\n",
    "    text = re.sub(r\"ì¥ì›\", \"í™ì¥ì›\", text)\n",
    "    text = re.sub(r\"ê³„ì—„ë ¹\", \"ê³„ì—„\", text)\n",
    "\n",
    "    return text\n",
    "# YOUTUBE APIë¥¼ í†µí•´ ë™ì˜ìƒ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "def fetch_trending_videos(category_id, region_code=\"KR\", max_results=200):\n",
    "    \n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(videos) < max_results:\n",
    "        try:\n",
    "            request = youtube.videos().list(\n",
    "                part=\"snippet,statistics,contentDetails\",\n",
    "                chart=\"mostPopular\",\n",
    "                regionCode=region_code,\n",
    "                videoCategoryId=category_id,\n",
    "                maxResults=min(50, max_results - len(videos)),\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                duration = isodate.parse_duration(item[\"contentDetails\"][\"duration\"])\n",
    "                duration_in_seconds = duration.total_seconds()  #ì´ˆë¡œ ë°”ê¾¸ê¸°ê¸°\n",
    "\n",
    "                if duration_in_seconds > 80:  # 80ì´ˆ ì´ìƒì˜ ë™ì˜ìƒë§Œ ê°€ì ¸ì˜¤ê¸°\n",
    "                    videos.append({\n",
    "                        \"video_id\": item[\"id\"],\n",
    "                        \"title\": item[\"snippet\"][\"title\"],\n",
    "                        \"description\": item[\"snippet\"][\"description\"],\n",
    "                        \"tags\": item[\"snippet\"].get(\"tags\", []),\n",
    "                        \"duration\": str(duration),\n",
    "                        \"view_count\": int(item[\"statistics\"].get(\"viewCount\", 0)),\n",
    "                        \"like_count\": int(item[\"statistics\"].get(\"likeCount\", 0)),\n",
    "                        \"comment_count\": int(item[\"statistics\"].get(\"commentCount\", 0)),\n",
    "                        \"category_id\": category_id,\n",
    "                    })\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching videos: {e}\")\n",
    "            time.sleep(5)  # ì ì‹œ ëŒ€ê¸° í›„ ë‹¤ì‹œ ì‹œë„\n",
    "\n",
    "    return videos\n",
    "# ëª¨ë“  ë™ì˜ìƒ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°ê¸°\n",
    "def write_all_videos(output_file=\"raw_video_data.json\"):\n",
    "    all_videos = {}\n",
    "\n",
    "    for category_name, category_id in CATEGORIES.items():\n",
    "        print(f\"ë¹„ë””ì˜¤ ê°€ì ¸ì˜¬ ì¹´í…Œê³ ë¦¬ : {category_name}\")\n",
    "        videos = fetch_trending_videos(category_id, region_code=\"KR\", max_results=200)\n",
    "        all_videos[category_name] = videos\n",
    "        print(f\"{len(videos)}ê°œ ì™„ë£Œ.\")\n",
    "        \n",
    "    save_to_json(all_videos, output_file)\n",
    "    print(f\"write_all_videos : ëª¨ë“  ë¹„ë””ì˜¤ ë°ì´í„°ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "# ë¹„ë””ì˜¤ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜\n",
    "def fetch_video_comments(video_id, max_results=100):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < max_results:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=min(50, max_results - len(comments)),\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                comments.append(comment)\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                if not comments:\n",
    "                    print(f\"{video_id} - ëŒ“ê¸€ ì—†ìŒ\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "# ë¹„ë””ì˜¤ ëŒ“ê¸€ ì €ì¥ í•¨ìˆ˜\n",
    "def write_video_comments(input_file, output_file):\n",
    "    # JSON ë°ì´í„° ë¡œë“œ\n",
    "    data = load_json(input_file)\n",
    "    \n",
    "    # ë¹„ë””ì˜¤ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° ë° ì €ì¥\n",
    "    all_comments = {}\n",
    "    for category, videos in data.items():\n",
    "        all_comments[category] = []  # ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "\n",
    "        for video in videos:\n",
    "            video_id = video[\"video_id\"]\n",
    "            comments = fetch_video_comments(video_id, max_results=1000)\n",
    "            all_comments[category].append({\"video_id\": video_id, \"comments\": comments})\n",
    "             \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    save_to_json(all_comments, output_file)\n",
    "    print(f\"get_video_comments : ë¹„ë””ì˜¤ ëŒ“ê¸€ì´ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "# ë¹„ë””ì˜¤ ëŒ“ê¸€ ì „ì²˜ë¦¬ ì €ì¥ í•¨ìˆ˜\n",
    "def write_clean_text(input_file, output_file):\n",
    "    data = load_json(input_file)\n",
    "    processed_data = {} \n",
    "    \n",
    "    for category, videos in data.items():\n",
    "        processed_data[category] = []  # ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "        if isinstance(videos, list):  # ê°’ì´ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸\n",
    "            for video in videos:\n",
    "                video_id = video[\"video_id\"]\n",
    "                comments = video.get(\"comments\", [])\n",
    "                cleaned_comments = [clean_text(comment) for comment in comments]\n",
    "                processed_data[category].append({\"video_id\": video_id, \"comments\": cleaned_comments})\n",
    "        \n",
    "    # ì´ ë¬¸ì¥ ìˆ˜ ê³„ì‚°\n",
    "    total_sentences = sum(len(video[\"comments\"]) for videos in processed_data.values() for video in videos)\n",
    "    print(f\"wirte_clean : ì´ ë¬¸ì¥ ìˆ˜ = {total_sentences}\")  \n",
    "    save_to_json(processed_data, output_file)\n",
    "# Kiwi ê°ì²´ ìƒì„± í•¨ìˆ˜\n",
    "def study_extract_kiwi(input_file, output_file, train_file):\n",
    "    # Kiwi ê°ì²´ ìƒì„±\n",
    "    kiwi = Kiwi()\n",
    "    # Load the cleaned video comments\n",
    "    data = load_json(input_file)\n",
    "    train_data = load_json(train_file)\n",
    "    kiwi_objects = {}\n",
    "    \n",
    "    # for category, videos in data.items():\n",
    "    #     if isinstance(videos, list):  # Check if the value is a list\n",
    "    #         train_data[category] = []  # Initialize list for each category\n",
    "    #         for video in videos:\n",
    "    #             comments = video.get(\"comments\", [])\n",
    "    #             train_data[category].extend(comments)  # Add corrected comments to train_data\n",
    "    # save_to_json(train_data, \"train_data.json\")\n",
    "    # í›ˆë ¨\n",
    "    for category, comments in train_data.items():\n",
    "        kiwi = Kiwi()\n",
    "        kiwi.extract_add_words(comments, min_cnt=5, max_word_len=10, min_score=0.25, pos_score=0.5, lm_filter=True)\n",
    "        kiwi_objects[category] = kiwi\n",
    "        \n",
    "    print(\"study_kiwi : í›ˆë ¨ ì™„ë£Œ\")\n",
    "    print(kiwi_objects)\n",
    "    \n",
    "    write_tokenize_nouns(input_file, output_file, kiwi_objects)\n",
    "    \n",
    "    return kiwi_objects\n",
    "# í˜•íƒœì†Œ ë¶„ì„ ë° ëª…ì‚¬ ì¶”ì¶œ í•¨ìˆ˜\n",
    "def write_tokenize_nouns(input_file, output_file, kiwi_objects):\n",
    "    \n",
    "    kiwi_stopwords = Stopwords()\n",
    "    \n",
    "    eng_stopwords = list(stopwords.words('english'))  # ì˜ì–´ ë¶ˆìš©ì–´ ì‚¬ì „\n",
    "    \n",
    "    for stopword in eng_stopwords:\n",
    "        kiwi_stopwords.add((stopword, 'SL'))\n",
    "    \n",
    "    tokenized_nouns = {}\n",
    "    data = load_json(input_file)\n",
    "\n",
    "    for category, videos in data.items():\n",
    "        tokenized_nouns[category] = []  # ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "\n",
    "        for video in videos:\n",
    "            video_id = video[\"video_id\"]\n",
    "            comments = video.get(\"comments\", [])\n",
    "\n",
    "            # ëŒ“ê¸€ì„ í˜•íƒœì†Œ ë¶„ì„ í›„ ëª…ì‚¬ë§Œ ì¶”ì¶œ\n",
    "            tokenized = []\n",
    "            for comment in comments:\n",
    "                tokens = kiwi_objects[category].tokenize(comment, stopwords = kiwi_stopwords )  # ì¹´í…Œê³ ë¦¬ë³„ Kiwi ê°ì²´ ì‚¬ìš©í•˜ì—¬ í˜•íƒœì†Œ ë¶„ì„\n",
    "                nouns = [\n",
    "                    token.form for token in tokens\n",
    "                    if token.tag in [\"NNP\", \"SL\"]  # ëª…ì‚¬(NNP)ë§Œ ì¶”ì¶œ\n",
    "                    and len(token.form) > 1  # 1ê¸€ì ì œì™¸\n",
    "                ]\n",
    "                tokenized.append(nouns)\n",
    "\n",
    "            # êµ¬ì¡° ìœ ì§€\n",
    "            tokenized_nouns[category].append({\"video_id\": video_id, \"nouns\": tokenized})\n",
    "\n",
    "    save_to_json(tokenized_nouns, output_file)    \n",
    "# KR-WordRank ê°ì²´ ìƒì„± í•¨ìˆ˜\n",
    "def work_krRank(input_file, output_file):\n",
    "\n",
    "    data = load_json(input_file)\n",
    "    ranked_keywords = {}  # âœ… ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "\n",
    "    for category, values in data.items():\n",
    "        print(f\"ğŸ” ì¹´í…Œê³ ë¦¬: {category}\")\n",
    "        print(f\"ğŸ“Œ ë°ì´í„° íƒ€ì…: {type(values)}\")\n",
    "\n",
    "        if not isinstance(values, list) or not values:\n",
    "            print(f\"âš ï¸ '{category}' ì¹´í…Œê³ ë¦¬ì—ì„œ í‚¤ì›Œë“œ ë¶„ì„ì„ ê±´ë„ˆëœ€ (ë°ì´í„° ì—†ìŒ)\")\n",
    "            continue\n",
    "\n",
    "        # âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤í–‰\n",
    "        keywords = summarize_with_keywords(values)\n",
    "        # âœ… ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 100ê°œë§Œ ì„ íƒ\n",
    "        sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:1000]\n",
    "        ranked_keywords[category] = sorted_keywords  # âœ… ê²°ê³¼ ì €ì¥\n",
    "\n",
    "        # âœ… í˜„ì¬ ì¹´í…Œê³ ë¦¬ì˜ ê²°ê³¼ ì¶œë ¥\n",
    "        print(f\"âœ… {category} í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ\")\n",
    "\n",
    "    # âœ… ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ê²°ê³¼ë¥¼ í•œ ë²ˆì— ì €ì¥\n",
    "    save_to_json(ranked_keywords, output_file)\n",
    "    print(f\"âœ… í‚¤ì›Œë“œ ë­í‚¹ì´ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "# kr-wrodrank cleaned data\n",
    "def work_krRank2(input_file):\n",
    "    min_count = 10   # ë‹¨ì–´ì˜ ìµœì†Œ ì¶œí˜„ ë¹ˆë„ìˆ˜\n",
    "    max_length = 10  # ë‹¨ì–´ì˜ ìµœëŒ€ ê¸¸ì´\n",
    "    wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "\n",
    "    data = load_json(input_file)\n",
    "    \n",
    "    beta = 0.85    # PageRankì˜ ê°ì‡„ ê³„ìˆ˜\n",
    "    max_iter = 10  # ë°˜ë³µ íšŸìˆ˜\n",
    "    ranked_keywords = {}  # âœ… ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "    noun_keywords = {}  # âœ… ëª…ì‚¬ í‚¤ì›Œë“œë§Œ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "    for category, videos in data.items():\n",
    "        for video in videos:\n",
    "            comments = video.get(\"comments\", [])\n",
    "            document = \" \".join(comments)  # âœ… ëª¨ë“  ëŒ“ê¸€ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ê²°í•©\n",
    "\n",
    "        # âœ… ë°ì´í„° ìƒíƒœ í™•ì¸\n",
    "        print(f\"âœ… ì¹´í…Œê³ ë¦¬: {category}, ëŒ“ê¸€ ê°œìˆ˜: {len(comments)}\")\n",
    "        print(f\"ğŸ“Œ ë¬¸ì„œ ë°ì´í„° ìƒ˜í”Œ: {comments[:10]}\")  # ì¼ë¶€ ì¶œë ¥\n",
    "\n",
    "        # âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤í–‰ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜ì„ ë³´ì¥)\n",
    "        try:\n",
    "            keywords, rank, graph = wordrank_extractor.extract(comments, beta, max_iter)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {category} ì¹´í…Œê³ ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨. ì˜¤ë¥˜ ë©”ì‹œì§€: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # 4ï¸âƒ£ Kiwië¥¼ ì‚¬ìš©í•´ì„œ ëª…ì‚¬ë§Œ í•„í„°ë§\n",
    "        kiwi = Kiwi()\n",
    "\n",
    "        noun_keywords = {\n",
    "            word: score for word, score in keywords.items()\n",
    "            if any(token.tag.startswith(\"NNP\") for token in kiwi.tokenize(word))\n",
    "        }\n",
    "\n",
    "        sorted_noun_keywords = sorted(noun_keywords.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "        \n",
    "        ranked_keywords[category] = sorted_noun_keywords  # âœ… ê²°ê³¼ ì €ì¥\n",
    "\n",
    "        # âœ… í˜„ì¬ ì¹´í…Œê³ ë¦¬ì˜ ê²°ê³¼ ì¶œë ¥\n",
    "        print(f\"Category: {category}\")\n",
    "        print(sorted_noun_keywords[:10])  # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œë§Œ ì¶œë ¥\n",
    "\n",
    "    # âœ… ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ê²°ê³¼ë¥¼ í•œ ë²ˆì— ì €ì¥\n",
    "    save_to_json(ranked_keywords, \"ranked_keywords.json\")\n",
    "    print(\"âœ… work_krRank2 : í‚¤ì›Œë“œ ë­í‚¹ì´ 'ranked_keywords.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "# í…ìŠ¤íŠ¸ merge\n",
    "def merge_nouns(input_file, output_file):\n",
    "    data = load_json(input_file)\n",
    "    merged_data = {}  # ê²°ê³¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "\n",
    "    for category, videos in data.items():\n",
    "        category_texts = []  # í˜„ì¬ ì¹´í…Œê³ ë¦¬ì˜ videoë³„ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "        for video in videos:\n",
    "            nouns = video.get(\"nouns\", [])  # nouns ê°€ì ¸ì˜¤ê¸°\n",
    "            \n",
    "            # âœ… videoì˜ ëª¨ë“  nounsë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "            merged_text = \" \".join([\" \".join(noun_list) for noun_list in nouns])\n",
    "            \n",
    "            if merged_text:  # ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš° ì¶”ê°€\n",
    "                category_texts.append(merged_text)\n",
    "\n",
    "        # âœ… ìµœì¢…ì ìœ¼ë¡œ categoryë³„ str ë¦¬ìŠ¤íŠ¸ ì €ì¥\n",
    "        category_texts = [re.sub(r'\\s{2,}', ' ', text) for text in category_texts]\n",
    "        merged_data[category] = category_texts\n",
    "\n",
    "    save_to_json(merged_data, output_file)\n",
    "    print(\"âœ… merge_nouns : ëª…ì‚¬ ë°ì´í„°ê°€ 'merged_nouns.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "     \n",
    "def merge_comments(input_file, output_file):\n",
    "    data = load_json(input_file)\n",
    "    merged_data = {}  # ê²°ê³¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "\n",
    "    for category, videos in data.items():\n",
    "        category_comments = []  # í˜„ì¬ ì¹´í…Œê³ ë¦¬ì˜ videoë³„ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "        for video in videos:\n",
    "            comments = video.get(\"comments\", [])  # nouns ê°€ì ¸ì˜¤ê¸°\n",
    "                \n",
    "                # âœ… videoì˜ ëª¨ë“  commentsë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "            merged_comments = \" \".join(comments)  \n",
    "            if merged_comments:  # ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš° ì¶”ê°€\n",
    "                category_comments.append(merged_comments)\n",
    "\n",
    "            # âœ… ìµœì¢…ì ìœ¼ë¡œ categoryë³„ str ë¦¬ìŠ¤íŠ¸ ì €ì¥\n",
    "            category_comments = [re.sub(r'\\s{2,}', ' ', text) for text in category_comments]\n",
    "            merged_data[category] = category_comments\n",
    "\n",
    "    save_to_json(merged_data, output_file)\n",
    "    print(\"âœ… merge_nouns : ëª…ì‚¬ ë°ì´í„°ê°€ 'merged_nouns.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¹„ë””ì˜¤ ê°€ì ¸ì˜¬ ì¹´í…Œê³ ë¦¬ : News & Politics\n",
      "38ê°œ ì™„ë£Œ.\n",
      "ë¹„ë””ì˜¤ ê°€ì ¸ì˜¬ ì¹´í…Œê³ ë¦¬ : Music\n",
      "30ê°œ ì™„ë£Œ.\n",
      "ë¹„ë””ì˜¤ ê°€ì ¸ì˜¬ ì¹´í…Œê³ ë¦¬ : Sports\n",
      "9ê°œ ì™„ë£Œ.\n",
      "ë¹„ë””ì˜¤ ê°€ì ¸ì˜¬ ì¹´í…Œê³ ë¦¬ : Gaming\n",
      "71ê°œ ì™„ë£Œ.\n",
      "ë¹„ë””ì˜¤ ê°€ì ¸ì˜¬ ì¹´í…Œê³ ë¦¬ : Science & Technology\n",
      "67ê°œ ì™„ë£Œ.\n",
      "save_to_json : ë°ì´í„° ì €ì¥: data\\raw_video_data.json\n",
      "write_all_videos : ëª¨ë“  ë¹„ë””ì˜¤ ë°ì´í„°ê°€ 'raw_video_data.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "write_all_videos(\"raw_video_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fgorh0Oh4pM - ëŒ“ê¸€ ì—†ìŒ\n",
      "save_to_json : ë°ì´í„° ì €ì¥: data\\video_comments.json\n",
      "get_video_comments : ë¹„ë””ì˜¤ ëŒ“ê¸€ì´ 'video_comments.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "write_video_comments(\"raw_video_data.json\", \"video_comments.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wirte_clean : ì´ ë¬¸ì¥ ìˆ˜ = 79528\n",
      "save_to_json : ë°ì´í„° ì €ì¥: data\\cleaned_video_comments.json\n"
     ]
    }
   ],
   "source": [
    "write_clean_text(\"video_comments.json\", \"cleaned_video_comments.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study_kiwi : í›ˆë ¨ ì™„ë£Œ\n",
      "{'News & Politics': Kiwi(num_workers=1, model_path=None, integrate_allomorph=True, load_default_dict=True, load_typo_dict=True, model_type='knlm', typos=None, typo_cost_threshold=2.5), 'Music': Kiwi(num_workers=1, model_path=None, integrate_allomorph=True, load_default_dict=True, load_typo_dict=True, model_type='knlm', typos=None, typo_cost_threshold=2.5), 'Sports': Kiwi(num_workers=1, model_path=None, integrate_allomorph=True, load_default_dict=True, load_typo_dict=True, model_type='knlm', typos=None, typo_cost_threshold=2.5), 'Gaming': Kiwi(num_workers=1, model_path=None, integrate_allomorph=True, load_default_dict=True, load_typo_dict=True, model_type='knlm', typos=None, typo_cost_threshold=2.5), 'Science & Technology': Kiwi(num_workers=1, model_path=None, integrate_allomorph=True, load_default_dict=True, load_typo_dict=True, model_type='knlm', typos=None, typo_cost_threshold=2.5)}\n",
      "save_to_json : ë°ì´í„° ì €ì¥: data\\tokenized_nouns.json\n"
     ]
    }
   ],
   "source": [
    "kiwi_objects = study_extract_kiwi(\"cleaned_video_comments.json\",\"tokenized_nouns.json\", \"merged_comments.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_to_json : ë°ì´í„° ì €ì¥: data\\merged_data.json\n",
      "âœ… merge_nouns : ëª…ì‚¬ ë°ì´í„°ê°€ 'merged_nouns.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "merge_nouns(\"tokenized_nouns.json\", \"merged_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ì¹´í…Œê³ ë¦¬: News & Politics\n",
      "ğŸ“Œ ë°ì´í„° íƒ€ì…: <class 'list'>\n",
      "âœ… News & Politics í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ\n",
      "ğŸ” ì¹´í…Œê³ ë¦¬: Music\n",
      "ğŸ“Œ ë°ì´í„° íƒ€ì…: <class 'list'>\n",
      "âœ… Music í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ\n",
      "ğŸ” ì¹´í…Œê³ ë¦¬: Sports\n",
      "ğŸ“Œ ë°ì´í„° íƒ€ì…: <class 'list'>\n",
      "âœ… Sports í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ\n",
      "ğŸ” ì¹´í…Œê³ ë¦¬: Gaming\n",
      "ğŸ“Œ ë°ì´í„° íƒ€ì…: <class 'list'>\n",
      "âœ… Gaming í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ\n",
      "ğŸ” ì¹´í…Œê³ ë¦¬: Science & Technology\n",
      "ğŸ“Œ ë°ì´í„° íƒ€ì…: <class 'list'>\n",
      "âœ… Science & Technology í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ\n",
      "save_to_json : ë°ì´í„° ì €ì¥: data\\ranked_keywords.json\n",
      "âœ… í‚¤ì›Œë“œ ë­í‚¹ì´ 'ranked_keywords.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "work_krRank(\"merged_data.json\", \"ranked_keywords.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_to_json : ë°ì´í„° ì €ì¥: data\\title_tags.json\n"
     ]
    }
   ],
   "source": [
    "merged_data = merge_clean_title_tags(load_json(\"raw_video_data.json\"))\n",
    "\n",
    "save_to_json(merged_data, \"title_tags.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_to_json : ë°ì´í„° ì €ì¥: data\\merged_comments.json\n",
      "âœ… merge_nouns : ëª…ì‚¬ ë°ì´í„°ê°€ 'merged_nouns.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "merge_comments(\"cleaned_video_comments.json\", \"merged_comments.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Okt tokenizer\n",
    "okt = Okt()\n",
    "\n",
    "# Function to extract nouns using Okt\n",
    "\n",
    "\n",
    "# Example usage\n",
    "data = load_json('title_tags.json')\n",
    "\n",
    "for category, list in data.items():\n",
    "    for item in list:\n",
    "        nouns = okt.nouns(item)\n",
    "        print(nouns)\n",
    "nouns = extract_nouns(sample_text)\n",
    "print(nouns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
