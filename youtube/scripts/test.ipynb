{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SoyNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223357"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp import DoublespaceLineCorpus\n",
    "\n",
    "#문서 단위 말뭉치 생성\n",
    "corups = DoublespaceLineCorpus(\"2016-10-20.txt\", iter_sent = True)\n",
    "len(corups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.658 Gbse memory 0.704 Gb\n"
     ]
    }
   ],
   "source": [
    "from soynlp.word import WordExtractor\n",
    "\n",
    "word_extractor = WordExtractor()\n",
    "\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(corups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 360721\n",
      "all accessor variety was computed # words = 360721\n"
     ]
    }
   ],
   "source": [
    "word_score = word_extractor.extract()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.43154839105434084)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_score[\"연합\"].cohesion_forward\n",
    "word_score[\"연합뉴\"].cohesion_forward\n",
    "# word_score[\"연합뉴스\"].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['연합뉴스', '는', '정말로', '좋은', '언론', '사이다']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer\n",
    "scores = {word:score.cohesion_forward for word, score in word_score.items()}\n",
    "l_tokenizer = LTokenizer(scores = scores)\n",
    "l_tokenizer.tokenize(\"연합뉴스는 정말로 좋은 언론사이다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching trending videos for category: News & Politics\n",
      "비디오 61 개 카테고리: News & Politics fetch 완료.\n",
      "데이터 저장 : data/raw_video_data.json'\n",
      "\n",
      "Category: News & Politics\n",
      " - 미친 듯 날뛰던 이재명. 전한길 등장에 꼭꼭 숨었다 [주말 몰아보기] (xJbj4MRPXOs), 조회수: 1008830 회, 좋아요: 89223 개\n",
      " - 尹 사건 맡은 중앙지법! 대형사건 터졌다![배승희 뉴스배송] (xYld5mZwuQU), 조회수: 842793 회, 좋아요: 141548 개\n",
      " - 이상민 입 열자 尹 '휘청' \"10시 KBS\" 발언의 비밀 [뉴스.zip/MBC뉴스] (EMKYRHDCDC8), 조회수: 1133126 회, 좋아요: 21969 개\n",
      " - [겸공뉴스특보] 2025년 2월 3일 월요일 (ZS7yJum20ic), 조회수: 550098 회, 좋아요: 32739 개\n",
      " - 일타강사 전한길 “비상계엄은 100% 계몽령” / 채널A / 뉴스 TOP10 (OQ5GJ0eMCKI), 조회수: 523307 회, 좋아요: 28908 개\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import isodate\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 환경 변수에서 API 키 가져오기\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API 키가 설정되지 않았습니다. 환경 변수 'YOUTUBE_API_KEY'를 설정하세요.\")\n",
    "\n",
    "# YouTube API 설정\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# 카테고리 ID 설정\n",
    "CATEGORIES = {\n",
    "    \"News & Politics\": \"25\"\n",
    "}\n",
    "\n",
    "# 동영상 데이터 가져오기\n",
    "def fetch_trending_videos(category_id, region_code=\"KR\", max_results=200):\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(videos) < max_results:\n",
    "        try:\n",
    "            request = youtube.videos().list(\n",
    "                part=\"snippet,statistics,contentDetails\",\n",
    "                chart=\"mostPopular\",\n",
    "                regionCode=region_code,\n",
    "                videoCategoryId=category_id,\n",
    "                maxResults=min(50, max_results - len(videos)),\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                duration = isodate.parse_duration(item[\"contentDetails\"][\"duration\"])\n",
    "                duration_in_seconds = duration.total_seconds()  #초로 바꾸기기\n",
    "\n",
    "                if duration_in_seconds > 80:  # 80초 이상의 동영상만 가져오기\n",
    "                    videos.append({\n",
    "                        \"video_id\": item[\"id\"],\n",
    "                        \"title\": item[\"snippet\"][\"title\"],\n",
    "                        \"description\": item[\"snippet\"][\"description\"],\n",
    "                        \"tags\": item[\"snippet\"].get(\"tags\", []),\n",
    "                        \"duration\": str(duration),\n",
    "                        \"view_count\": int(item[\"statistics\"].get(\"viewCount\", 0)),\n",
    "                        \"like_count\": int(item[\"statistics\"].get(\"likeCount\", 0)),\n",
    "                        \"comment_count\": int(item[\"statistics\"].get(\"commentCount\", 0)),\n",
    "                        \"category_id\": category_id,\n",
    "                    })\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching videos: {e}\")\n",
    "            time.sleep(5)  # 잠시 대기 후 다시 시도\n",
    "\n",
    "    return videos\n",
    "\n",
    "# JSON 파일로 저장하는 함수\n",
    "def save_to_json(data, filename):\n",
    "    # data 폴더가 존재하지 않으면 생성\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    \n",
    "    # 파일 경로를 data 폴더 아래로 설정\n",
    "    filepath = os.path.join('data', filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 실행\n",
    "all_videos = {}\n",
    "\n",
    "for category_name, category_id in CATEGORIES.items():\n",
    "    print(f\"Fetching trending videos for category: {category_name}\")\n",
    "    videos = fetch_trending_videos(category_id, region_code=\"KR\", max_results=200)\n",
    "    all_videos[category_name] = videos\n",
    "    print(f\"비디오 {len(videos)} 개 카테고리: {category_name} fetch 완료.\")\n",
    "\n",
    "# 결과를 하나의 JSON 파일로 저장\n",
    "output_file = \"raw_video_data.json\"\n",
    "save_to_json(all_videos, output_file)\n",
    "print(f\"데이터 저장 : data/{output_file}'\")\n",
    "\n",
    "# 결과 출력 예시\n",
    "for category, videos in all_videos.items():\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    for video in videos[:5]:\n",
    "        print(f\" - {video['title']} ({video['video_id']}), 조회수: {video['view_count']} 회, 좋아요: {video['like_count']} 개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미친 듯 날뛰던 이재명. 전한길 등장에 꼭꼭 숨었다 [주말 몰아보기] 영상 ‘좋아요’와 ‘구독’은 큰 힘이 됩니다\n",
      "\n",
      "*‘굿모닝 대한민국’ 주말 몰아보기 영상 목록\n",
      "\n",
      "00:00:00\n",
      "尹 사건 맡은 중앙지법! 대형사건 터졌다![배승희 뉴스배송] [제보 및 비지니스 문의]\n",
      "tatajebo@gmail.com\n",
      "\n",
      "[변호사세요? 유튜버세요? - 도서 구입] \n",
      "교보문고 :\n",
      "이상민 입 열자 尹 '휘청' \"10시 KBS\" 발언의 비밀 [뉴스.zip/MBC뉴스] 00:00 [단독] 이상민 \"尹, '22시 KBS 생방송 있다'며 계엄 강행하려 해\" (202\n",
      "[겸공뉴스특보] 2025년 2월 3일 월요일 #김어준 #겸손은힘들다 #뉴스공장 #명랑사회 #홍사훈 #이재석\n",
      "\n",
      "00:00:00 대기화면\n",
      "00:09:45 홍사훈의 겸공뉴스특보\n",
      "00:0\n",
      "일타강사 전한길 “비상계엄은 100% 계몽령” / 채널A / 뉴스 TOP10 일타강사 전한길 “비상계엄은 100% 계몽령”\n",
      "\n",
      "100만 명 모여달라\"던 전한길, 부산 집회 참석\n",
      "부산\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "# json 파일을 로드하는 함수\n",
    "def load_json(filename):\n",
    "    \"\"\"\n",
    "    현재 디렉토리 내 data 폴더 아래의 JSON 파일을 로드하여 데이터를 반환.\n",
    "    \n",
    "    :param filename: 로드할 JSON 파일의 이름\n",
    "    :return: JSON 데이터\n",
    "    \"\"\"\n",
    "    data_folder = 'data'\n",
    "    file_path = os.path.join(data_folder, filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"파일 '{file_path}'이(가) 존재하지 않습니다.\")\n",
    "        return None\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# video데이터를 병합하는 함수\n",
    "def merge_text_fields(video):\n",
    "    \"\"\"\n",
    "    동영상의 title, description, tags를 하나의 긴 문자열로 병합.\n",
    "    \n",
    "    :param video: 동영상 데이터 딕셔너리\n",
    "    :return: 병합된 문자열\n",
    "    \"\"\"\n",
    "    title = video.get(\"title\", \"\")\n",
    "    description = video.get(\"description\", \"\")\n",
    "    tags = \" \".join(video.get(\"tags\", []))  # 태그 리스트를 공백으로 연결\n",
    "\n",
    "    # 병합\n",
    "    merged_text = \" \".join([title, description, tags])\n",
    "    return merged_text.strip()  # 공백 제거\n",
    "\n",
    "# 카테고리별 동영상 데이터를 병합하는 함수\n",
    "def merge_videos_with_category(data):\n",
    "    \"\"\"\n",
    "    카테고리별 동영상 데이터를 병합하여 텍스트 필드를 생성.\n",
    "    \n",
    "    :param data: JSON 데이터\n",
    "    :return: 병합된 텍스트 리스트\n",
    "    \"\"\"\n",
    "    merged_texts_by_category = {}\n",
    "\n",
    "    for category, videos in data.items():\n",
    "        merged_texts = []\n",
    "        for video in videos:\n",
    "            # 병합된 텍스트 생성\n",
    "            merged_text = merge_text_fields(video)\n",
    "            merged_texts.append(merged_text)\n",
    "            \n",
    "        merged_texts_by_category[category] = merged_texts\n",
    "            \n",
    "    return merged_texts_by_category\n",
    "\n",
    "# JSON 파일 경로\n",
    "input_file_path = \"raw_video_data.json\"\n",
    "\n",
    "# Load JSON data\n",
    "data = load_json(input_file_path)\n",
    "\n",
    "if data:\n",
    "    # Merge text fields\n",
    "    merged_texts_by_category = merge_videos_with_category(data)\n",
    "\n",
    "    # 결과 출력\n",
    "    for item in merged_texts_by_category[\"News & Politics\"][:5]:\n",
    "        print(item[:100])  # 100자까지만 출력\n",
    "\n",
    "save_to_json(merged_texts_by_category, \"merged_text_data.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoNLPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "카테고리: News & Politics\n",
      "['이재명', '전한길', '등장', '꼭꼭', '주말', '보기', '영상', '구독', '굿모닝', '대한민국']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "\n",
    "# 단어 추출\n",
    "def extract_nouns(text):\n",
    "    nouns = okt.nouns(text)\n",
    "    return [n for n in nouns if len(n) > 1]  # 2음절 이상의 명사만 추출\n",
    "\n",
    "# 카테고리별 단어 추출\n",
    "def extract_nouns_by_category(merge_texts_by_category):\n",
    "    nouns_by_category = {}\n",
    "\n",
    "    for category, texts in merged_texts_by_category.items(): #texts는 배열\n",
    "        nouns = []\n",
    "        for text in texts:\n",
    "            nouns.extend(extract_nouns(text))\n",
    "        nouns_by_category[category] = nouns\n",
    "\n",
    "    return nouns_by_category\n",
    "\n",
    "# 실행\n",
    "\n",
    "merged_texts_by_category = load_json(\"merged_text_data.json\")\n",
    "\n",
    "nouns_by_category = extract_nouns_by_category(merged_texts_by_category)\n",
    "\n",
    "# 파일 저장\n",
    "save_to_json(nouns_by_category, \"nouns_by_category.json\")\n",
    "\n",
    "# 결과 출력\n",
    "for category, nouns in nouns_by_category.items():\n",
    "    print(f\"\\n카테고리: {category}\")\n",
    "    print(nouns[:10])  # 처음 10개 단어만 출력\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soynlp\n",
    "\n",
    "word_extractor = soynlp.extractor.WordExtractor()  #훈련 객체\n",
    "\n",
    "word_extractor.train(comments) #훈련\n",
    "\n",
    "word_scores = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 데이터 저장: data/preprocessed_text_data.json\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import emoji # type: ignore\n",
    "import json\n",
    "\n",
    "\n",
    "# 기본 텍스트 정제 함수\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    텍스트 데이터를 전처리하고 문장을 반환합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str): 입력 텍스트 데이터.\n",
    "\n",
    "    Returns:\n",
    "        str: 전처리된 텍스트.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)  # 문자열로 변환\n",
    "\n",
    "    # 1. HTML 태그 제거\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "    # 2. URL 제거\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # 3. 이메일 제거\n",
    "    text = re.sub(r\"\\S+@\\S+\\.\\S+\", \"\", text)\n",
    "\n",
    "    # 4. 숫자 제거 (명확히 숫자만 제거)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # 5. 반복된 ㅋ, ㅎ, ㅠ, ㅜ 등 제거\n",
    "    text = re.sub(r\"[ㅋㅎㅠㅜ]+\", \"\", text)\n",
    "\n",
    "    # 6. 반복된 점(...) 제거\n",
    "    text = re.sub(r\"\\.\\.+\", \".\", text)\n",
    "\n",
    "    # 7. 반복된 문자 축소 (e.g., \"와아아\" -> \"와\")\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\", text)\n",
    "\n",
    "    # 8. 이모지 제거\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "\n",
    "    # 9. 특수문자 및 영어 알파벳 제거\n",
    "    text = re.sub(r\"[^\\w\\s가-힣]\", \"\", text)  # 영어 알파벳 포함 특수문자 제거\n",
    "    text = re.sub(r\"[a-zA-Z]\", \"\", text)     # 영어 알파벳 제거\n",
    "\n",
    "    # 10. 양쪽 공백 제거\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 실행\n",
    "input_file_path = \"merged_text_data.json\"\n",
    "output_file_path = \"preprocessed_text_data.json\"\n",
    "\n",
    "merged_texts_by_category = load_json(input_file_path)\n",
    "# 카테고리별로 텍스트 전처리\n",
    "preprocessed_texts_by_category = {}\n",
    "\n",
    "for category, texts in merged_texts_by_category.items():\n",
    "    preprocessed_texts = [preprocess_text(text) for text in texts]\n",
    "    preprocessed_texts_by_category[category] = preprocessed_texts\n",
    "\n",
    "# 결과를 JSON 파일로 저장\n",
    "save_to_json(preprocessed_texts_by_category, output_file_path)\n",
    "print(f\"전처리된 데이터 저장: data/{output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract KeyWords as Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리된 데이터가 'test_data/keyword.json'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_ngrams(texts, n=2):\n",
    "    \"\"\"\n",
    "    텍스트에서 N-그램 기반 구 단위 키워드 추출.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): 텍스트 리스트.\n",
    "        n (int): N-그램 크기 (2: bigram, 3: trigram).\n",
    "\n",
    "    Returns:\n",
    "        dict: N-그램과 빈도수 딕셔너리.\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n))\n",
    "    ngram_matrix = vectorizer.fit_transform(texts)\n",
    "    ngram_counts = ngram_matrix.sum(axis=0).A1\n",
    "    ngram_vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # numpy.int64 -> int 변환\n",
    "    return {ngram: int(count) for ngram, count in zip(ngram_vocab, ngram_counts)}\n",
    "\n",
    "\n",
    "def extract_keyword_from_file(input_file, output_file, n=2):\n",
    "    \"\"\"\n",
    "    문자열과 문자열 리스트가 섞인 데이터를 처리하여 결과를 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): 입력 파일 경로.\n",
    "        output_file (str): 출력 파일 경로.\n",
    "        n (int): N-그램 크기.\n",
    "    \"\"\"\n",
    "    # 파일 로드\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 문자열 리스트로 변환 (리스트나 문자열 모두 처리)\n",
    "    texts = []\n",
    "    for item in data:\n",
    "        if isinstance(item, list):  # 문자열 리스트인 경우 병합\n",
    "            texts.append(\" \".join(item))\n",
    "        elif isinstance(item, str):  # 단일 문자열인 경우 그대로 추가\n",
    "            texts.append(item)\n",
    "\n",
    "    # N-그램 키워드 추출\n",
    "    ngram_keywords = extract_ngrams(texts, n=n)\n",
    "\n",
    "    # 결과 저장\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(ngram_keywords, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 입력 및 출력 파일 경로\n",
    "    input_file = \"test_data/cleaned.json\"  # 문자열과 문자열 리스트가 섞인 txt 파일\n",
    "    output_file = \"test_data/keyword.json\"\n",
    "\n",
    "    # 텍스트 파일 처리 (2-그램 추출)\n",
    "    extract_keyword_from_file(input_file, output_file, n=2)\n",
    "    print(f\"처리된 데이터가 '{output_file}'에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 및 1번 등장 키워드 제외 후 결과가 'test_data/filtered_keywords.json'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# def load_stopwords(filepath):\n",
    "#     \"\"\"\n",
    "#     불용어 리스트를 로드합니다.\n",
    "\n",
    "#     Args:\n",
    "#         filepath (str): 불용어 파일 경로.\n",
    "\n",
    "#     Returns:\n",
    "#         set: 불용어 집합.\n",
    "#     \"\"\"\n",
    "#     with open(filepath, 'r', encoding='utf-8') as f:\n",
    "#         stopwords = {line.strip() for line in f}\n",
    "#     return stopwords\n",
    "\n",
    "\n",
    "# def split_filter_and_count_keywords(keywords, stopwords):\n",
    "#     \"\"\"\n",
    "#     키워드를 단어로 나누고, 불용어를 제거하며 1번만 등장한 키워드를 필터링합니다.\n",
    "\n",
    "#     Args:\n",
    "#         keywords (dict): 키워드와 점수로 이루어진 딕셔너리.\n",
    "#         stopwords (set): 불용어 집합.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: 불용어 제거 및 1번만 등장한 키워드 제거 후 결과.\n",
    "#     \"\"\"\n",
    "#     filtered_keywords = {}\n",
    "\n",
    "#     for key, value in keywords.items():\n",
    "#         # 1. 키워드를 공백으로 분리하여 단어 리스트 생성\n",
    "#         words = key.split()\n",
    "\n",
    "#         # 2. 단어들 중 불용어가 포함되어 있는지 확인\n",
    "#         if any(word in stopwords for word in words):\n",
    "#             continue  # 불용어가 포함된 경우 제거\n",
    "        \n",
    "    \n",
    "#     # 단어 분리 및 출현 횟수 계산\n",
    "#     word_counts = Counter()\n",
    "#     for phrase, count in keywords.items():\n",
    "#     words = phrase.split()  # 띄어쓰기 기준으로 단어 분리\n",
    "#     word_counts.update({word: count for word in words})\n",
    "       \n",
    "#     return filtered_keywords\n",
    "\n",
    "\n",
    "# def process_keywords(input_file, stopwords_file, output_file):\n",
    "#     \"\"\"\n",
    "#     키워드 파일에서 불용어를 제거하고 1번 등장한 키워드를 제외한 결과를 저장합니다.\n",
    "\n",
    "#     Args:\n",
    "#         input_file (str): 입력 키워드 파일 경로.\n",
    "#         stopwords_file (str): 불용어 파일 경로.\n",
    "#         output_file (str): 결과 저장 경로.\n",
    "#     \"\"\"\n",
    "#     # 불용어 로드\n",
    "#     stopwords = load_stopwords(stopwords_file)\n",
    "\n",
    "#     # 키워드 파일 로드\n",
    "#     with open(input_file, 'r', encoding='utf-8') as f:\n",
    "#         keywords = json.load(f)\n",
    "\n",
    "#     # 불용어 제거 및 1번 등장한 키워드 필터링\n",
    "#     filtered_keywords = split_filter_and_count_keywords(keywords, stopwords)\n",
    "\n",
    "#     # 결과 저장\n",
    "#     with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(filtered_keywords, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "#     print(f\"불용어 제거 및 1번 등장 키워드 제외 후 결과가 '{output_file}'에 저장되었습니다.\")\n",
    "\n",
    "\n",
    "# # 실행\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 파일 경로 설정\n",
    "#     input_file = \"test_data/keyword.json\"  # 키워드 추출 결과 파일\n",
    "#     stopwords_file = \"test_data/stopwords-ko.txt\"  # 불용어 리스트 파일\n",
    "#     output_file = \"test_data/filtered_keywords.json\"  # 결과 저장 파일\n",
    "\n",
    "#     # 키워드 필터링 수행\n",
    "#     process_keywords(input_file, stopwords_file, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
