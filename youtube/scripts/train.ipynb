{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ì´ 840212ê°œì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from soynlp.noun import LRNounExtractor\n",
    "\n",
    "# ğŸ“Œ JSON íŒŒì¼ì´ ë“¤ì–´ìˆëŠ” í´ë” ê²½ë¡œ (ì‚¬ìš©ìì˜ í´ë” ê²½ë¡œë¡œ ìˆ˜ì •)\n",
    "folder_path = \"../../../ê²Œì„\"\n",
    "\n",
    "# ğŸ“Œ í´ë” ë‚´ ëª¨ë“  JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (CODE0001.json, CODE0002.json, ...)\n",
    "json_files = glob.glob(os.path.join(folder_path, \"BO*.json\"))\n",
    "\n",
    "# ğŸ“Œ ëª¨ë“  íŒŒì¼ì—ì„œ ëŒ“ê¸€(content)ë§Œ ì¶”ì¶œ\n",
    "comments = []\n",
    "for file in json_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)  # JSON íŒŒì¼ ë¡œë“œ\n",
    "\n",
    "        # 'text' í•­ëª©ì—ì„œ 'content'ë§Œ ì¶”ì¶œ\n",
    "        for entry in data[\"SJML\"][\"text\"]:\n",
    "            comments.append(entry[\"content\"])\n",
    "\n",
    "print(f\"ğŸ“Œ ì´ {len(comments)}ê°œì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ ì „ì²˜ë¦¬ í›„ 840212ê°œì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['7ë¹  ì•¼íš¨', 'ì–‘ê²½ì²  ì–‘ê²½ì²  hello', 'êµ¬ë¦„ë¹µ how sit goi bff', 'ìœ ë¦¼ hi', '1ë¹ ë‹¤ ë”°ì„ë“¤ì•„']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# ğŸ“Œ ê´„í˜¸ì™€ ê´„í˜¸ ì•ˆì˜ ë‚´ìš©ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜\n",
    "def remove_parentheses(text):\n",
    "    return re.sub(r'\\([^)]*\\)', '', text)\n",
    "\n",
    "# ğŸ“Œ ëª¨ë“  ëŒ“ê¸€ì— ëŒ€í•´ ê´„í˜¸ì™€ ê´„í˜¸ ì•ˆì˜ ë‚´ìš©ì„ ì œê±°\n",
    "comments = [remove_parentheses(comment) for comment in comments]\n",
    "\n",
    "print(f\"ğŸ“Œ ì „ì²˜ë¦¬ í›„ {len(comments)}ê°œì˜ ëŒ“ê¸€ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "comments[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í…ìŠ¤íŠ¸ ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ë°ì´í„° ê°œìˆ˜: 30000\n",
      "í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°œìˆ˜: 420106\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# ğŸ“Œ ë°ì´í„° ì…”í”Œë§ (ë¬´ì‘ìœ„ ìˆœì„œë¡œ ì„ê¸°)\n",
    "random.shuffle(comments)\n",
    "\n",
    "# ğŸ“Œ 50:50 ë°ì´í„° ë¶„í•  (42ë§Œ ê°œì”©)\n",
    "split_index = len(comments) // 2\n",
    "train_data = comments[:30000]  # í•™ìŠµ ë°ì´í„° 42ë§Œ ê°œ\n",
    "test_data = comments[split_index:]  # í…ŒìŠ¤íŠ¸ ë°ì´í„° 42ë§Œ ê°œ\n",
    "\n",
    "# ğŸ“Œ ë°ì´í„° ê°œìˆ˜ í™•ì¸\n",
    "print(f\"í•™ìŠµ ë°ì´í„° ê°œìˆ˜: {len(train_data)}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°œìˆ˜: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] used default noun predictor; Sejong corpus predictor\n",
      "[Noun Extractor] used noun_predictor_sejong\n",
      "[Noun Extractor] All 2398 r features was loaded\n",
      "[Noun Extractor] scanning was done (L,R) has (2402, 1459) tokens\n",
      "[Noun Extractor] building L-R graph was done\n",
      "[Noun Extractor] 573 nouns are extracted\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ğŸ“Œ nounextractor í•™ìŠµ\n",
    "noun_extractor = LRNounExtractor()\n",
    "nouns = noun_extractor.train_extract(\n",
    "    train_data,\n",
    "    min_noun_score=0.3,\n",
    "    min_noun_frequency=20\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ë•Œë¬¸', NounScore_v1(frequency=78, score=0.9997221142857142, known_r_ratio=0.9333333333333333)), ('ë„ì›€', NounScore_v1(frequency=18, score=0.968345625, known_r_ratio=1.0)), ('ì´ë‘', NounScore_v1(frequency=274, score=0.7998286, known_r_ratio=0.7142857142857143)), ('êµ­ë¯¼', NounScore_v1(frequency=6, score=0.8774915135135136, known_r_ratio=0.8705882352941177)), ('ë‚´ê°€', NounScore_v1(frequency=173, score=0.9913243333333334, known_r_ratio=0.6)), ('ë‹‰ë„¤', NounScore_v1(frequency=32, score=0.999185, known_r_ratio=0.03125)), ('ìì£¼', NounScore_v1(frequency=42, score=0.9887579999999999, known_r_ratio=0.75)), ('ë–¡êµ­', NounScore_v1(frequency=22, score=0.6561567777777778, known_r_ratio=0.9)), ('ë­ë¼', NounScore_v1(frequency=14, score=0.48428875, known_r_ratio=1.0)), ('ì»¨ì…‰', NounScore_v1(frequency=32, score=0.9103542222222223, known_r_ratio=0.72))]\n"
     ]
    }
   ],
   "source": [
    "# Extract 10 nouns from the trained model\n",
    "top_10_nouns = list(nouns.items())[:10]\n",
    "print(top_10_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ê¿€ì¼ (0.99)    ë„ˆë¬´ (0.57)    ì‚¬ë‘ (0.97)    ì§„ì§œ (0.70)    ì œê°€ (0.99)\n",
      "   ì¢‹ì•„ìš” (0.54)    ê°ì‚¬ (0.94)    ì˜¤ëŠ˜ (0.33)   ë§Œì•½ì— (0.66)    ì •ë§ (0.81)\n",
      "    ë‹¤ìŒ (1.00)   ëª©ì†Œë¦¬ (0.73)    ì¶•í•˜ (0.77)    ì¬ë¯¸ (0.48)    ì²˜ìŒ (1.00)\n",
      "    ì´ë‘ (0.80)   ì˜¤ëœë§Œ (0.96)    ì¢‹ì•„ (0.61)    ìƒì¼ (0.76)   ì½”ì•„ë‹˜ (0.51)\n",
      "    ë³´ê³  (0.42)    ì™„ì „ (1.00)    ë§ˆí¬ (0.77)    ì•ˆë…• (1.00)    ë‚´ê°€ (0.99)\n",
      "   ì–´ë–»ê²Œ (0.85)    ê²Œì„ (0.61)    ë…¸ë˜ (0.53)   ë§ˆì§€ë§‰ (0.93)   ì´ë ‡ê²Œ (1.00)\n",
      "    ëŒ“ê¸€ (0.63)   ì¹œêµ¬ë“¤ (0.94)    í•˜íŠ¸ (0.87)  ìƒŒë“œë°•ìŠ¤ (0.70)    ì œì¼ (1.00)\n",
      "    ëŒ€ë°• (0.86)    ë‚˜ì´ (0.81)   êµ¬ë…ì (0.76)    ì‘ì› (0.99)    ìƒê° (0.75)\n",
      "   ìœ íŠœë¸Œ (0.66)    ìŠ¤í‚¨ (0.66)    í•˜ì§€ (1.00)    ì¸ì • (0.83)    ì£„ì†¡ (0.94)\n",
      "    ì ëœ° (0.51)   í™”ì´íŒ… (0.69)    ì—„ë§ˆ (0.83)    í•˜ê³  (0.50)    ê°™ì´ (0.80)\n",
      "   ìˆëŠ”ë° (1.00)    í•œë²ˆ (0.69)    ì—„ì²­ (0.62)  ë¡œë¸”ë¡ìŠ¤ (0.65)    í•™êµ (0.83)\n",
      "   ì˜¤í”„ë‹ (0.58)   ë´¤ëŠ”ë° (1.00)    ì‹œê°„ (0.80)   ê¾¸ë¥´ì¼ (1.00)    ì‚¬ëŒ (0.44)\n",
      "    ë„˜ë‚  (0.75)   32ë¶„ (1.00)    ì¹œêµ¬ (0.69)    ë¶€íƒ (0.96)   ë„ë„í•œ (0.94)\n",
      "    ë³´ë‹ˆ (1.00)    ì´ë²ˆ (0.90)    ì´ë¦„ (0.38)    ë¨¸ë¦¬ (0.97)   ì—´ì‹¬íˆ (1.00)\n",
      "    ë‚´ì¼ (0.63)    ìƒì‹  (0.82)    ê³ ì • (0.60)   ê°‘ìê¸° (1.00)    ì í”„ (0.91)\n",
      "    ë•Œë¬¸ (1.00)    ì†Œë¦¬ (0.75)    ì¡°ì‹¬ (1.00)    ìš”ì¦˜ (0.54)    ê¸°ëŒ€ (0.92)\n",
      "    ë‚˜ì¤‘ (1.00)    ê±´ê°• (0.95)   ë‹¤ì´ì•„ (0.76)   ê·¸ë ‡ê²Œ (0.98)    ì½”ì•„ (0.52)\n",
      "    ê³„ì† (0.42)    ì¶”ì–µ (0.83)    í•˜ë£¨ (0.82)    ë¼ê³  (1.00)    ë°©ì†¡ (0.35)\n",
      "    í• ë•Œ (0.58)ë„ë„í•œì¹œêµ¬ë“¤ (0.97)    ë¶„ë“¤ (0.86)    í•˜ë‚˜ (0.56)    ëŒ€ë‹¨ (0.99)\n",
      "    ì§„ì‹¬ (1.00)    ëª¨ë“œ (0.75)    í–‰ë³µ (0.96)    ë¨¹ê³  (1.00)   êµ­ë¯¼ì´ (0.67)"
     ]
    }
   ],
   "source": [
    "top100 = sorted(nouns.items(), \n",
    "    key=lambda x:-x[1].frequency * x[1].score)[:100]\n",
    "\n",
    "for i, (word, score) in enumerate(top100):\n",
    "    if i % 5 == 0:\n",
    "        print()\n",
    "    print('%6s (%.2f)' % (word, score.score), end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
