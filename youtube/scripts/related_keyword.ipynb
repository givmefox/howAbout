{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ì—°ê´€í‚¤ì›Œë“œ ì°¾ëŠ” ì½”ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# JSON ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "def load_json(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "input_file_path = \"data/cleaned_video_comments.json\"\n",
    "data = load_json(input_file_path)\n",
    "\n",
    "# ëª¨ë“  ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "all_comments = [comment for category in data.values() for video in category for comment in video[\"comments\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Binary n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ ë‘ ê¸€ìž ì´ìƒ ëª…ì‚¬ë§Œ í¬í•¨ëœ ìƒìœ„ 10ê°œ ì—°ê´€ í‚¤ì›Œë“œ:\n",
      "('ë¶€ì •', 'ì„ ê±°') -> 619íšŒ ë“±ìž¥\n",
      "('ìŒì£¼', 'ìš´ì „') -> 460íšŒ ë“±ìž¥\n",
      "('ì˜ìƒ', 'ê°ì‚¬') -> 432íšŒ ë“±ìž¥\n",
      "('ì •ë³´', 'ê°ì‚¬') -> 407íšŒ ë“±ìž¥\n",
      "('íƒ„í•µ', 'ë°˜ëŒ€') -> 322íšŒ ë“±ìž¥\n",
      "('ê³ ì¸', 'ëª…ë³µ') -> 321íšŒ ë“±ìž¥\n",
      "('ìžìœ ', 'ë¯¼ì£¼ì£¼ì˜') -> 210íšŒ ë“±ìž¥\n",
      "('ëŒ€í†µë ¹', 'íƒ„í•µ') -> 186íšŒ ë“±ìž¥\n",
      "('íƒ„í•µ', 'ë¬´íš¨') -> 160íšŒ ë“±ìž¥\n",
      "('êµ­íšŒ', 'ì˜ì›') -> 157íšŒ ë“±ìž¥\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# JSON ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "def load_json(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„ê¸° ê°ì²´ ìƒì„±\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# 2ê¸€ìž ì´ìƒ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜\n",
    "def extract_nouns(text):\n",
    "    nouns = [word for word, tag, _, _ in kiwi.analyze(text)[0][0] if tag.startswith(\"NNG\") and len(word) > 1]\n",
    "    return nouns\n",
    "\n",
    "# N-gram ì¶”ì¶œ í•¨ìˆ˜ (ëª…ì‚¬ë§Œ í™œìš©)\n",
    "def get_noun_ngrams(texts, n=2):\n",
    "    all_ngrams = []\n",
    "    for text in texts:\n",
    "        nouns = extract_nouns(text)  # ëª…ì‚¬ë§Œ ì¶”ì¶œ\n",
    "        if len(nouns) >= n:  # N-gramì„ ë§Œë“¤ê¸° ìœ„í•´ ìµœì†Œ nê°œ ë‹¨ì–´ í•„ìš”\n",
    "            all_ngrams.extend(ngrams(nouns, n))\n",
    "    \n",
    "    return Counter(all_ngrams)  # N-gram ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "\n",
    "# JSON ë°ì´í„° ë¡œë“œ\n",
    "input_file_path = \"data/cleaned_video_comments.json\"\n",
    "data = load_json(input_file_path)\n",
    "\n",
    "# ëª¨ë“  ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "all_comments = [comment for category in data.values() for video in category for comment in video[\"comments\"]]\n",
    "\n",
    "# Bigram(2-gram) ë¶„ì„ ì‹¤í–‰ (ëª…ì‚¬ë§Œ ì‚¬ìš©)\n",
    "bigram_counter = get_noun_ngrams(all_comments, n=2)\n",
    "\n",
    "# ìƒìœ„ 10ê°œ ì—°ê´€ í‚¤ì›Œë“œ ì¶œë ¥\n",
    "print(\"ðŸ“Œ ë‘ ê¸€ìž ì´ìƒ ëª…ì‚¬ë§Œ í¬í•¨ëœ ìƒìœ„ 10ê°œ ì—°ê´€ í‚¤ì›Œë“œ:\")\n",
    "for ngram, freq in bigram_counter.most_common(10):\n",
    "    print(f\"{ngram} -> {freq}íšŒ ë“±ìž¥\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tri n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ ë‘ ê¸€ìž ì´ìƒ ëª…ì‚¬ë§Œ í¬í•¨ëœ ìƒìœ„ 10ê°œ ì—°ê´€ í‚¤ì›Œë“œ:\n",
      "('ë¶€ì •', 'ì„ ê±°') -> 619íšŒ ë“±ìž¥\n",
      "('ìŒì£¼', 'ìš´ì „') -> 460íšŒ ë“±ìž¥\n",
      "('ì˜ìƒ', 'ê°ì‚¬') -> 432íšŒ ë“±ìž¥\n",
      "('ì •ë³´', 'ê°ì‚¬') -> 407íšŒ ë“±ìž¥\n",
      "('íƒ„í•µ', 'ë°˜ëŒ€') -> 322íšŒ ë“±ìž¥\n",
      "('ê³ ì¸', 'ëª…ë³µ') -> 321íšŒ ë“±ìž¥\n",
      "('ìžìœ ', 'ë¯¼ì£¼ì£¼ì˜') -> 210íšŒ ë“±ìž¥\n",
      "('ëŒ€í†µë ¹', 'íƒ„í•µ') -> 186íšŒ ë“±ìž¥\n",
      "('íƒ„í•µ', 'ë¬´íš¨') -> 160íšŒ ë“±ìž¥\n",
      "('êµ­íšŒ', 'ì˜ì›') -> 157íšŒ ë“±ìž¥\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# JSON ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "def load_json(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„ê¸° ê°ì²´ ìƒì„±\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# 2ê¸€ìž ì´ìƒ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜\n",
    "def extract_nouns(text):\n",
    "    nouns = [word for word, tag, _, _ in kiwi.analyze(text)[0][0] if tag.startswith(\"NNG\") and len(word) > 1]\n",
    "    return nouns\n",
    "\n",
    "# N-gram ì¶”ì¶œ í•¨ìˆ˜ (ëª…ì‚¬ë§Œ í™œìš©)\n",
    "def get_noun_ngrams(texts, n=2):\n",
    "    all_ngrams = []\n",
    "    for text in texts:\n",
    "        nouns = extract_nouns(text)  # ëª…ì‚¬ë§Œ ì¶”ì¶œ\n",
    "        if len(nouns) >= n:  # N-gramì„ ë§Œë“¤ê¸° ìœ„í•´ ìµœì†Œ nê°œ ë‹¨ì–´ í•„ìš”\n",
    "            all_ngrams.extend(ngrams(nouns, n))\n",
    "    \n",
    "    return Counter(all_ngrams)  # N-gram ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "\n",
    "# JSON ë°ì´í„° ë¡œë“œ\n",
    "input_file_path = \"data/cleaned_video_comments.json\"\n",
    "data = load_json(input_file_path)\n",
    "\n",
    "# ëª¨ë“  ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "all_comments = [comment for category in data.values() for video in category for comment in video[\"comments\"]]\n",
    "\n",
    "trigram_counter = get_noun_ngrams(all_comments, n=3)\n",
    "\n",
    "# ìƒìœ„ 10ê°œ ì—°ê´€ í‚¤ì›Œë“œ ì¶œë ¥\n",
    "print(\"ðŸ“Œ ë‘ ê¸€ìž ì´ìƒ ëª…ì‚¬ë§Œ í¬í•¨ëœ ìƒìœ„ 10ê°œ ì—°ê´€ í‚¤ì›Œë“œ:\")\n",
    "for ngram, freq in bigram_counter.most_common(10):\n",
    "    print(f\"{ngram} -> {freq}íšŒ ë“±ìž¥\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì—°ê´€í‚¤ì›Œë“œ ì¶”ì¶œì€ ìž˜ ë˜ëŠ”ê±° ê°™ì•„. ê·¸ëŸ¬ë©´ ì´ì œ ëª©í‘œë¥¼ ì¡°ê¸ˆ ë°”ê¿€ê²Œ.\n",
    "final_keywords.jsonì— í‚¤ì›Œë“œëž­í‚¹ì´ ë‚˜ì˜¤ìž–ì•„. \n",
    "ì´ì œ ê·¸ ëž­í‚¹ì— ìžˆëŠ” ê° í‚¤ì›Œë“œë“¤ì˜ ì—°ê´€í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ í•´ë‚´ê³  ì‹¶ì–´.\n",
    "ìœ„ì—ì„œ ìž‘ë™í•œ ë°©ì‹ëŒ€ë¡œ ì½”ë”©ì„ í•´ì„œ íŠ¹ì • í‚¤ì›Œë“œì˜ ì—°ê´€í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ í•´ë‚¼ ìˆ˜ ìžˆì–´?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#íŠ¹ì •í‚¤ì›Œë“œì˜ ì—°ê´€ í‚¤ì›Œë“œ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ 'ë¡œë¸”ë¡ìŠ¤'ì™€ í•¨ê»˜ ìžì£¼ ë“±ìž¥í•˜ëŠ” ì—°ê´€ í‚¤ì›Œë“œ:\n",
      "('ë¡œë¸”ë¡ìŠ¤', 'ìœ íŠœë²„') -> 15íšŒ ë“±ìž¥\n",
      "('ìœ íŠœë²„', 'ë¡œë¸”ë¡ìŠ¤') -> 12íšŒ ë“±ìž¥\n",
      "('ê´´ë¬¼', 'ë¡œë¸”ë¡ìŠ¤') -> 11íšŒ ë“±ìž¥\n",
      "('ë¡œë¸”ë¡ìŠ¤', 'ì‹œë¦¬ì¦ˆ') -> 4íšŒ ë“±ìž¥\n",
      "('ë¡œë¸”ë¡ìŠ¤', 'ê²Œìž„') -> 4íšŒ ë“±ìž¥\n",
      "('ë¡œë¸”ë¡ìŠ¤', 'í”¼ì‰¬') -> 2íšŒ ë“±ìž¥\n",
      "('ë¡œë¸”ë¡ìŠ¤', 'ê³µí¬') -> 1íšŒ ë“±ìž¥\n",
      "('ê²œê¸‰ë™', 'ë¡œë¸”ë¡ìŠ¤') -> 1íšŒ ë“±ìž¥\n",
      "('ë¡œë¸”ë¡ìŠ¤', 'í–‰ë³µ') -> 1íšŒ ë“±ìž¥\n",
      "('ë¡œë¸”ë¡ìŠ¤', 'ì°¨ì„¸ëŒ€') -> 1íšŒ ë“±ìž¥\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# JSON ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "def load_json(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# N-gram ì¶”ì¶œ í•¨ìˆ˜ (ì´ë¯¸ ëª…ì‚¬ë§Œ ì €ìž¥ëœ ë°ì´í„° ì‚¬ìš©)\n",
    "def get_noun_ngrams(nouns_data, n=2):\n",
    "    all_ngrams = []\n",
    "    for category in nouns_data.values():\n",
    "        for video in category:\n",
    "            for nouns in video[\"nouns\"]:  # ëŒ“ê¸€ë³„ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©\n",
    "                if len(nouns) >= n:  # ìµœì†Œ nê°œ ë‹¨ì–´ í•„ìš”\n",
    "                    all_ngrams.extend(ngrams(nouns, n))\n",
    "    \n",
    "    return Counter(all_ngrams)  # N-gram ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "\n",
    "# íŠ¹ì • í‚¤ì›Œë“œì™€ ì—°ê´€ëœ N-gram ì°¾ê¸°\n",
    "def find_related_keywords(ngrams_counter, target_word):\n",
    "    related_words = []\n",
    "    for ngram, freq in ngrams_counter.items():\n",
    "        if target_word in ngram:  # íŠ¹ì • í‚¤ì›Œë“œê°€ í¬í•¨ëœ N-gram ì°¾ê¸°\n",
    "            related_words.append((ngram, freq))\n",
    "\n",
    "    return sorted(related_words, key=lambda x: x[1], reverse=True)  # ë¹ˆë„ìˆœ ì •ë ¬\n",
    "\n",
    "# JSON ë°ì´í„° ë¡œë“œ\n",
    "input_file_path = \"data/kiwi_nouns_video_comments.json\"\n",
    "data = load_json(input_file_path)\n",
    "\n",
    "# Bigram(2-gram) ë¶„ì„ ì‹¤í–‰ (ëª…ì‚¬ë§Œ ì‚¬ìš©)\n",
    "bigram_counter = get_noun_ngrams(data, n=2)\n",
    "\n",
    "# ðŸ”¥ íŠ¹ì • í‚¤ì›Œë“œ ìž…ë ¥ í›„ ì—°ê´€ í‚¤ì›Œë“œ ì°¾ê¸°\n",
    "target_word = \"ë¡œë¸”ë¡ìŠ¤\"  # ì˜ˆì œ í‚¤ì›Œë“œ\n",
    "related_keywords = find_related_keywords(bigram_counter, target_word)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ðŸ“Œ '{target_word}'ì™€ í•¨ê»˜ ìžì£¼ ë“±ìž¥í•˜ëŠ” ì—°ê´€ í‚¤ì›Œë“œ:\")\n",
    "for ngram, freq in related_keywords[:10]:  # ìƒìœ„ 10ê°œ ì¶œë ¥\n",
    "    print(f\"{ngram} -> {freq}íšŒ ë“±ìž¥\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
