{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# JSON 저장 함수\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"\n",
    "    데이터를 JSON 파일로 저장합니다.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# JSON 파일에서 명사 리스트를 읽는 함수\n",
    "def load_json(file_path):\n",
    "    \"\"\"\n",
    "    JSON 파일에서 데이터를 불러옵니다.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): JSON 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "        list: JSON 파일의 데이터.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching trending videos for category: News & Politics\n",
      "Fetched 45 videos for category: News & Politics\n",
      "Data saved to 'trending_videos_with_channels.json'\n",
      "\n",
      "Category: News & Politics\n",
      " - [윤석열 체포 LIVE] 겸손은힘들다 뉴스공장 + 겸공뉴스특보 1월 15일 (oD_d0a0HOZU), Channel: 김어준의 겸손은힘들다 뉴스공장 (2010000 subscribers)\n",
      " - 조직적인 폭도와 배후 총정리! (박은정,김경호,신장식,양지열) | 풀버전 (IzEdXoIigoM), Channel: [팟빵] 매불쇼 (2210000 subscribers)\n",
      " - 현직 대통령 첫 체포‥ 계엄 선포 43일 만 - [LIVE] MBC 뉴스특보 2025년 01월 15일 (8XQBluUq3bM), Channel: MBCNEWS (5320000 subscribers)\n",
      " - 정청래도 처음 듣는 소식 \"영장 판사 방만 의도적 파손\" 큰 충격 받은 천대엽 법원행정처장.. (39IL4c5TUgA), Channel: 엠키타카 MKTK (614000 subscribers)\n",
      " - 명태균 카톡 담은 '검찰 수사보고서' 전면 공개 - 뉴스타파 (ra2Z7sWAa9E), Channel: 뉴스타파 Newstapa (1530000 subscribers)\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build # type: ignore\n",
    "import isodate  # ISO 8601 형식의 시간을 파싱하는 라이브러리\n",
    "\n",
    "# YouTube API 설정\n",
    "API_KEY = \"AIzaSyDKlMcyK3peJ-woNyOLuMt5MCW_uZpOL0g\"  # 자신의 YouTube API 키를 입력하세요\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# 카테고리 ID 설정\n",
    "CATEGORIES = {\n",
    "    \"News & Politics\": \"25\"\n",
    "}\n",
    "\n",
    "# 채널 정보 가져오기\n",
    "def fetch_channel_info(channel_id):\n",
    "    \"\"\"\n",
    "    채널 ID를 사용해 채널 정보를 가져옵니다.\n",
    "    \"\"\"\n",
    "    request = youtube.channels().list(\n",
    "        part=\"snippet,statistics\",\n",
    "        id=channel_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    if response[\"items\"]:\n",
    "        channel = response[\"items\"][0]\n",
    "        return {\n",
    "            \"channel_id\": channel[\"id\"],\n",
    "            \"channel_title\": channel[\"snippet\"][\"title\"],\n",
    "            \"channel_description\": channel[\"snippet\"][\"description\"],\n",
    "            \"subscriber_count\": int(channel[\"statistics\"].get(\"subscriberCount\", 0))\n",
    "        }\n",
    "    return {}\n",
    "\n",
    "# 동영상 데이터 가져오기\n",
    "def fetch_trending_videos(category_id, region_code=\"KR\", max_results=200):\n",
    "    \"\"\"\n",
    "    특정 카테고리의 인기 급상승 동영상을 가져옵니다.\n",
    "    \"\"\"\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "\n",
    "    # page token을 통해 api 추가 호출 (총 4번) 4점점\n",
    "    while len(videos) < max_results:\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,statistics,contentDetails\",\n",
    "            chart=\"mostPopular\",\n",
    "            regionCode=region_code,\n",
    "            videoCategoryId=category_id,\n",
    "            maxResults=min(50, max_results - len(videos)),\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # 결과에서 동영상 정보 저장\n",
    "        for item in response.get(\"items\", []):\n",
    "            # 동영상 길이 확인\n",
    "            duration = isodate.parse_duration(item[\"contentDetails\"][\"duration\"])\n",
    "            duration_in_seconds = duration.total_seconds()\n",
    "\n",
    "            if duration_in_seconds >= 90:  # 1분 30초 이상인 경우만 저장\n",
    "                channel_id = item[\"snippet\"][\"channelId\"]\n",
    "                channel_info = fetch_channel_info(channel_id)\n",
    "\n",
    "                videos.append({\n",
    "                    \"video_id\": item[\"id\"],\n",
    "                    \"title\": item[\"snippet\"][\"title\"],\n",
    "                    \"description\": item[\"snippet\"][\"description\"],\n",
    "                    \"tags\": item[\"snippet\"].get(\"tags\", []),  # 태그\n",
    "                    \"duration\": str(duration),  # 동영상 길이\n",
    "                    \"view_count\": int(item[\"statistics\"].get(\"viewCount\", 0)),  # 조회수\n",
    "                    \"like_count\": int(item[\"statistics\"].get(\"likeCount\", 0)),  # 좋아요\n",
    "                    \"comment_count\": int(item[\"statistics\"].get(\"commentCount\", 0)),    # 댓글 수수\n",
    "                    \"category_id\": category_id,     # 카테고리 아이디디\n",
    "                    \"channel_info\": channel_info  # 채널 정보 추가\n",
    "                })\n",
    "\n",
    "        # 다음 페이지 토큰\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return videos\n",
    "\n",
    "\n",
    "\n",
    "# 실행\n",
    "all_videos = {}\n",
    "\n",
    "for category_name, category_id in CATEGORIES.items():\n",
    "    print(f\"Fetching trending videos for category: {category_name}\")\n",
    "    videos = fetch_trending_videos(category_id, region_code=\"KR\", max_results=200)\n",
    "    all_videos[category_name] = videos\n",
    "    print(f\"Fetched {len(videos)} videos for category: {category_name}\")\n",
    "\n",
    "# 결과를 JSON 파일로 저장\n",
    "output_file = \"trending_videos_with_channels.json\"\n",
    "save_to_json(all_videos, output_file)\n",
    "print(f\"Data saved to '{output_file}'\")\n",
    "\n",
    "# 결과 출력 예시\n",
    "for category, videos in all_videos.items():\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    for video in videos[:5]:  # 각 카테고리에서 상위 5개 출력\n",
    "        channel = video[\"channel_info\"]\n",
    "        print(f\" - {video['title']} ({video['video_id']}), Channel: {channel['channel_title']} ({channel['subscriber_count']} subscribers)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'merged_texts.txt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 병합 함수 정의\n",
    "def merge_text_fields(video):\n",
    "    \"\"\"\n",
    "    주어진 동영상 데이터에서 필요한 텍스트 필드를 병합하여 하나의 문자열로 반환.\n",
    "    \"\"\"\n",
    "    # 개별 필드 추출\n",
    "    title = video.get(\"title\", \"\")\n",
    "    description = video.get(\"description\", \"\")\n",
    "    tags = \" \".join(video.get(\"tags\", []))  # 태그 리스트를 공백으로 연결\n",
    "\n",
    "    # 병합\n",
    "    merged_text = \" \".join([title, description, tags])\n",
    "    return merged_text.strip()  # 공백 제거\n",
    "\n",
    "# JSON 데이터에서 각 동영상의 텍스트 필드를 병합\n",
    "def preprocess_and_merge(data):\n",
    "    \"\"\"\n",
    "    카테고리별 동영상 데이터를 병합하여 텍스트 필드를 생성.\n",
    "    \"\"\"\n",
    "    merged_texts = []\n",
    "\n",
    "    for category, videos in data.items():\n",
    "        for video in videos:\n",
    "            # 병합된 텍스트 생성\n",
    "            merged_text = merge_text_fields(video)\n",
    "            merged_texts.append({\"category\": category, \"merged_text\": merged_text})\n",
    "\n",
    "    return merged_texts\n",
    "\n",
    "# 병합 결과 JSON 저장 함수\n",
    "def save_merged_texts(data, output_file):\n",
    "    \"\"\"\n",
    "    병합된 텍스트 데이터를 JSON 파일로 저장.\n",
    "    \"\"\"\n",
    "    merged_texts = preprocess_and_merge(data)\n",
    "\n",
    "    # 병합된 텍스트를 JSON으로 저장\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_texts, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def save_merged_texts_as_list(data, output_file):\n",
    "    \"\"\"\n",
    "    병합된 텍스트 데이터를 문자열 리스트로 저장.\n",
    "    \"\"\"\n",
    "    merged_texts = [merge_text_fields(video) for category, videos in data.items() for video in videos]\n",
    "\n",
    "    # 문자열 리스트를 JSON으로 저장\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_texts, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "input_file_path = \"test_data/trending_videos_with_channels.json\"\n",
    "output_file_path = \"test_data/merged_texts.txt\"\n",
    "\n",
    "# Load, process, and save\n",
    "data = load_json(input_file_path)\n",
    "save_merged_texts_as_list(data, output_file_path)\n",
    "\n",
    "output_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리된 데이터가 'test_data/processed_sentences.json'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import emoji # type: ignore\n",
    "import json\n",
    "\n",
    "\n",
    "# 기본 텍스트 정제 함수\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    텍스트 데이터를 전처리하고 문장을 반환합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str): 입력 텍스트 데이터.\n",
    "\n",
    "    Returns:\n",
    "        str: 전처리된 텍스트.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)  # 문자열로 변환\n",
    "\n",
    "    # 1. HTML 태그 제거\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "    # 2. URL 제거\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # 3. 이메일 제거\n",
    "    text = re.sub(r\"\\S+@\\S+\\.\\S+\", \"\", text)\n",
    "\n",
    "    # 4. 숫자 제거 (명확히 숫자만 제거)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # 5. 반복된 ㅋ, ㅎ, ㅠ, ㅜ 등 제거\n",
    "    text = re.sub(r\"[ㅋㅎㅠㅜ]+\", \"\", text)\n",
    "\n",
    "    # 6. 반복된 점(...) 제거\n",
    "    text = re.sub(r\"\\.\\.+\", \".\", text)\n",
    "\n",
    "    # 7. 반복된 문자 축소 (e.g., \"와아아\" -> \"와\")\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\", text)\n",
    "\n",
    "    # 8. 이모지 제거\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "\n",
    "    # 9. 특수문자 및 영어 알파벳 제거\n",
    "    text = re.sub(r\"[^\\w\\s가-힣]\", \"\", text)  # 영어 알파벳 포함 특수문자 제거\n",
    "    text = re.sub(r\"[a-zA-Z]\", \"\", text)     # 영어 알파벳 제거\n",
    "\n",
    "    # 10. 양쪽 공백 제거\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 데이터 리스트 처리 함수\n",
    "def process_text_file(input_file, output_file):\n",
    "    \"\"\"\n",
    "    문자열과 문자열 리스트가 섞인 데이터를 처리하여 결과를 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): 입력 파일 경로.\n",
    "        output_file (str): 출력 파일 경로.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # 파일 로드\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 데이터 처리\n",
    "    for item in data:\n",
    "        if isinstance(item, list):  # 문자열 리스트인 경우\n",
    "            merged_text = \" \".join(item)\n",
    "            processed_text = preprocess_text(merged_text)\n",
    "        elif isinstance(item, str):  # 문자열인 경우\n",
    "            processed_text = preprocess_text(item)\n",
    "        else:\n",
    "            continue  # 알 수 없는 형식은 건너뜀\n",
    "\n",
    "        results.append(processed_text)\n",
    "\n",
    "    # 결과 저장\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# 실행\n",
    "    # 입력 및 출력 파일 경로\n",
    "input_file = \"test_data/merged_texts.txt\"  # 문자열과 문자열 리스트가 섞인 txt 파일\n",
    "output_file = \"test_data/cleaned.json\"\n",
    "\n",
    "# 텍스트 파일 처리\n",
    "process_text_file(input_file, output_file)\n",
    "print(f\"처리된 데이터가 '{output_file}'에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리된 데이터가 'test_data/keyword.json'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_ngrams(texts, n=2):\n",
    "    \"\"\"\n",
    "    텍스트에서 N-그램 기반 구 단위 키워드 추출.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): 텍스트 리스트.\n",
    "        n (int): N-그램 크기 (2: bigram, 3: trigram).\n",
    "\n",
    "    Returns:\n",
    "        dict: N-그램과 빈도수 딕셔너리.\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n))\n",
    "    ngram_matrix = vectorizer.fit_transform(texts)\n",
    "    ngram_counts = ngram_matrix.sum(axis=0).A1\n",
    "    ngram_vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # numpy.int64 -> int 변환\n",
    "    return {ngram: int(count) for ngram, count in zip(ngram_vocab, ngram_counts)}\n",
    "\n",
    "\n",
    "def extract_keyword_from_file(input_file, output_file, n=2):\n",
    "    \"\"\"\n",
    "    문자열과 문자열 리스트가 섞인 데이터를 처리하여 결과를 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): 입력 파일 경로.\n",
    "        output_file (str): 출력 파일 경로.\n",
    "        n (int): N-그램 크기.\n",
    "    \"\"\"\n",
    "    # 파일 로드\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 문자열 리스트로 변환 (리스트나 문자열 모두 처리)\n",
    "    texts = []\n",
    "    for item in data:\n",
    "        if isinstance(item, list):  # 문자열 리스트인 경우 병합\n",
    "            texts.append(\" \".join(item))\n",
    "        elif isinstance(item, str):  # 단일 문자열인 경우 그대로 추가\n",
    "            texts.append(item)\n",
    "\n",
    "    # N-그램 키워드 추출\n",
    "    ngram_keywords = extract_ngrams(texts, n=n)\n",
    "\n",
    "    # 결과 저장\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(ngram_keywords, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 입력 및 출력 파일 경로\n",
    "    input_file = \"test_data/cleaned.json\"  # 문자열과 문자열 리스트가 섞인 txt 파일\n",
    "    output_file = \"test_data/keyword.json\"\n",
    "\n",
    "    # 텍스트 파일 처리 (2-그램 추출)\n",
    "    extract_keyword_from_file(input_file, output_file, n=2)\n",
    "    print(f\"처리된 데이터가 '{output_file}'에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 및 1번 등장 키워드 제외 후 결과가 'test_data/filtered_keywords.json'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "def load_stopwords(filepath):\n",
    "    \"\"\"\n",
    "    불용어 리스트를 로드합니다.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): 불용어 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "        set: 불용어 집합.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        stopwords = {line.strip() for line in f}\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def split_filter_and_count_keywords(keywords, stopwords):\n",
    "    \"\"\"\n",
    "    키워드를 단어로 나누고, 불용어를 제거하며 1번만 등장한 키워드를 필터링합니다.\n",
    "\n",
    "    Args:\n",
    "        keywords (dict): 키워드와 점수로 이루어진 딕셔너리.\n",
    "        stopwords (set): 불용어 집합.\n",
    "\n",
    "    Returns:\n",
    "        dict: 불용어 제거 및 1번만 등장한 키워드 제거 후 결과.\n",
    "    \"\"\"\n",
    "    filtered_keywords = {}\n",
    "\n",
    "    for key, value in keywords.items():\n",
    "        # 1. 키워드를 공백으로 분리하여 단어 리스트 생성\n",
    "        words = key.split()\n",
    "\n",
    "        # 2. 단어들 중 불용어가 포함되어 있는지 확인\n",
    "        if any(word in stopwords for word in words):\n",
    "            continue  # 불용어가 포함된 경우 제거\n",
    "        \n",
    "    \n",
    "    # 단어 분리 및 출현 횟수 계산\n",
    "    word_counts = Counter()\n",
    "    for phrase, count in keywords.items():\n",
    "    words = phrase.split()  # 띄어쓰기 기준으로 단어 분리\n",
    "    word_counts.update({word: count for word in words})\n",
    "       \n",
    "    return filtered_keywords\n",
    "\n",
    "\n",
    "def process_keywords(input_file, stopwords_file, output_file):\n",
    "    \"\"\"\n",
    "    키워드 파일에서 불용어를 제거하고 1번 등장한 키워드를 제외한 결과를 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): 입력 키워드 파일 경로.\n",
    "        stopwords_file (str): 불용어 파일 경로.\n",
    "        output_file (str): 결과 저장 경로.\n",
    "    \"\"\"\n",
    "    # 불용어 로드\n",
    "    stopwords = load_stopwords(stopwords_file)\n",
    "\n",
    "    # 키워드 파일 로드\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        keywords = json.load(f)\n",
    "\n",
    "    # 불용어 제거 및 1번 등장한 키워드 필터링\n",
    "    filtered_keywords = split_filter_and_count_keywords(keywords, stopwords)\n",
    "\n",
    "    # 결과 저장\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered_keywords, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"불용어 제거 및 1번 등장 키워드 제외 후 결과가 '{output_file}'에 저장되었습니다.\")\n",
    "\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # 파일 경로 설정\n",
    "    input_file = \"test_data/keyword.json\"  # 키워드 추출 결과 파일\n",
    "    stopwords_file = \"test_data/stopwords-ko.txt\"  # 불용어 리스트 파일\n",
    "    output_file = \"test_data/filtered_keywords.json\"  # 결과 저장 파일\n",
    "\n",
    "    # 키워드 필터링 수행\n",
    "    process_keywords(input_file, stopwords_file, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
