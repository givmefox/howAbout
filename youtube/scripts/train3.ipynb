{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "# ë¬¸ì„ ê³¤ - ìˆ˜ì • ì•ˆí•¨\n",
    "\n",
    "# JSON ë°ì´í„° ì €ì¥ í•¨ìˆ˜\n",
    "def save_to_json(data, filename):\n",
    "    # data í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    \n",
    "    # íŒŒì¼ ê²½ë¡œë¥¼ data í´ë” ì•„ë˜ë¡œ ì„¤ì •\n",
    "    filepath = os.path.join('data', filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    print(f\"ë°ì´í„° ì €ì¥: {filepath}\")\n",
    "    \n",
    "    # JSON ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "# JSON íŒŒì¼ì„ ì½ì–´ì„œ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "# valueê°€ ë¬¸ìì—´ë°°ì—´ì˜ ë°°ì—´ì¼ ë•Œ í•œê¸€ìì™€ ë¹ˆ ë¬¸ìì—´ì„ ì œì™¸í•˜ê³  ë¬¸ìì—´ë¡œ ë§Œë“œëŠ” í•¨ìˆ˜\n",
    "def merge_values(data):\n",
    "    \"\"\"\n",
    "    key: valueì—ì„œ valueê°€ ë¬¸ìì—´ ë°°ì—´ì˜ ë°°ì—´ì¼ ë•Œ,\n",
    "    í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ë¹ˆ ë¬¸ìì—´(\"\") ë° í•œ ê¸€ì ë‹¨ì–´ëŠ” ì œì™¸í•˜ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    :param data: dict, key: list of lists (e.g., { \"key1\": [[\"ë¬¸ì¥1\", \"ë¬¸ì¥2\"], [\"ë¬¸ì¥3\"]] })\n",
    "    :return: dict, key: merged string (e.g., { \"key1\": \"ë¬¸ì¥1 ë¬¸ì¥2 ë¬¸ì¥3\" })\n",
    "    \"\"\"\n",
    "    merged_data = {}\n",
    "    for key, values in data.items():\n",
    "        if isinstance(values, list):  # ê°’ì´ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸\n",
    "            merged_sentence = \" \".join(\n",
    "                s for sublist in values for s in sublist if s.strip() and len(s.strip()) > 1\n",
    "            )  # ë¹ˆ ë¬¸ìì—´ & í•œ ê¸€ì ì œì™¸ í›„ ê³µë°±ìœ¼ë¡œ ì—°ê²°\n",
    "            merged_data[key] = merged_sentence  # key: ë³‘í•©ëœ ë¬¸ìì—´ í˜•íƒœë¡œ ì €ì¥\n",
    "\n",
    "    return merged_data\n",
    "# titleê³¼ tagsë¥¼ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ í•©ì¹˜ëŠ” í•¨ìˆ˜\n",
    "def merge_title_tags(data):\n",
    "    \"\"\"\n",
    "    ë”•ì…”ë„ˆë¦¬ì—ì„œ 'title'ê³¼ 'tags'ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜.\n",
    "    \n",
    "    :param data: dict (ìœ íŠœë¸Œ ë¹„ë””ì˜¤ ì •ë³´)\n",
    "    :return: dict (titleê³¼ tagsê°€ í•©ì³ì§„ ë¬¸ì¥)\n",
    "    \"\"\"\n",
    "    merged_data = []\n",
    "    for key, videos in data.items():\n",
    "        for video in videos:\n",
    "            title = video.get(\"title\", \"\").strip()  # titleì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´\n",
    "            tags = video.get(\"tags\", [])  # tagsê°€ ì—†ê±°ë‚˜ Noneì´ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬\n",
    "            merged = \" \".join([title] + tags)  # titleê³¼ tags ê²°í•©\n",
    "            merged_data.append(merged)\n",
    "    \n",
    "    return merged_data\n",
    "# í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n",
    "def clean_text(text):   \n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\S+|#\\S+', '', text)\n",
    "    text = re.sub(r\"[^ê°€-í£a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"[a-zA-Z]\", \"\", text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching trending videos for category: News & Politics\n",
      "Error fetching videos: Unable to find the server at youtube.googleapis.com\n",
      "Error fetching videos: Unable to find the server at youtube.googleapis.com\n",
      "Error fetching videos: Unable to find the server at youtube.googleapis.com\n",
      "ë¹„ë””ì˜¤ 44 ê°œ ì¹´í…Œê³ ë¦¬: News & Politics fetch ì™„ë£Œ.\n",
      "Fetching trending videos for category: Film & Animation\n",
      "ë¹„ë””ì˜¤ 7 ê°œ ì¹´í…Œê³ ë¦¬: Film & Animation fetch ì™„ë£Œ.\n",
      "Fetching trending videos for category: Music\n",
      "ë¹„ë””ì˜¤ 30 ê°œ ì¹´í…Œê³ ë¦¬: Music fetch ì™„ë£Œ.\n",
      "Fetching trending videos for category: Pets & Animals\n",
      "ë¹„ë””ì˜¤ 0 ê°œ ì¹´í…Œê³ ë¦¬: Pets & Animals fetch ì™„ë£Œ.\n",
      "Fetching trending videos for category: Sports\n",
      "ë¹„ë””ì˜¤ 13 ê°œ ì¹´í…Œê³ ë¦¬: Sports fetch ì™„ë£Œ.\n",
      "Fetching trending videos for category: Gaming\n",
      "ë¹„ë””ì˜¤ 77 ê°œ ì¹´í…Œê³ ë¦¬: Gaming fetch ì™„ë£Œ.\n",
      "Fetching trending videos for category: Entertainment\n",
      "ë¹„ë””ì˜¤ 7 ê°œ ì¹´í…Œê³ ë¦¬: Entertainment fetch ì™„ë£Œ.\n",
      "Fetching trending videos for category: Science & Technology\n",
      "ë¹„ë””ì˜¤ 60 ê°œ ì¹´í…Œê³ ë¦¬: Science & Technology fetch ì™„ë£Œ.\n",
      "ë°ì´í„° ì €ì¥: data/raw_video_data.json\n",
      "ë°ì´í„° ì €ì¥ : data/raw_video_data.json'\n",
      "\n",
      "Category: News & Politics\n",
      " - ê¹€ì–´ì¤€ì˜ ê²¸ì†ì€í˜ë“¤ë‹¤ ë‰´ìŠ¤ê³µì¥ 2025ë…„ 2ì›” 19ì¼ ìˆ˜ìš”ì¼ [ìœ¤ê±´ì˜, ê¹€ê¸°í‘œ, ë°•ì€ì •, ê²¸ì†NSC, ê¹€ì‹œì—°, ìŠ¤í¬ì¸ ê³µì¥] (IC3iNhz02l0), ì¡°íšŒìˆ˜: 1278696 íšŒ, ì¢‹ì•„ìš”: 97711 ê°œ\n",
      " - í­íƒ„ ë´‰ì§€ìš± â€œì €ëŠ” ê³„ì† í„°íŠ¸ë¦´ ê²ë‹ˆë‹¤. ë²„í‹°ê¸° í˜ë“¤ ê±°ì•¼~â€ï½œí’€ë²„ì „ (cvmdNi_oBYA), ì¡°íšŒìˆ˜: 1418688 íšŒ, ì¢‹ì•„ìš”: 73932 ê°œ\n",
      " - ë˜ 'ì§„ìˆ  ì¡°ì„œ' ë¬¸ì œ ì‚¼ì€ å°¹ì¸¡â€¥ê±°ì ˆí•˜ì ì§ ì‹¸ì„œ í‡´ì¥ (2025.02.18/ë‰´ìŠ¤ë°ìŠ¤í¬/MBC) (PaFf3jmHVS8), ì¡°íšŒìˆ˜: 1247302 íšŒ, ì¢‹ì•„ìš”: 17802 ê°œ\n",
      " - ê²€ì°° ë³´ê³ ì„œ ì¶”ê°€ ê³µê°œ...'ê¹€ê±´í¬ ê³µì²œê°œì…' í”ì  ìˆ˜ë‘ë£© ã€ˆì£¼ê°„ ë‰´ìŠ¤íƒ€íŒŒã€‰ (5V8bHfodNU8), ì¡°íšŒìˆ˜: 1136689 íšŒ, ì¢‹ì•„ìš”: 59170 ê°œ\n",
      " - é‡ íšŒìœ  ì˜í˜¹ì— ê¹€í˜„íƒœ ì… ì—´ì—ˆë‹¤ / ì±„ë„A / ë‰´ìŠ¤TOP 10 (b3aRCbm-NhE), ì¡°íšŒìˆ˜: 766558 íšŒ, ì¢‹ì•„ìš”: 21858 ê°œ\n",
      "\n",
      "Category: Film & Animation\n",
      " - ìˆ˜ì§€ì˜ ìƒˆë¡œìš´ ê²½í˜¸ì› (SmygsXbknXo), ì¡°íšŒìˆ˜: 2222297 íšŒ, ì¢‹ì•„ìš”: 78837 ê°œ\n",
      " - ë°°ìš° ê¹€ìƒˆë¡ , ë§ˆì§€ë§‰ SNS ê²Œì‹œë¬¼ì€?â€¦ë¹ˆì†Œ ì°¾ì€ ì›ë¹ˆ 'ì¹¨í†µ' / ì—°í•©ë‰´ìŠ¤ (Yonhapnews) (6k74FOZ7HuU), ì¡°íšŒìˆ˜: 1989801 íšŒ, ì¢‹ì•„ìš”: 9621 ê°œ\n",
      " - [ğŸŒˆìŠµì§¤ì˜ ê¸°ì›] ì•ˆê²½ ë²—ê²¨ì§€ê³  êµìˆ˜ë‹˜í•œí…Œ ëŒë ¤ê°€ëŠ” í˜„ì‹¤ ë ˆì§€ë˜íŠ¸ 2ë…„ ì°¨ğŸ˜‚ | ì²­ì¶˜ì˜êµ­ | SBS (_Auauw4MoIg), ì¡°íšŒìˆ˜: 2220356 íšŒ, ì¢‹ì•„ìš”: 38879 ê°œ\n",
      " - ë‹¬ì˜ ê¸°ì› - íŒŒíŠ¸1(ì‡¼ì¸ ë²„ì „) (40cZ0GzbsiY), ì¡°íšŒìˆ˜: 293341 íšŒ, ì¢‹ì•„ìš”: 15387 ê°œ\n",
      " - ê°œìš±ê³€ã…‹ã…‹ã…‹ã…‹ #ë ˆì „ë“œì‚¬ì—° #ì»¬íˆ¬ì‡¼ #ì›ƒê¸´ë™ì˜ìƒ (FezHGedgr18), ì¡°íšŒìˆ˜: 634639 íšŒ, ì¢‹ì•„ìš”: 15098 ê°œ\n",
      "\n",
      "Category: Music\n",
      " - KiiiKiii í‚¤í‚¤ 'I DO ME' MV (hAEfi_SKTEU), ì¡°íšŒìˆ˜: 2926190 íšŒ, ì¢‹ì•„ìš”: 178912 ê°œ\n",
      " - JISOO - earthquake (Official Music Video) (2V6lvCUPT8I), ì¡°íšŒìˆ˜: 23371226 íšŒ, ì¢‹ì•„ìš”: 2197051 ê°œ\n",
      " - ATTITUDE (ATTITUDE) (7sJES_4FiAs), ì¡°íšŒìˆ˜: 3715827 íšŒ, ì¢‹ì•„ìš”: 15098 ê°œ\n",
      " - REBEL HEART (REBEL HEART) (N1so5Q60Chw), ì¡°íšŒìˆ˜: 3075804 íšŒ, ì¢‹ì•„ìš”: 11559 ê°œ\n",
      " - I Miss You So Much (ë¯¸ì¹˜ê²Œ ê·¸ë¦¬ì›Œì„œ) (q9UUIH6xZ9U), ì¡°íšŒìˆ˜: 1140494 íšŒ, ì¢‹ì•„ìš”: 9036 ê°œ\n",
      "\n",
      "Category: Pets & Animals\n",
      "\n",
      "Category: Sports\n",
      " - [24/25 PL] 25R í† íŠ¸ë„˜ vs ë§¨ìœ  H/Lï½œSPOTV FOOTBALL (Sz1YQ4_JZZw), ì¡°íšŒìˆ˜: 1212172 íšŒ, ì¢‹ì•„ìš”: 10076 ê°œ\n",
      " - ì²´ëŒ€ìƒ VS ì•¼êµ¬ì„ ìˆ˜ (DpzAVGlquE4), ì¡°íšŒìˆ˜: 262168 íšŒ, ì¢‹ì•„ìš”: 8235 ê°œ\n",
      " - ê¸°ì ì„ ë…¸ë˜í•˜ë¼ ë‹¹êµ¬ìŠ¤íƒ€ K (1pIsYX286is), ì¡°íšŒìˆ˜: 1188401 íšŒ, ì¢‹ì•„ìš”: 10459 ê°œ\n",
      " - [ì¤‘êµ­í•´ì„¤ ì‹¤ì œ ë²ˆì—­] í•œêµ­ì´ í‹° ì•ˆë‚´ê³  ì¼ë¶€ëŸ¬ ëª» í•˜ëŠ”ì²™ ì—°ê¸°í•˜ë‹¤ê°€ ê²°ìŠ¹ì „ì—ì„œ ê°‘ìê¸° ì§„ì§œ ì‹¤ë ¥ ë“œëŸ¬ë‚´ë©° ê¸ˆì€ë™ ì‹¹ì“¸ì´í•˜ë‹ˆ ë‚œë¦¬ë‚œ CCTV ì¤‘ê³„ì„ ë°˜ì‘ ë²ˆì—­ (d0UYzYUe20c), ì¡°íšŒìˆ˜: 1492140 íšŒ, ì¢‹ì•„ìš”: 10430 ê°œ\n",
      " - â€˜í”¼ê²¨ í”„ë¦°ìŠ¤â€™ ì°¨ì¤€í™˜ ê¸ˆë©”ë‹¬! ì¼ë³¸ ê°€ê¸°ì•¼ë§ˆ ìœ ë§ˆì— ëŒ€ì—­ì „ ë“œë¼ë§ˆ! [í•˜ì–¼ë¹ˆ ë™ê³„ ì•„ì‹œì•ˆê²Œì„] (dEn5iPslOag), ì¡°íšŒìˆ˜: 428696 íšŒ, ì¢‹ì•„ìš”: 9026 ê°œ\n",
      "\n",
      "Category: Gaming\n",
      " - í™”ì„± ì²˜ìŒ ê°€ëŠ” ë‚¨ì (WA4wCIEREeM), ì¡°íšŒìˆ˜: 349372 íšŒ, ì¢‹ì•„ìš”: 3365 ê°œ\n",
      " - í¬ì˜¤ë„¤ ì´ìŠ¤í„°ì—ê·¸ (71KzEzcRA7o), ì¡°íšŒìˆ˜: 262418 íšŒ, ì¢‹ì•„ìš”: 2192 ê°œ\n",
      " - ìƒì¡´ ê°€ëŠ¥ì„± 0.001%.. ì•„ë¬´ê²ƒë„ ì—†ëŠ” ìš°ì£¼ì—ì„œ ì‚´ì•„ë‚¨ê¸° (IgXcQ3fyUug), ì¡°íšŒìˆ˜: 265130 íšŒ, ì¢‹ì•„ìš”: 11000 ê°œ\n",
      " - ë“€ì˜¤ vs ë“€ì˜¤ ì‹¸ì›€ì— ë‚‘ê²¨ë²„ë¦° ì•„ì¬ (T_uOWdI8xgo), ì¡°íšŒìˆ˜: 174389 íšŒ, ì¢‹ì•„ìš”: 0 ê°œ\n",
      " - ì œìë“¤ì˜ ì§„ì§€í•œ ê°œë…ì‹¸ì›€ì— ë‹¹í™©í•œ í´ë¦¬ë“œ;; (hiig0HuQib4), ì¡°íšŒìˆ˜: 196619 íšŒ, ì¢‹ì•„ìš”: 1877 ê°œ\n",
      "\n",
      "Category: Entertainment\n",
      " - [êµ¿ë°ì´] ê·¸ë¦¬ì› ë‹¤ ì´ ì¼€ë¯¸... 11ë…„ ì§€ë‚˜ë„ ì—¬ì „í•œ í˜•ëˆâ™¥ì§€ìš©, ê·¸ë¦¬ê³  ìŠ¤íƒ€(?)ê°€ ë˜ì–´ ëŒì•„ì˜¨ í”„ì½˜ì´ã…£#GD #ì •í˜•ëˆ #ë°í”„ì½˜ MBC250216ë°©ì†¡ (oFdtUKHKOZc), ì¡°íšŒìˆ˜: 2103511 íšŒ, ì¢‹ì•„ìš”: 24815 ê°œ\n",
      " - ë…¸ìƒì›ì„ íŒŒë„ ê¹€ê±´í¬! ê±´ì§„ë²•ì‚¬ë¥¼ íŒŒë„ ê¹€ê±´í¬!ï½œí’€ë²„ì „ (TaOwNNLFb1g), ì¡°íšŒìˆ˜: 1551079 íšŒ, ì¢‹ì•„ìš”: 78557 ê°œ\n",
      " - ìˆ˜ì§€ì˜ ìƒˆë¡œìš´ ê²½í˜¸ì› (SmygsXbknXo), ì¡°íšŒìˆ˜: 2223031 íšŒ, ì¢‹ì•„ìš”: 78837 ê°œ\n",
      " - ì´ì¤‘ ì–¸ì–´ì˜ ë¬¸ì œ?ğŸ”¥#ê¸ˆìª½ê°™ì€ë‚´ìƒˆë¼ #shorts (5pKJY1yjqvQ), ì¡°íšŒìˆ˜: 1005976 íšŒ, ì¢‹ì•„ìš”: 15645 ê°œ\n",
      " - ë°°ìš° ê¹€ìƒˆë¡ , ë§ˆì§€ë§‰ SNS ê²Œì‹œë¬¼ì€?â€¦ë¹ˆì†Œ ì°¾ì€ ì›ë¹ˆ 'ì¹¨í†µ' / ì—°í•©ë‰´ìŠ¤ (Yonhapnews) (6k74FOZ7HuU), ì¡°íšŒìˆ˜: 1989801 íšŒ, ì¢‹ì•„ìš”: 9621 ê°œ\n",
      "\n",
      "Category: Science & Technology\n",
      " - ì•„ì´í° ì ˆë°˜ê¸‰ ë‘ê»˜ì— ì£¼ë¦„ë„ ì—†ë‹¤?;; ìƒíƒœê³„ êµë€í•  ì¤‘êµ­ì‚° í´ë”ë¸”í° ê³§ ë‚˜ì˜¨ë‹¤. (jAV4NnFlrV0), ì¡°íšŒìˆ˜: 594427 íšŒ, ì¢‹ì•„ìš”: 5927 ê°œ\n",
      " - ì•„ì´í° SE4, ëª¨ë“  ê²Œ ë¹—ë‚˜ê°”ë‹¤! ì‹¤ë¬¼ ë””ìì¸ ì„ ê³µê°œ ë° ìµœì‹  ìœ ì¶œ ì •ë³´ ì´ì •ë¦¬ (ëª©ì—…, ìƒ‰ìƒ í¬ê¸° ë‘ê»˜, ë‹¤ì´ë‚˜ë¯¹ ì•„ì¼ëœë“œ, ì¹´ë©”ë¼, ì„±ëŠ¥, ë°°í„°ë¦¬, ê°€ê²© ì¶œì‹œì¼) (DKxZNTsRxqo), ì¡°íšŒìˆ˜: 54018 íšŒ, ì¢‹ì•„ìš”: 505 ê°œ\n",
      " - ì§€ê¸ˆ ì´ ë²„íŠ¼ ë‹¹ì¥ ë„ì„¸ìš”!! ë¹„ë°€ë²ˆí˜¸/ê³„ì •/ì´ë©”ì¼ ì „ë¶€ í„¸ì–´ê°‘ë‹ˆë‹¤! (WVEJwFlQnPU), ì¡°íšŒìˆ˜: 454118 íšŒ, ì¢‹ì•„ìš”: 9329 ê°œ\n",
      " - OpenAI, êµ¬ê¸€, ë”¥ì”¨í¬ ëª¨ë‘ ì••ì‚´...Grok3 | GPU 20ë§Œê°œì˜ ê²°ê³¼ë¬¼ Grok3ê°€ ì™œ ë†€ë¼ìš´ ê²ƒì¸ê°€ (hoQwz_kQ0Xc), ì¡°íšŒìˆ˜: 80005 íšŒ, ì¢‹ì•„ìš”: 2296 ê°œ\n",
      " - ì™€ ì—„ì²­ ë¬´í„±ì´ë‹¤ (jR4UpUPzWhk), ì¡°íšŒìˆ˜: 515182 íšŒ, ì¢‹ì•„ìš”: 5272 ê°œ\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import isodate\n",
    "import os\n",
    "\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ 'YOUTUBE_API_KEY'ë¥¼ ì„¤ì •í•˜ì„¸ìš”.\")\n",
    "\n",
    "# YouTube API ì„¤ì •\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ ID ì„¤ì •\n",
    "CATEGORIES = {\n",
    "    \"News & Politics\": \"25\",\n",
    "    'Film & Animation' : \"1\",\n",
    "    'Music' : \"10\",\n",
    "    'Pets & Animals' : \"15\",\n",
    "    'Sports' : \"17\",\n",
    "    'Gaming' : \"20\",\n",
    "    'Entertainment' : \"24\",\n",
    "    'Science & Technology': \"28\"\n",
    "}\n",
    "\n",
    "# ë™ì˜ìƒ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "def fetch_trending_videos(category_id, region_code=\"KR\", max_results=200):\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(videos) < max_results:\n",
    "        try:\n",
    "            request = youtube.videos().list(\n",
    "                part=\"snippet,statistics,contentDetails\",\n",
    "                chart=\"mostPopular\",\n",
    "                regionCode=region_code,\n",
    "                videoCategoryId=category_id,\n",
    "                maxResults=min(50, max_results - len(videos)),\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                duration = isodate.parse_duration(item[\"contentDetails\"][\"duration\"])\n",
    "                duration_in_seconds = duration.total_seconds()  #ì´ˆë¡œ ë°”ê¾¸ê¸°ê¸°\n",
    "\n",
    "                if duration_in_seconds > 80:  # 80ì´ˆ ì´ìƒì˜ ë™ì˜ìƒë§Œ ê°€ì ¸ì˜¤ê¸°\n",
    "                    videos.append({\n",
    "                        \"video_id\": item[\"id\"],\n",
    "                        \"title\": item[\"snippet\"][\"title\"],\n",
    "                        \"description\": item[\"snippet\"][\"description\"],\n",
    "                        \"tags\": item[\"snippet\"].get(\"tags\", []),\n",
    "                        \"duration\": str(duration),\n",
    "                        \"view_count\": int(item[\"statistics\"].get(\"viewCount\", 0)),\n",
    "                        \"like_count\": int(item[\"statistics\"].get(\"likeCount\", 0)),\n",
    "                        \"comment_count\": int(item[\"statistics\"].get(\"commentCount\", 0)),\n",
    "                        \"category_id\": category_id,\n",
    "                    })\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching videos: {e}\")\n",
    "            time.sleep(5)  # ì ì‹œ ëŒ€ê¸° í›„ ë‹¤ì‹œ ì‹œë„\n",
    "\n",
    "    return videos\n",
    "\n",
    "\n",
    "\n",
    "# ì‹¤í–‰\n",
    "all_videos = {}\n",
    "\n",
    "for category_name, category_id in CATEGORIES.items():\n",
    "    print(f\"Fetching trending videos for category: {category_name}\")\n",
    "    videos = fetch_trending_videos(category_id, region_code=\"KR\", max_results=200)\n",
    "    all_videos[category_name] = videos\n",
    "    print(f\"ë¹„ë””ì˜¤ {len(videos)} ê°œ ì¹´í…Œê³ ë¦¬: {category_name} fetch ì™„ë£Œ.\")\n",
    "\n",
    "# ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "output_file = \"raw_video_data.json\"\n",
    "save_to_json(all_videos, output_file)\n",
    "print(f\"ë°ì´í„° ì €ì¥ : data/{output_file}'\")\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥ ì˜ˆì‹œ\n",
    "for category, videos in all_videos.items():\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    for video in videos[:5]:\n",
    "        print(f\" - {video['title']} ({video['video_id']}), ì¡°íšŒìˆ˜: {video['view_count']} íšŒ, ì¢‹ì•„ìš”: {video['like_count']} ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching comments for video CfK_9iDlCSo: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=CfK_9iDlCSo&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video 7sJES_4FiAs: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=7sJES_4FiAs&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video N1so5Q60Chw: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=N1so5Q60Chw&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video q9UUIH6xZ9U: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=q9UUIH6xZ9U&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video GBm2e08CKSo: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=GBm2e08CKSo&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video Y8rHo43-LyQ: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=Y8rHo43-LyQ&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video ieWjkkZADkQ: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=ieWjkkZADkQ&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video oda74gp5RsY: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=oda74gp5RsY&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video EnsuJgsxM-8: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=EnsuJgsxM-8&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video OveWq3SnznE: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=OveWq3SnznE&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video oEBbSqUHfuI: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=oEBbSqUHfuI&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video DG_GrIsD-tg: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=DG_GrIsD-tg&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video UZ34FXPquTI: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=UZ34FXPquTI&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video 0ujged1o7zg: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=0ujged1o7zg&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video n2Lh4tvWsAg: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=n2Lh4tvWsAg&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video XEQc-2HoE1c: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=XEQc-2HoE1c&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video 529VgRXlmuI: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=529VgRXlmuI&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video xzKsXyRMrKs: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=xzKsXyRMrKs&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video xE886FlLWNA: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=xE886FlLWNA&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video rwwsLML-mSM: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=rwwsLML-mSM&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video O4-ZfIN1IM0: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=O4-ZfIN1IM0&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video 5pKJY1yjqvQ: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=5pKJY1yjqvQ&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video uYbXv0yW0wo: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=uYbXv0yW0wo&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video S1CbhpKC6SE: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=S1CbhpKC6SE&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "ë°ì´í„° ì €ì¥: data/video_comments.json\n",
      "ë¹„ë””ì˜¤ ëŒ“ê¸€ì´ 'video_comments.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "\n",
    "# ë¬¸ì„ ê³¤ - ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì •ë¦¬ ë˜ë„ë¡ ìˆ˜ì •\n",
    "\n",
    "\n",
    "# ë¹„ë””ì˜¤ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜\n",
    "def get_video_comments(youtube, video_id, max_results=100):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < max_results:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=min(50, max_results - len(comments)),\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                comments.append(comment)\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching comments for video {video_id}: {e}\")\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "# JSON ë°ì´í„° ì €ì¥ í•¨ìˆ˜\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "input_file_path = 'data/raw_video_data.json'\n",
    "output_file_path = 'video_comments.json'\n",
    "\n",
    "# JSON ë°ì´í„° ë¡œë“œ\n",
    "data = load_json(input_file_path)\n",
    "\n",
    "\n",
    "# ë¹„ë””ì˜¤ ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸° ë° ì €ì¥\n",
    "all_comments = {}\n",
    "for category, videos in data.items():\n",
    "    all_comments[category] = []  # ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "\n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        comments = get_video_comments(youtube, video_id, max_results=1000)\n",
    "        all_comments[category].append({\"video_id\": video_id, \"comments\": comments})\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "save_to_json(all_comments, output_file_path)\n",
    "print(f\"ë¹„ë””ì˜¤ ëŒ“ê¸€ì´ '{output_file_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì²˜ë¦¬ëœ ë°ì´í„°ê°€ 'cleaned.txt'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# import emoji # type: ignore\n",
    "# import json\n",
    "\n",
    "\n",
    "# # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜\n",
    "# def preprocess_text(text):\n",
    "#     \"\"\"\n",
    "#     í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê³  ë¬¸ì¥ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "#     Args:\n",
    "#         text (str): ì…ë ¥ í…ìŠ¤íŠ¸ ë°ì´í„°.\n",
    "\n",
    "#     Returns:\n",
    "#         str: ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(text, str):\n",
    "#         text = str(text)  # ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "\n",
    "#     # 1. HTML íƒœê·¸ ì œê±°\n",
    "#     text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "#     # 2. URL ì œê±°\n",
    "#     text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "#     # 3. ì´ë©”ì¼ ì œê±°\n",
    "#     text = re.sub(r\"\\S+@\\S+\\.\\S+\", \"\", text)\n",
    "\n",
    "#     # 4. ìˆ«ì ì œê±° (ëª…í™•íˆ ìˆ«ìë§Œ ì œê±°)\n",
    "#     text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "#     # 5. ë°˜ë³µëœ ã…‹, ã…, ã… , ã…œ ë“± ì œê±°\n",
    "#     text = re.sub(r\"[ã…‹ã…ã… ã…œ]+\", \"\", text)\n",
    "\n",
    "#     # 6. ë°˜ë³µëœ ì (...) ì œê±°\n",
    "#     text = re.sub(r\"\\.\\.+\", \".\", text)\n",
    "\n",
    "#     # 7. ë°˜ë³µëœ ë¬¸ì ì¶•ì†Œ (e.g., \"ì™€ì•„ì•„\" -> \"ì™€\")\n",
    "#     text = re.sub(r\"(.)\\1{2,}\", r\"\\1\", text)\n",
    "\n",
    "#     # 8. ì´ëª¨ì§€ ì œê±°\n",
    "#     text = emoji.replace_emoji(text, replace=\"\")\n",
    "\n",
    "#     # 9. íŠ¹ìˆ˜ë¬¸ì ë° ì˜ì–´ ì•ŒíŒŒë²³ ì œê±°\n",
    "#     text = re.sub(r\"[^\\w\\sê°€-í£]\", \"\", text)  # ì˜ì–´ ì•ŒíŒŒë²³ í¬í•¨ íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "#     text = re.sub(r\"[a-zA-Z]\", \"\", text)     # ì˜ì–´ ì•ŒíŒŒë²³ ì œê±°\n",
    "\n",
    "#     # 10. ì–‘ìª½ ê³µë°± ì œê±°\n",
    "#     text = text.strip()\n",
    "\n",
    "#     return text\n",
    "\n",
    "\n",
    "# # ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜\n",
    "# def process_text_file(input_file, output_file):\n",
    "#     \"\"\"\n",
    "#     ë¬¸ìì—´ê³¼ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ê°€ ì„ì¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "#     Args:\n",
    "#         input_file (str): ì…ë ¥ íŒŒì¼ ê²½ë¡œ.\n",
    "#         output_file (str): ì¶œë ¥ íŒŒì¼ ê²½ë¡œ.\n",
    "#     \"\"\"\n",
    "#     results = []\n",
    "\n",
    "#     # íŒŒì¼ ë¡œë“œ\n",
    "#     with open(input_file, 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     # ë°ì´í„° ì²˜ë¦¬\n",
    "#     for item in data:\n",
    "#         if isinstance(item, list):  # ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°\n",
    "#             merged_text = \" \".join(item)\n",
    "#             processed_text = preprocess_text(merged_text)\n",
    "#         elif isinstance(item, str):  # ë¬¸ìì—´ì¸ ê²½ìš°\n",
    "#             processed_text = preprocess_text(item)\n",
    "#         else:\n",
    "#             continue  # ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹ì€ ê±´ë„ˆëœ€\n",
    "\n",
    "#         results.append(processed_text)\n",
    "\n",
    "#     # ê²°ê³¼ ì €ì¥\n",
    "#     with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# # ì‹¤í–‰\n",
    "#     # ì…ë ¥ ë° ì¶œë ¥ íŒŒì¼ ê²½ë¡œ\n",
    "# input_file = \"merged_texts.txt\"  # ë¬¸ìì—´ê³¼ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ê°€ ì„ì¸ txt íŒŒì¼\n",
    "# output_file = \"cleaned.txt\"\n",
    "\n",
    "# # í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬\n",
    "# process_text_file(input_file, output_file)\n",
    "# print(f\"ì²˜ë¦¬ëœ ë°ì´í„°ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ë¬¸ì¥ ìˆ˜: 238\n",
      "ë°ì´í„° ì €ì¥: data/cleaned_video_comments.json\n"
     ]
    }
   ],
   "source": [
    "data = load_json('data/video_comments.json')\n",
    "\n",
    "processed_data = {}\n",
    "\n",
    "for category, videos in data.items():\n",
    "    processed_data[category] = []  # ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "    \n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        comments = video.get(\"comments\", [])\n",
    "\n",
    "        # ëŒ“ê¸€ í…ìŠ¤íŠ¸ ì •ë¦¬ ì ìš©\n",
    "        cleaned_comments = [clean_text(comment) for comment in comments]\n",
    "        \n",
    "        # ì €ì¥ í˜•ì‹ ìœ ì§€\n",
    "        processed_data[category].append({\"video_id\": video_id, \"comments\": cleaned_comments})\n",
    "\n",
    "# ì´ ë¬¸ì¥ ìˆ˜ ê³„ì‚°\n",
    "total_sentences = sum(len(value) for value in processed_data.values())\n",
    "print(f\"ì´ ë¬¸ì¥ ìˆ˜: {total_sentences}\")\n",
    "        \n",
    "save_to_json(processed_data, 'cleaned_video_comments.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í† í¬ë‚˜ì´ì € ìƒì„± Soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.182 Gbory 0.875 Gb\n",
      "all cohesion probabilities was computed. # words = 49846\n",
      "all branching entropies was computed # words = 83419\n",
      "all accessor variety was computed # words = 83419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Scores(cohesion_forward=np.float64(0.6554912257227551), cohesion_backward=np.float64(0.29004652062554404), left_branching_entropy=3.791747353178408, right_branching_entropy=2.8167883720666143, left_accessor_variety=86, right_accessor_variety=55, leftside_frequency=895, rightside_frequency=53)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp.word import WordExtractor\n",
    "\n",
    "# Load the cleaned video comments\n",
    "data = load_json('data/cleaned_video_comments.json')\n",
    "\n",
    "# ëª¨ë“  ëŒ“ê¸€ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸° (ì¹´í…Œê³ ë¦¬ êµ¬ì¡° ìœ ì§€)\n",
    "corpus = []\n",
    "for category, videos in data.items():\n",
    "    for video in videos:\n",
    "        comments = video.get(\"comments\", [])\n",
    "        corpus.extend(comments)  # ëª¨ë“  ëŒ“ê¸€ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ê°€\n",
    "\n",
    "# DoublespaceLineCorpus ê°ì²´ ìƒì„±\n",
    "corpus[5]\n",
    "\n",
    "# WordExtractor ê°ì²´ ìƒì„± ë° í•™ìŠµ\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(corpus)\n",
    "word_scores = word_extractor.extract()\n",
    "\n",
    "word_scores[\"ìœ¤ì„ì—´\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 215196 from 86878 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=643051, mem=1.455 Gb\n",
      "[Noun Extractor] batch prediction was completed for 66369 words\n",
      "[Noun Extractor] checked compounds. discovered 40896 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52429 -> 39486\n",
      "[Noun Extractor] postprocessing ignore_features : 39486 -> 39194\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39194 -> 38448\n",
      "[Noun Extractor] 38448 nouns (40896 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.294 Gb                    \n",
      "[Noun Extractor] 65.61 % eojeols are covered\n",
      "ë°ì´í„° ì €ì¥: data/tokenized_video_comments.json\n"
     ]
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "\n",
    "cohesion_score = {word:score.cohesion_forward for word, score in word_scores.items()}\n",
    "\n",
    "tokenizer = MaxScoreTokenizer(scores=cohesion_score)\n",
    "\n",
    "\n",
    "noun_extractor = LRNounExtractor_v2()\n",
    "nouns = noun_extractor.train_extract(corpus) # list of str like\n",
    "\n",
    "noun_scores = {noun:score.score for noun, score in nouns.items()}\n",
    "combined_scores = {noun:score + cohesion_score.get(noun, 0)\n",
    "    for noun, score in noun_scores.items()}\n",
    "combined_scores.update(\n",
    "    {subword:cohesion for subword, cohesion in cohesion_score.items()\n",
    "    if not (subword in combined_scores)}\n",
    ")\n",
    "\n",
    "\n",
    "# Load the cleaned video comments\n",
    "data = load_json('data/cleaned_video_comments.json')\n",
    "tokenizer = LTokenizer(scores=combined_scores)\n",
    "\n",
    "# Tokenize each comment\n",
    "tokenized_comments = {}\n",
    "\n",
    "for category, videos in data.items():\n",
    "    tokenized_comments[category] = []  # ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "\n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        comments = video.get(\"comments\", [])\n",
    "\n",
    "        # ëŒ“ê¸€ì„ í† í°í™”\n",
    "        tokenized = [tokenizer.tokenize(comment) for comment in comments]\n",
    "\n",
    "        # êµ¬ì¡° ìœ ì§€\n",
    "        tokenized_comments[category].append({\"video_id\": video_id, \"tokenized_comments\": tokenized})\n",
    "\n",
    "\n",
    "# Save the tokenized comments back to JSON\n",
    "save_to_json(tokenized_comments, 'tokenized_video_comments.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 215196 from 86878 sents. mem=1.132 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=643051, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 66369 words\n",
      "[Noun Extractor] checked compounds. discovered 40896 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52429 -> 39486\n",
      "[Noun Extractor] postprocessing ignore_features : 39486 -> 39194\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39194 -> 38448\n",
      "[Noun Extractor] 38448 nouns (40896 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.269 Gb                    \n",
      "[Noun Extractor] 65.61 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "from soynlp.noun import LRNounExtractor_v2\n",
    "noun_extractor = LRNounExtractor_v2()\n",
    "nouns = noun_extractor.train_extract(corpus) # list of str like\n",
    "\n",
    "noun_scores = {noun:score.score for noun, score in nouns.items()}\n",
    "combined_scores = {noun:score + cohesion_score.get(noun, 0)\n",
    "    for noun, score in noun_scores.items()}\n",
    "combined_scores.update(\n",
    "    {subword:cohesion for subword, cohesion in cohesion_score.items()\n",
    "    if not (subword in combined_scores)}\n",
    ")\n",
    "\n",
    "tokenizer = LTokenizer(scores=combined_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì €ì¥: data/okt_tokenized_video_comments.json\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "data = load_json('data/cleaned_video_comments.json')\n",
    "\n",
    "# Tokenize each comment using Okt\n",
    "tokenized_comments_okt = {}\n",
    "for category, videos in data.items():\n",
    "    tokenized_comments_okt[category] = []  # ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "\n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        comments = video.get(\"comments\", [])\n",
    "\n",
    "        # ëŒ“ê¸€ì„ í˜•íƒœì†Œ ë¶„ì„í•˜ì—¬ í† í°í™”\n",
    "        tokenized = [okt.morphs(comment) for comment in comments]\n",
    "\n",
    "        # êµ¬ì¡° ìœ ì§€\n",
    "        tokenized_comments_okt[category].append({\"video_id\": video_id, \"tokenized_comments\": tokenized})\n",
    "\n",
    "# Save the tokenized comments back to JSON\n",
    "save_to_json(tokenized_comments_okt, 'okt_tokenized_video_comments.json')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì €ì¥: data/okt_nouns_video_comments.json\n"
     ]
    }
   ],
   "source": [
    "# Extract nouns from each comment using Okt\n",
    "nouns_comments_okt = {}\n",
    "for category, videos in data.items():\n",
    "    nouns_comments_okt[category] = []  # ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "\n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        comments = video.get(\"comments\", [])\n",
    "\n",
    "        # ëŒ“ê¸€ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œ\n",
    "        extracted_nouns = [okt.nouns(comment) for comment in comments]\n",
    "\n",
    "        # êµ¬ì¡° ìœ ì§€\n",
    "        nouns_comments_okt[category].append({\"video_id\": video_id, \"nouns\": extracted_nouns})\n",
    "\n",
    "\n",
    "# Save the nouns extracted comments back to JSON\n",
    "save_to_json(nouns_comments_okt, 'okt_nouns_video_comments.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kiwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ë°˜êµ­ê°€ì„¸ë ¥', 0.5480024814605713, 76, -1.5345635414123535),\n",
       " ('ê³½ì¢…ê·¼', 0.5139729380607605, 119, -2.7739651203155518),\n",
       " ('ë¶€ì •ì„ ê±°', 0.4741281569004059, 575, -2.10939621925354),\n",
       " ('ë”¥í˜ì´í¬', 0.3987986743450165, 91, -2.4564383029937744),\n",
       " ('ììœ ëŒ€í•œë¯¼êµ­', 0.3961898982524872, 179, -2.89668607711792),\n",
       " ('ë‹¤ëŒ€í•œë¯¼êµ­', 0.3665927052497864, 49, -2.0551440715789795),\n",
       " ('íƒ„í•µë°˜ëŒ€', 0.3618530035018921, 224, -1.878380537033081),\n",
       " ('ì°¬ì„±ì§‘íšŒ', 0.3450433909893036, 51, -1.5257374048233032),\n",
       " ('ë‹ˆë‹¤ëŒ€í•œë¯¼êµ­', 0.33699408173561096, 26, -1.5568251609802246),\n",
       " ('ë€ìˆ˜ê´´', 0.33526986837387085, 170, -2.487471342086792),\n",
       " ('ìœ¤ëŒ€í†µë ¹', 0.32256072759628296, 325, -0.2791236937046051),\n",
       " ('ì˜¨ì•¤ì˜¤í”„', 0.3092728555202484, 169, -2.5341715812683105),\n",
       " ('ì–‘ìì»´í“¨í„°', 0.3085829019546509, 66, -0.8343412280082703),\n",
       " ('ë°•ê·¼í˜œëŒ€í†µë ¹', 0.2979728579521179, 41, -1.4679926633834839),\n",
       " ('íƒœê· ', 0.29359349608421326, 531, -2.337681531906128),\n",
       " ('ë°˜ëŒ€ì§‘íšŒ', 0.2841551899909973, 67, -1.4197840690612793),\n",
       " ('í”Œë ˆì´ë¸Œ', 0.280100017786026, 183, -1.9951273202896118),\n",
       " ('ë°•ì •í¬ëŒ€í†µë ¹', 0.26961392164230347, 11, -0.8941963315010071),\n",
       " ('ê´‘ì£¼ì‹œë¯¼', 0.2681012451648712, 164, -1.3160964250564575),\n",
       " ('ìœ¤ì„ì—´ëŒ€í†µë ¹', 0.2633002996444702, 107, -0.7175803184509277),\n",
       " ('íƒ„í•µì°¬ì„±', 0.26183268427848816, 79, -1.875105381011963),\n",
       " ('êµ°ì‚¬ë°˜ë€', 0.2608729898929596, 24, -0.6887761354446411),\n",
       " ('ë¬¸í˜•ë°°', 0.26036399602890015, 499, -2.542275905609131),\n",
       " ('ìš”ëŒ€í•œë¯¼êµ­', 0.2583264410495758, 18, -1.8522074222564697),\n",
       " ('ìœ íŠœë²„', 0.25691354274749756, 228, -1.4269683361053467),\n",
       " ('í•©ë‹ˆë‹¤ëŒ€í•œë¯¼êµ­', 0.2529016435146332, 11, -1.9155044555664062),\n",
       " ('ëª…íƒœê· ê²Œì´íŠ¸', 0.25219473242759705, 10, -1.4267553091049194)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# Kiwi ê°ì²´ ìƒì„±\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# Load the cleaned video comments\n",
    "data = load_json('data/cleaned_video_comments.json')\n",
    "\n",
    "corpus = []\n",
    "for category, videos in data.items():\n",
    "    for video in videos:\n",
    "        comments = video.get(\"comments\", [])\n",
    "        corpus.extend(comments)  # ëª¨ë“  ëŒ“ê¸€ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ê°€\n",
    "\n",
    "kiwi.extract_add_words(corpus)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì €ì¥: data/kiwi_nouns_video_comments.json\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy.utils import Stopwords\n",
    "stopwords = Stopwords()\n",
    "\n",
    "# âœ… Kiwië¥¼ ì´ìš©í•œ í† í°í™” & ëª…ì‚¬ ì¶”ì¶œ\n",
    "tokenized_nouns = {}\n",
    "stopwords = Stopwords()\n",
    "\n",
    "data = load_json('data/cleaned_video_comments.json')\n",
    "\n",
    "for category, videos in data.items():\n",
    "    tokenized_nouns[category] = []  # ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "\n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        comments = video.get(\"comments\", [])\n",
    "\n",
    "        # ëŒ“ê¸€ì„ í˜•íƒœì†Œ ë¶„ì„ í›„ ëª…ì‚¬ë§Œ ì¶”ì¶œ\n",
    "        tokenized = []\n",
    "        for comment in comments:\n",
    "            tokens = kiwi.tokenize(comment)  # í˜•íƒœì†Œ ë¶„ì„\n",
    "            nouns = [\n",
    "                token.form for token in tokens\n",
    "                if token.tag in [\"NNG\", \"NNP\"]  # ëª…ì‚¬(NNG, NNP)ë§Œ ì¶”ì¶œ\n",
    "                and len(token.form) > 1  # 1ê¸€ì ì œì™¸\n",
    "                and (token.form, token.tag) not in stopwords  # âœ… ìˆ˜ì •ëœ í•„í„°ë§\n",
    "            ]\n",
    "            tokenized.append(nouns)\n",
    "\n",
    "        # êµ¬ì¡° ìœ ì§€\n",
    "        tokenized_nouns[category].append({\"video_id\": video_id, \"nouns\": tokenized})\n",
    "\n",
    "# âœ… ê²°ê³¼ JSON ì €ì¥\n",
    "save_to_json(tokenized_nouns, 'kiwi_nouns_video_comments.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KRWordRank(kiwi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì €ì¥: data/kiwi_data.json\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned video comments\n",
    "data = load_json('data/kiwi_nouns_video_comments.json')\n",
    "\n",
    "# âœ… ë°ì´í„°ë¥¼ `merge_values` í•¨ìˆ˜ì— ë§ê²Œ ë³€í˜•\n",
    "# (ì¹´í…Œê³ ë¦¬ â†’ ë¹„ë””ì˜¤ â†’ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ë¥¼ `merge_values`ì—ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë³€í˜•)\n",
    "transformed_data = {}\n",
    "\n",
    "for category, videos in data.items():\n",
    "    transformed_data[category] = []  # ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "\n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        nouns = video.get(\"nouns\", [])  # ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "        # ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í‰íƒ„í™” (nested list ì œê±°)\n",
    "        flat_nouns = [word for noun_list in nouns for word in noun_list]\n",
    "\n",
    "        # ë³‘í•©í•  ë°ì´í„°ë¡œ ë³€í™˜\n",
    "        transformed_data[category].append(flat_nouns)\n",
    "# âœ… `merge_values` ì ìš©\n",
    "processed_data = merge_values(transformed_data)\n",
    "\n",
    "save_to_json(processed_data,'kiwi_data.json')\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.word import KRWordRank\n",
    "\n",
    "min_count = 10   # ë‹¨ì–´ì˜ ìµœì†Œ ì¶œí˜„ ë¹ˆë„ìˆ˜ (ê·¸ë˜í”„ ìƒì„± ì‹œ)\n",
    "max_length = 10 # ë‹¨ì–´ì˜ ìµœëŒ€ ê¸¸ì´\n",
    "wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "\n",
    "data = load_json('data/kiwi_data.json')\n",
    "\n",
    "documents = list(data.values())  # ê° ì˜ìƒì˜ ëŒ“ê¸€ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
    "#save_to_json(documents, \"zzzzzz.json\")\n",
    "#save_to_json(documents,\"kkkkkkkk.json\")\n",
    "beta = 0.85    # PageRankì˜ ê°ì‡„ ê³„ìˆ˜\n",
    "max_iter = 10  # ë°˜ë³µ íšŸìˆ˜\n",
    "\n",
    "keywords, rank, graph = wordrank_extractor.extract(documents, beta, max_iter)\n",
    "\n",
    "top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ì˜ìƒ', 25.502771313687216),\n",
       " ('ì¤‘êµ­', 25.074480449318674),\n",
       " ('ê°ì‚¬', 22.73052987677838),\n",
       " ('ë‚˜ë¼', 21.934993295355003),\n",
       " ('êµ­ë¯¼', 19.50593275415689),\n",
       " ('ë¯¼ì£¼', 16.632856910458496),\n",
       " ('ìƒê°', 16.579941894027638),\n",
       " ('ëŒ€í†µë ¹', 16.24687301827613),\n",
       " ('ë¯¸êµ­', 15.288325852647821),\n",
       " ('í•œêµ­', 14.855848868533247),\n",
       " ('ì„ ìˆ˜', 12.069327406563033),\n",
       " ('íƒ„í•µ', 11.228954217748734),\n",
       " ('ì–¸ë¡ ', 10.513135664646446),\n",
       " ('ì‘ì›', 10.29886196699711),\n",
       " ('ëŒ€í•œ', 10.186445433564293),\n",
       " ('ì‚¬ë‘', 9.651541204189082),\n",
       " ('ì¸ê°„', 9.58221759097418),\n",
       " ('ê´‘ì£¼', 9.39334142255769),\n",
       " ('ê²Œì„', 8.721073688402956),\n",
       " ('ìœ¤ì„ì—´', 8.344642215758588),\n",
       " ('ìµœê³ ', 8.114560166445347),\n",
       " ('ì•„ì´', 8.063425272705212),\n",
       " ('ì´ìƒ', 7.720615919002939),\n",
       " ('ë¬¸ì œ', 7.538474090864509),\n",
       " ('ì •ë„', 7.48857273174012),\n",
       " ('ì‚¬ìš©', 7.15719455600075),\n",
       " ('ë‚´ë€', 7.041447022052002),\n",
       " ('ë…¸ë˜', 6.999396110327232),\n",
       " ('ì´ì¬ëª…', 6.977740407384103),\n",
       " ('ì •ì¹˜', 6.846065123879742),\n",
       " ('ììœ ', 6.807886469390505),\n",
       " ('ë°©ì†¡', 6.776148208969578),\n",
       " ('êµ­íšŒ', 6.696478355776185),\n",
       " ('íŠ¸ëŸ¼í”„', 6.694016136806612),\n",
       " ('ì´ë²ˆ', 6.667760698942341),\n",
       " ('ì†Œë¦¬', 6.548912710316956),\n",
       " ('ì“°ë ˆê¸°', 6.360723669650443),\n",
       " ('ê±°ì§“', 6.222909225734387),\n",
       " ('ì •ë³´', 6.17270083162189),\n",
       " ('êµ¬ì†', 6.104568143524099),\n",
       " ('ì •ì‹ ', 5.923236625423641),\n",
       " ('ê°¤ëŸ­ì‹œ', 5.893123487347862),\n",
       " ('ì˜ì›', 5.833160437672497),\n",
       " ('ì‹œê°„', 5.784321657397902),\n",
       " ('ì‚¼ì„±', 5.712288840515198),\n",
       " ('ëŒ“ê¸€', 5.655391920290096),\n",
       " ('ì¢ŒíŒŒ', 5.604409410564074),\n",
       " ('êµ­ê°€', 5.601752529485392),\n",
       " ('ê¸°ì', 5.551323749658223),\n",
       " ('ê³„ì—„', 5.533998645516685),\n",
       " ('ì¬íŒ', 5.503263869729048),\n",
       " ('ë²”ì£„', 5.4198393612246685),\n",
       " ('ì˜¤ëŠ˜', 5.381789025077249),\n",
       " ('í—Œì¬', 5.375934259112313),\n",
       " ('í•„ìš”', 5.361171866764325),\n",
       " ('ë‰´ìŠ¤', 5.206588810349325),\n",
       " ('ì¶•í•˜', 5.164886742117667),\n",
       " ('ì´ìœ ', 5.078649404227084),\n",
       " ('ìˆ˜ì¤€', 4.863568904719699),\n",
       " ('ê°€ëŠ¥', 4.812147726290076),\n",
       " ('ë‹¤ìŒ', 4.7435721677428235),\n",
       " ('ë¶€ì •', 4.717945612277831),\n",
       " ('ì§€ê¸ˆ', 4.658257556186458),\n",
       " ('ê³µì‚°', 4.618140989572579),\n",
       " ('ìŒì£¼', 4.617596251071208),\n",
       " ('êµ¬ë…', 4.59583392480476),\n",
       " ('ëª©ì†Œë¦¬', 4.571508143390411),\n",
       " ('ë§ˆìŒ', 4.528108439165859),\n",
       " ('ë…¸ì•„', 4.51867284866697),\n",
       " ('ì§‘íšŒ', 4.460205597691972),\n",
       " ('í”„ë¡œ', 4.362759590318432),\n",
       " ('ê²€ì°°', 4.358030512036283),\n",
       " ('ì²˜ë²Œ', 4.35022778262654),\n",
       " ('ì„¸ê³„', 4.3242813273234155),\n",
       " ('ë¬¸í˜•ë°°', 4.240315992711679),\n",
       " ('ìˆ˜ì‚¬', 4.185971142605684),\n",
       " ('ì„¸ìƒ', 4.152698848234621),\n",
       " ('í–‰ë³µ', 4.138329041167774),\n",
       " ('ê¸°ëŠ¥', 4.110538602361143),\n",
       " ('ì‹œì‘', 4.066593160070242),\n",
       " ('ëŠë‚Œ', 4.0634881979108055),\n",
       " ('êµ°ì¸', 3.9663593913625785),\n",
       " ('ê¸°ëŒ€', 3.9566605885003914),\n",
       " ('ìš”ì¦˜', 3.9431237387590548),\n",
       " ('ìœ íŠœ', 3.9104243669327734),\n",
       " ('ì²˜ìŒ', 3.9036439192397725),\n",
       " ('ìë‘', 3.891422403200031),\n",
       " ('ê¹€í˜„', 3.84001076218302),\n",
       " ('ì„œìš¸', 3.837908461181433),\n",
       " ('ì§„ì‹¤', 3.8151165276608405),\n",
       " ('ë””ìì¸', 3.7705200658084657),\n",
       " ('ì§€ì§€', 3.7579804435461046),\n",
       " ('ì†¡ê°€', 3.728475624895238),\n",
       " ('ê¹€ê±´', 3.710812120487687),\n",
       " ('ìš°ë¦¬', 3.6913531829034922),\n",
       " ('ì´ì œ', 3.6682065553923464),\n",
       " ('ì¤‘ê³µ', 3.631746313792637),\n",
       " ('ê¸°ìˆ ', 3.579770221198547),\n",
       " ('ì•„ì‚¬ë‹¬', 3.573046845507894),\n",
       " ('í•´ì²´', 3.5460711559967835)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# KRWordrank(konlpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì €ì¥: data/konlpy_data.json\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned video comments\n",
    "data = load_json('data/okt_nouns_video_comments.json')\n",
    "\n",
    "    \n",
    "# âœ… ë°ì´í„°ë¥¼ `merge_values` í•¨ìˆ˜ì— ë§ê²Œ ë³€í˜•\n",
    "# (ì¹´í…Œê³ ë¦¬ â†’ ë¹„ë””ì˜¤ â†’ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ë¥¼ `merge_values`ì—ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë³€í˜•)\n",
    "transformed_data = {}\n",
    "\n",
    "for category, videos in data.items():\n",
    "    transformed_data[category] = []  # ì¹´í…Œê³ ë¦¬ë³„ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "\n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        nouns = video.get(\"nouns\", [])  # ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n",
    "\n",
    "        # ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í‰íƒ„í™” (nested list ì œê±°)\n",
    "        flat_nouns = [word for noun_list in nouns for word in noun_list]\n",
    "\n",
    "        # ë³‘í•©í•  ë°ì´í„°ë¡œ ë³€í™˜\n",
    "        transformed_data[category].append(flat_nouns)\n",
    "# âœ… `merge_values` ì ìš©\n",
    "processed_data = merge_values(transformed_data)\n",
    "\n",
    "save_to_json(processed_data,'konlpy_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì €ì¥: data/all_keywords_ranking.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom krwordrank.word import KRWordRank\\n\\nmin_count = 10   # ë‹¨ì–´ì˜ ìµœì†Œ ì¶œí˜„ ë¹ˆë„ìˆ˜ (ê·¸ë˜í”„ ìƒì„± ì‹œ)\\nmax_length = 10  # ë‹¨ì–´ì˜ ìµœëŒ€ ê¸¸ì´\\nbeta = 0.85      # PageRankì˜ ê°ì‡„ ê³„ìˆ˜\\nmax_iter = 10    # ë°˜ë³µ íšŸìˆ˜\\n\\ndata = load_json(\\'data/konlpy_data.json\\')\\n\\ncategory_keywords = {}\\ni = 1\\nfor category, videos in data.items():\\n    \\n    if not documents:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°©ì§€\\n        continue\\n\\n    wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\\n    save_to_json(videos, f\"del{category}\")\\n    # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ\\n    keywords, rank, graph = wordrank_extractor.extract([videos], beta, max_iter)\\n    \\n    # í‚¤ì›Œë“œ ë­í‚¹ ì •ë ¬ í›„ ì €ì¥\\n    top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]\\n    category_keywords[category] = top_keywords\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_count = 10   # ë‹¨ì–´ì˜ ìµœì†Œ ì¶œí˜„ ë¹ˆë„ìˆ˜ (ê·¸ë˜í”„ ìƒì„± ì‹œ)\n",
    "max_length = 10 # ë‹¨ì–´ì˜ ìµœëŒ€ ê¸¸ì´\n",
    "wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "\n",
    "data = load_json('data/konlpy_data.json')\n",
    "\n",
    "documents = list(data.values())  # ê° ì˜ìƒì˜ ëŒ“ê¸€ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
    "\n",
    "beta = 0.85    # PageRankì˜ ê°ì‡„ ê³„ìˆ˜\n",
    "max_iter = 10  # ë°˜ë³µ íšŸìˆ˜\n",
    "\n",
    "keywords, rank, graph = wordrank_extractor.extract(documents, beta, max_iter)\n",
    "\n",
    "top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "save_to_json(top_keywords,\"all_keywords_ranking.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_keywords   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ì¹´í…Œê³ ë¦¬ë³„ ì „ì²´ ë­í‚¹ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ì „ì²˜ë¦¬ ë° ì •ë¦¬ ë‹¨ê³„ (ìµœì¢…ì ìœ¼ë¡œ ë‚˜ì˜¨ ê²°ê³¼ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬ í• ê±°ì„). all_categories ë””ë ‰í† ë¦¬ì— ê° ì¹´í…Œê³ ë¦¬ì˜ ëŒ“ê¸€ì„ jsoníŒŒì¼ë¡œ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê° ì¹´í…Œê³ ë¦¬ë³„ JSON íŒŒì¼ì´ 'all_categories' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "data = load_json('data/konlpy_data.json')\n",
    "\n",
    "# ì €ì¥í•  ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "output_dir = \"all_categories\"\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ì—†ìœ¼ë©´ ìƒì„±\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# ê° ì¹´í…Œê³ ë¦¬ë³„ JSON íŒŒì¼ ì €ì¥\n",
    "for category, videos in data.items():\n",
    "    # ì˜ìƒë“¤ì˜ ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥\n",
    "    documents = [comment for video_comments in videos for comment in video_comments]\n",
    "    category_text = \"\".join(documents)  # ëª¨ë“  ëŒ“ê¸€ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)\n",
    "\n",
    "    # ì¹´í…Œê³ ë¦¬ë³„ JSON íŒŒì¼ ì €ì¥ (íŒŒì¼ëª…ì€ ì¹´í…Œê³ ë¦¬ëª… ê·¸ëŒ€ë¡œ)\n",
    "    category_file_path = os.path.join(output_dir, f\"{category}.json\")\n",
    "    with open(category_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([category_text], f, ensure_ascii=False, indent=4)  # ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ì— ë„£ì–´ ì €ì¥\n",
    "\n",
    "print(f\"âœ… ê° ì¹´í…Œê³ ë¦¬ë³„ JSON íŒŒì¼ì´ '{output_dir}' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë­í‚¹ ì¶”ì¶œ (ì§€ê¸ˆì€ kiwiê°€ í›ˆë ¨ì´ ì•ˆë˜ì–´ ìˆì–´ì„œ ì“°ë ˆê¸° ë°ì´í„°ê°€ ì¶”ì¶œë¨) ver1 (category_keywords_ranking.json íŒŒì¼ì— ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ í‚¤ì›Œë“œ ë­í‚¹ì„ ì €ì¥)\n",
    "\n",
    "ê·¸ë¦¬ê³  ì§€í›ˆì´ê°€ ì§  ì½”ë“œê°€ ì¶”ì¶œí•˜ëŠ” all_keywords_ranking.json íŒŒì¼ì€ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜ê°€ ì•ˆë˜ë‹ˆê¹Œ, ëª¨ë¸ í›ˆë ¨ ë’¤ì— category_keywords_ranking.json ë¡œ í•˜ëŠ”ê²Œ ë‚˜ì„ë“¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  Pets & Animals ì¹´í…Œê³ ë¦¬ì— ë‹¨ì–´ê°€ ë¶€ì¡±í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œì„ ê±´ë„ˆëœ€\n",
      "ë°ì´í„° ì €ì¥: data/category_keywords_ranking.json\n",
      "âœ… ê° ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë­í‚¹ì´ 'category_keywords_ranking.json' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ë°ì´í„° ì €ì¥: data/all_keywords_ranking.json\n",
      "âœ… ì „ì²´ í‚¤ì›Œë“œ ë­í‚¹ì´ 'all_keywords_ranking.json' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    \\nmin_count = 10   # ë‹¨ì–´ì˜ ìµœì†Œ ì¶œí˜„ ë¹ˆë„ìˆ˜ (ê·¸ë˜í”„ ìƒì„± ì‹œ)\\nmax_length = 10 # ë‹¨ì–´ì˜ ìµœëŒ€ ê¸¸ì´\\nwordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\\n\\ndata = load_json(\\'data/konlpy_data.json\\')\\n\\ndocuments = list(data.values())  # ê° ì˜ìƒì˜ ëŒ“ê¸€ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\\n\\nbeta = 0.85    # PageRankì˜ ê°ì‡„ ê³„ìˆ˜\\nmax_iter = 10  # ë°˜ë³µ íšŸìˆ˜\\n\\nkeywords, rank, graph = wordrank_extractor.extract(documents, beta, max_iter)\\n\\ntop_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]\\nsave_to_json(top_keywords,\"all_keywords_ranking.json\")    \\n    '"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from krwordrank.word import KRWordRank\n",
    "\n",
    "# KRWordRank ì„¤ì •\n",
    "min_count = 10   # ë‹¨ì–´ì˜ ìµœì†Œ ì¶œí˜„ ë¹ˆë„ìˆ˜\n",
    "max_length = 10  # ë‹¨ì–´ì˜ ìµœëŒ€ ê¸¸ì´\n",
    "beta = 0.85      # PageRankì˜ ê°ì‡„ ê³„ìˆ˜\n",
    "max_iter = 10    # ë°˜ë³µ íšŸìˆ˜\n",
    "\n",
    "# `all_categories` í´ë”ì—ì„œ JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "input_dir = \"all_categories\"\n",
    "category_keywords = {}\n",
    "all_documents = []  # ì „ì²´ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        category_path = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # JSON íŒŒì¼ ë¡œë“œ\n",
    "        with open(category_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        category = filename.replace(\".json\", \"\")  # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°í•˜ì—¬ ì¹´í…Œê³ ë¦¬ëª…ìœ¼ë¡œ ì‚¬ìš©\n",
    "        text = data[0] if data else \"\"  # ë¦¬ìŠ¤íŠ¸ ë‚´ ì²« ë²ˆì§¸ ìš”ì†Œ(ë¬¸ìì—´) ê°€ì ¸ì˜¤ê¸°\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ë¥¼ ê³µë°± ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "        documents = text.split(\" \")\n",
    "\n",
    "        # ì „ì²´ ë°ì´í„°ì— ì¶”ê°€ (ì „ì²´ í‚¤ì›Œë“œ ë¶„ì„ìš©)\n",
    "        all_documents.extend(documents)\n",
    "\n",
    "        # ë¬¸ì„œ ë‚´ ë‹¨ì–´ê°€ 2ê°œ ì´ìƒ ìˆì–´ì•¼ KRWordRank ì‹¤í–‰ ê°€ëŠ¥\n",
    "        if len(documents) < 2:\n",
    "            print(f\"âš ï¸  {category} ì¹´í…Œê³ ë¦¬ì— ë‹¨ì–´ê°€ ë¶€ì¡±í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œì„ ê±´ë„ˆëœ€\")\n",
    "            continue\n",
    "\n",
    "        # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "        wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "        keywords, rank, graph = wordrank_extractor.extract(documents, beta, max_iter)\n",
    "\n",
    "        # ìƒìœ„ 100ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "        top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "        category_keywords[category] = top_keywords\n",
    "\n",
    "# âœ… ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë­í‚¹ ì €ì¥\n",
    "save_to_json(category_keywords, \"category_keywords_ranking.json\")\n",
    "\n",
    "print(\"âœ… ê° ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë­í‚¹ì´ 'category_keywords_ranking.json' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# âœ… ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ë¶„ì„í•˜ì—¬ í‚¤ì›Œë“œ ë­í‚¹ ì¶”ì¶œ\n",
    "if len(all_documents) >= 2:  # ìµœì†Œ ë‘ ê°œì˜ ë‹¨ì–´ê°€ ìˆì–´ì•¼ í•¨\n",
    "    wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "    keywords, rank, graph = wordrank_extractor.extract(all_documents, beta, max_iter)\n",
    "\n",
    "    # ìƒìœ„ 100ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "    top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "    save_to_json(top_keywords, \"all_keywords_ranking.json\")\n",
    "\n",
    "    print(\"âœ… ì „ì²´ í‚¤ì›Œë“œ ë­í‚¹ì´ 'all_keywords_ranking.json' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"âš ï¸ ì „ì²´ ë°ì´í„°ì— ë‹¨ì–´ê°€ ë¶€ì¡±í•˜ì—¬ í‚¤ì›Œë“œ ë­í‚¹ì„ ìƒì„±í•  ìˆ˜ ì—†ìŒ.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë­í‚¹ ì¶”ì¶œ ver2 (ë°”ë¡œ ìœ„ì˜ ì½”ë“œì™€ ë¹„ìŠ·í•¨) (category_ranking ë””ë ‰í† ë¦¬ì— ì¹´í…Œê³ ë¦¬ë³„ë¡œ í‚¤ì›Œë“œ ë­í‚¹ì´ jsoníŒŒì¼ë¡œ ìƒì„±ë˜ëŠ” ì½”ë“œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Film & Animation ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë­í‚¹ì´ 'category_ranking/Film & Animation_ranking.json' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… Music ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë­í‚¹ì´ 'category_ranking/Music_ranking.json' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âš ï¸  Pets & Animals ì¹´í…Œê³ ë¦¬ì— ë‹¨ì–´ê°€ ë¶€ì¡±í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œì„ ê±´ë„ˆëœ€\n",
      "âœ… Entertainment ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë­í‚¹ì´ 'category_ranking/Entertainment_ranking.json' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… News & Politics ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë­í‚¹ì´ 'category_ranking/News & Politics_ranking.json' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… Gaming ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë­í‚¹ì´ 'category_ranking/Gaming_ranking.json' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… Sports ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë­í‚¹ì´ 'category_ranking/Sports_ranking.json' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "âœ… Science & Technology ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë­í‚¹ì´ 'category_ranking/Science & Technology_ranking.json' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from krwordrank.word import KRWordRank\n",
    "\n",
    "# KRWordRank ì„¤ì •\n",
    "min_count = 10   # ë‹¨ì–´ì˜ ìµœì†Œ ì¶œí˜„ ë¹ˆë„ìˆ˜\n",
    "max_length = 10  # ë‹¨ì–´ì˜ ìµœëŒ€ ê¸¸ì´\n",
    "beta = 0.85      # PageRankì˜ ê°ì‡„ ê³„ìˆ˜\n",
    "max_iter = 10    # ë°˜ë³µ íšŸìˆ˜\n",
    "\n",
    "# ì…ë ¥ ë””ë ‰í† ë¦¬ (all_categories)\n",
    "input_dir = \"all_categories\"\n",
    "\n",
    "# ì¶œë ¥ ë””ë ‰í† ë¦¬ (category_ranking) ìƒì„±\n",
    "output_dir = \"category_ranking\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# ê° ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë­í‚¹ ì¶”ì¶œ\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        category_path = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # JSON íŒŒì¼ ë¡œë“œ\n",
    "        with open(category_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        category = filename.replace(\".json\", \"\")  # íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°í•˜ì—¬ ì¹´í…Œê³ ë¦¬ëª…ìœ¼ë¡œ ì‚¬ìš©\n",
    "        text = data[0] if data else \"\"  # ë¦¬ìŠ¤íŠ¸ ë‚´ ì²« ë²ˆì§¸ ìš”ì†Œ(ë¬¸ìì—´) ê°€ì ¸ì˜¤ê¸°\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ë¥¼ ê³µë°± ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "        documents = text.split(\" \")\n",
    "\n",
    "        # ë¬¸ì„œ ë‚´ ë‹¨ì–´ê°€ 2ê°œ ì´ìƒ ìˆì–´ì•¼ KRWordRank ì‹¤í–‰ ê°€ëŠ¥\n",
    "        if len(documents) < 2:\n",
    "            print(f\"âš ï¸  {category} ì¹´í…Œê³ ë¦¬ì— ë‹¨ì–´ê°€ ë¶€ì¡±í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œì„ ê±´ë„ˆëœ€\")\n",
    "            continue\n",
    "\n",
    "        \"\"\"\n",
    "        âœ… ê° ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë­í‚¹ ì¶”ì¶œ\n",
    "        \"\"\"\n",
    "        wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "        keywords, rank, graph = wordrank_extractor.extract(documents, beta, max_iter)\n",
    "\n",
    "        # ìƒìœ„ 100ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "        top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "        # ê²°ê³¼ ì €ì¥ (ê° ì¹´í…Œê³ ë¦¬ë³„ JSON íŒŒì¼ì„ `category_ranking` í´ë”ì— ì €ì¥)\n",
    "        ranking_file = os.path.join(output_dir, f\"{category}_ranking.json\")\n",
    "        with open(ranking_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(top_keywords, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"âœ… {category} ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë­í‚¹ì´ '{ranking_file}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì œëª©, íƒœê·¸ ê°€ì¤‘ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json('data/raw_video_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¹€ì–´ì¤€ì˜ ê²¸ì†ì€í˜ë“¤ë‹¤ ë‰´ìŠ¤ê³µì¥ 2025ë…„ 2ì›” 17ì¼ ì›”ìš”ì¼ [ê°•ê¸°ì •, ë…¸ì˜í¬, ë°•ì„ ì›, ê¹€ê²½í˜¸, ë°•ë²”ê³„, ì—¬ë¡ ì¡°ì‚¬]\n"
     ]
    }
   ],
   "source": [
    "title_tag = merge_title_tags(data)\n",
    "print(title_tag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Spacing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m cleaned_texts \u001b[38;5;241m=\u001b[39m [clean_text(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m title_tag]\n\u001b[1;32m      5\u001b[0m cleaned_texts\n\u001b[0;32m----> 7\u001b[0m ko_spacing \u001b[38;5;241m=\u001b[39m \u001b[43mSpacing\u001b[49m()\n\u001b[1;32m      9\u001b[0m merged_sentences \u001b[38;5;241m=\u001b[39m [ko_spacing(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m cleaned_texts]\n\u001b[1;32m     11\u001b[0m merged_sentences\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Spacing' is not defined"
     ]
    }
   ],
   "source": [
    "cleaned_texts = []\n",
    "\n",
    "cleaned_texts = [clean_text(text) for text in title_tag]\n",
    "    \n",
    "cleaned_texts\n",
    "\n",
    "ko_spacing = Spacing()\n",
    "\n",
    "merged_sentences = [ko_spacing(text) for text in cleaned_texts]\n",
    "\n",
    "merged_sentences\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Kiwië¥¼ ì´ìš©í•œ ëª…ì‚¬ ì¶”ì¶œ\u001b[39;00m\n\u001b[1;32m      2\u001b[0m kiwi_nouns_sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmerged_sentences\u001b[49m:\n\u001b[1;32m      5\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m kiwi\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[1;32m      6\u001b[0m     nouns \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mform \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mtag \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNNG\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNNP\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token\u001b[38;5;241m.\u001b[39mform) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "# Kiwië¥¼ ì´ìš©í•œ ëª…ì‚¬ ì¶”ì¶œ\n",
    "kiwi_nouns_sentences = []\n",
    "\n",
    "for text in merged_sentences:\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    nouns = [token.form for token in tokens if token.tag in [\"NNG\", \"NNP\"] and len(token.form) > 1]\n",
    "    kiwi_nouns_sentences.append(nouns)\n",
    "\n",
    "kiwi_nouns_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('The graph should consist of at least two nodes\\n', 'The node size of inserted graph is 0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.85\u001b[39m    \u001b[38;5;66;03m# PageRankì˜ ê°ì‡„ ê³„ìˆ˜\u001b[39;00m\n\u001b[1;32m     13\u001b[0m max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# ë°˜ë³µ íšŸìˆ˜\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m keywords, rank, graph \u001b[38;5;241m=\u001b[39m \u001b[43mwordrank_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m top_keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(keywords\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:\u001b[38;5;241m100\u001b[39m]\n",
      "File \u001b[0;32m~/soynlp_training_data/.venv/lib/python3.12/site-packages/krwordrank/word/_word.py:211\u001b[0m, in \u001b[0;36mKRWordRank.extract\u001b[0;34m(self, docs, beta, max_iter, num_keywords, num_rset, vocabulary, bias, rset)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract\u001b[39m(\u001b[38;5;28mself\u001b[39m, docs, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_keywords\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    165\u001b[0m     num_rset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocabulary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, rset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    It constructs word graph and trains ranks of each node using HITS algorithm.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    After training it selects suitable subwords as words.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m        >>> keywords, rank, graph = wordrank_extractor.extract(texts, beta, max_iter, verbose)\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     rank, graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     lset \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint2token(idx)[\u001b[38;5;241m0\u001b[39m]:r \u001b[38;5;28;01mfor\u001b[39;00m idx, r \u001b[38;5;129;01min\u001b[39;00m rank\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint2token(idx)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rset:\n",
      "File \u001b[0;32m~/soynlp_training_data/.venv/lib/python3.12/site-packages/krwordrank/word/_word.py:326\u001b[0m, in \u001b[0;36mKRWordRank.train\u001b[0;34m(self, docs, beta, max_iter, vocabulary, bias)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_index2vocab()\n\u001b[1;32m    324\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_word_graph(docs)\n\u001b[0;32m--> 326\u001b[0m rank \u001b[38;5;241m=\u001b[39m \u001b[43mhits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m            \u001b[49m\u001b[43msum_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnumber_of_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rank, graph\n",
      "File \u001b[0;32m~/soynlp_training_data/.venv/lib/python3.12/site-packages/krwordrank/graph/_rank.py:38\u001b[0m, in \u001b[0;36mhits\u001b[0;34m(graph, beta, max_iter, bias, verbose, sum_weight, number_of_nodes, converge)\u001b[0m\n\u001b[1;32m     35\u001b[0m     number_of_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(graph), \u001b[38;5;28mlen\u001b[39m(bias))\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m number_of_nodes \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe graph should consist of at least two nodes\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe node size of inserted graph is \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m number_of_nodes\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     43\u001b[0m dw \u001b[38;5;241m=\u001b[39m sum_weight \u001b[38;5;241m/\u001b[39m number_of_nodes\n\u001b[1;32m     44\u001b[0m rank \u001b[38;5;241m=\u001b[39m {node:dw \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mkeys()}\n",
      "\u001b[0;31mValueError\u001b[0m: ('The graph should consist of at least two nodes\\n', 'The node size of inserted graph is 0')"
     ]
    }
   ],
   "source": [
    "merged_sentences = []\n",
    "\n",
    "for values in kiwi_nouns_sentences:\n",
    "    if isinstance(values, list):  # ê°’ì´ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸\n",
    "        merged_sentence = \" \".join(\n",
    "            s for s in values if s.strip() and len(s.strip()) > 1\n",
    "        )  # ë¹ˆ ë¬¸ìì—´ & í•œ ê¸€ì ì œì™¸ í›„ ê³µë°±ìœ¼ë¡œ ì—°ê²°\n",
    "        merged_sentences.append(merged_sentence)\n",
    "    \n",
    "\n",
    "\n",
    "beta = 0.85    # PageRankì˜ ê°ì‡„ ê³„ìˆ˜\n",
    "max_iter = 10  # ë°˜ë³µ íšŸìˆ˜\n",
    "\n",
    "keywords, rank, graph = wordrank_extractor.extract(merged_sentences, beta, max_iter)\n",
    "\n",
    "top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ê´‘ì£¼', 21.883339764024786),\n",
       " ('ëŒ€í†µë ¹', 21.14930802419712),\n",
       " ('êµ­ë¯¼', 21.066524360917278),\n",
       " ('ë¯¼ì£¼', 17.903521408605034),\n",
       " ('ì¤‘êµ­', 14.755495150088827),\n",
       " ('ë¯¸êµ­', 14.219984383747393),\n",
       " ('ë‚˜ë¼', 13.290342295229914),\n",
       " ('ì‚¬ëŒ', 12.697506902207595),\n",
       " ('ìš°ë¦¬', 12.120483967025589),\n",
       " ('íƒ„í•µ', 12.114412133709308),\n",
       " ('ëŒ€í•œ', 11.135607329155505),\n",
       " ('í•œë™í›ˆ', 10.879634026849658),\n",
       " ('ì–¸ë¡ ', 10.203989812207837),\n",
       " ('ìœ¤ì„ì—´', 9.996447768514912),\n",
       " ('í•œêµ­', 8.879613386251465),\n",
       " ('ììœ ', 8.581780161186199),\n",
       " ('ì •ì¹˜', 8.448722047613371),\n",
       " ('ìƒê°', 8.13896711204735),\n",
       " ('ì¸ê°„', 7.869204905101948),\n",
       " ('íŠ¸ëŸ¼í”„', 7.204134168729392),\n",
       " ('ë‚´ë€', 7.20330227348747),\n",
       " ('ì§€ê¸ˆ', 6.771218567348587),\n",
       " ('ì§„ì§œ', 6.766817801477181),\n",
       " ('ì¢ŒíŒŒ', 6.467655625686199),\n",
       " ('ì •ë§', 6.44997889860016),\n",
       " ('ì´ì¬ëª…', 6.442539967645999),\n",
       " ('ê³„ì—„', 6.384078296097432),\n",
       " ('ì§‘íšŒ', 6.263202683852292),\n",
       " ('ë°©ì†¡', 6.197539372900395),\n",
       " ('ì‹œë¯¼', 6.101414643916003),\n",
       " ('ì“°ë ˆê¸°', 5.9595224050726365),\n",
       " ('ê¹€ê±´í¬', 5.930262026673098),\n",
       " ('ì‘ì›', 5.789553527904718),\n",
       " ('ê±°ì§“', 5.788991450295141),\n",
       " ('êµ­íšŒ', 5.730767645232271),\n",
       " ('í—Œì¬', 5.602791647780293),\n",
       " ('ê²€ì°°', 5.515897353983046),\n",
       " ('êµ­ê°€', 5.420032015619803),\n",
       " ('ë¶€ì •', 5.3511937164313235),\n",
       " ('êµ­í˜', 5.182193740831479),\n",
       " ('ì´ì œ', 5.179311080305875),\n",
       " ('í•˜ë‚˜', 5.173291089689636),\n",
       " ('ì „ë¼ë„', 5.109214575496954),\n",
       " ('ì˜ì›', 4.989661887427296),\n",
       " ('ë³€í˜¸', 4.813763403822942),\n",
       " ('ë‰´ìŠ¤', 4.604327750247186),\n",
       " ('ë³´ìˆ˜', 4.4946854659144595),\n",
       " ('ì¬íŒ', 4.365231551029191),\n",
       " ('ì†Œë¦¬', 4.272439610165227),\n",
       " ('ê¹€í˜„ì •', 4.210072597638495),\n",
       " ('ë•Œë¬¸', 4.205477126953501),\n",
       " ('ë¶í•œ', 4.165453711128667),\n",
       " ('ì¥ì„±', 4.149585108557824),\n",
       " ('êµ¬ì†', 4.04725063538093),\n",
       " ('ëŒ€í‘œ', 4.037122898067671),\n",
       " ('ê¸°ì', 3.9951583449985915),\n",
       " ('ëª¨ë‘', 3.965344732444664),\n",
       " ('ì •ì‹ ', 3.9436667209221494),\n",
       " ('í™”ì´íŒ…', 3.923224476799152),\n",
       " ('ì§€ì§€', 3.8944086604966395),\n",
       " ('í•´ì²´', 3.877620998924716),\n",
       " ('ê³µì‚°', 3.8724335122669467),\n",
       " ('ë°˜ëŒ€', 3.8533329515000987),\n",
       " ('ë°°ì‹ ì', 3.8502368920687826),\n",
       " ('ê·¹ìš°', 3.7600156917707923),\n",
       " ('ì „í•œê¸¸', 3.702940272882116),\n",
       " ('ì„¸ë ¥', 3.628194609242365),\n",
       " ('ìˆ˜ì‚¬', 3.621829709523002),\n",
       " ('ë¬¸ì œ', 3.6156090371507164),\n",
       " ('ìµœê³ ', 3.57038756115811),\n",
       " ('ê¹€ë³‘ì£¼', 3.4942996248508282),\n",
       " ('ìœ„í•´', 3.4785479034772333),\n",
       " ('ì¤‘ê³µ', 3.352202911442926),\n",
       " ('ìˆ˜ì¤€', 3.2748925913109366),\n",
       " ('ê·¸ëƒ¥', 3.2408305201835974),\n",
       " ('ì• êµ­', 3.2144645170912463),\n",
       " ('ë³´ê³ ', 3.2135380902133166),\n",
       " ('ëª…íƒœ', 3.15213680461609),\n",
       " ('ìš°íŒŒ', 3.0637982630495015),\n",
       " ('ì ˆëŒ€', 3.05675472287878),\n",
       " ('ê°„ì²©', 3.0239328349102124),\n",
       " ('ì €ëŸ°', 2.9700468412466288),\n",
       " ('í˜¸ë‚¨', 2.9120874769885905),\n",
       " ('ë³´ë„', 2.8111462053689302),\n",
       " ('ë°œì „', 2.7987970456592257),\n",
       " ('ì„ ë™', 2.7737436147964813),\n",
       " ('ì—­ì‚¬', 2.769319172185835),\n",
       " ('ë³µê·€', 2.756237069657814),\n",
       " ('í—Œë²•', 2.7266505823349765),\n",
       " ('ì´ìœ ', 2.7169482036559396),\n",
       " ('ì¡°ì‘', 2.6476384809073346),\n",
       " ('ì§€ì—­', 2.6327699144220365),\n",
       " ('ì‚¬ì‹¤', 2.62719260166544),\n",
       " ('ì œë°œ', 2.6211744120405744),\n",
       " ('ë²”ì£„', 2.619104760164978),\n",
       " ('ë¬´ìŠ¨', 2.6140136964487115),\n",
       " ('ì •ë„', 2.604586702212233),\n",
       " ('ì´ì¤€ì„', 2.5903641331294307),\n",
       " ('ì¼ë³¸', 2.560988416892556),\n",
       " ('ì§„ì‹¤', 2.5504779820564876)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     í‚¤ì›Œë“œ  TF-IDF ì ìˆ˜\n",
      "0     êµ­ë¯¼   0.312915\n",
      "1     ê´‘ì£¼   0.268098\n",
      "2     ë‚˜ë¼   0.267983\n",
      "3    ë¯¼ì£¼ë‹¹   0.259020\n",
      "4    ëŒ€í†µë ¹   0.238910\n",
      "5     ë¯¸êµ­   0.204320\n",
      "6     ì¤‘êµ­   0.182256\n",
      "7   ëŒ€í•œë¯¼êµ­   0.177315\n",
      "8     ê°ì‚¬   0.140772\n",
      "9     ì–¸ë¡    0.134911\n",
      "10    íƒ„í•µ   0.133877\n",
      "11    í•œêµ­   0.130659\n",
      "12   í•œë™í›ˆ   0.128016\n",
      "13    ìƒê°   0.124569\n",
      "14   ìœ¤ì„ì—´   0.119742\n",
      "15    ì¸ê°„   0.115260\n",
      "16   íŠ¸ëŸ¼í”„   0.103654\n",
      "17    ì¢ŒíŒŒ   0.100206\n",
      "18    ì˜ì›   0.098368\n",
      "19    ì •ì¹˜   0.093197\n",
      "20   ì´ì¬ëª…   0.091128\n",
      "21    ì§‘íšŒ   0.088485\n",
      "22    ë‚´ë€   0.088370\n",
      "23    ê²€ì°°   0.086646\n",
      "24    ë°©ì†¡   0.085153\n",
      "25    í—Œì¬   0.084003\n",
      "26    ê³„ì—„   0.081705\n",
      "27    ì‘ì›   0.078487\n",
      "28    êµ­ê°€   0.076994\n",
      "29  ë¶€ì •ì„ ê±°   0.076879\n",
      "30    ììœ    0.075500\n",
      "31   ì „ë¼ë„   0.075040\n",
      "32   ì“°ë ˆê¸°   0.073086\n",
      "33    êµ­íšŒ   0.070558\n",
      "34    ë³´ìˆ˜   0.068605\n",
      "35    ìˆ˜ì‚¬   0.068375\n",
      "36   ê±°ì§“ë§   0.066881\n",
      "37    ì •ì‹    0.065962\n",
      "38    êµ¬ì†   0.063778\n",
      "39    ì†Œë¦¬   0.062054\n",
      "40    ê¸°ì   0.060790\n",
      "41    ì¥ì„±   0.059411\n",
      "42   ê¹€í˜„ì •   0.057803\n",
      "43    ë¶í•œ   0.057458\n",
      "44    ê·¹ìš°   0.055734\n",
      "45  ê´‘ì£¼ì‹œë¯¼   0.055389\n",
      "46    ë¬¸ì œ   0.054815\n",
      "47   ë³€í˜¸ì‚¬   0.054240\n",
      "48   ê¹€ê±´í¬   0.053321\n",
      "49    ëŒ€í‘œ   0.052746\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# âœ… ì˜ˆì œ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ì œëª© + íƒœê·¸ ì¡°í•©)\n",
    "data = load_json('data/kiwi_data.json')\n",
    "\n",
    "documents = list(data.values())  # ê° ì˜ìƒ ëŒ“ê¸€ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
    "\n",
    "# âœ… TF-IDF ëª¨ë¸ ì ìš©\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# âœ… í‚¤ì›Œë“œë³„ TF-IDF ì ìˆ˜ ê³„ì‚°\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "word_tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "sorted_indices = word_tfidf_scores.argsort()[::-1]  # ì¤‘ìš”ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "keywords_ranked = [(feature_names[i], word_tfidf_scores[i]) for i in sorted_indices]\n",
    "\n",
    "# âœ… ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ ì¶œë ¥\n",
    "df_keywords = pd.DataFrame(keywords_ranked, columns=[\"í‚¤ì›Œë“œ\", \"TF-IDF ì ìˆ˜\"])\n",
    "print(df_keywords.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í‚¤ì›Œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì €ì¥: data/view_like_counts.json\n"
     ]
    }
   ],
   "source": [
    "data = load_json('data/kiwi_data.json')\n",
    "\n",
    "# Load the raw video data\n",
    "raw_data = load_json('data/raw_video_data.json')\n",
    "\n",
    "# Extract view count and like count for each video id\n",
    "view_like_counts = {}\n",
    "for video_id, text in data.items():\n",
    "    for category, videos in raw_data.items():\n",
    "        for video in videos:\n",
    "            if video[\"video_id\"] == video_id:\n",
    "                view_like_counts[video_id] = {\n",
    "                    \"view_count\": video[\"view_count\"],\n",
    "                    \"like_count\": video[\"like_count\"],\n",
    "                    \"text\": text\n",
    "                }\n",
    "                break\n",
    "\n",
    "save_to_json(view_like_counts, 'view_like_counts.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keyword ì¢‹ì•„ìš”, ì¡°íšŒìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [í‚¤ì›Œë“œ, ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ë“±ì¥ íšŸìˆ˜]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "data = load_json('data/view_like_counts.json')\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# âœ… í‚¤ì›Œë“œë³„ë¡œ ì¡°íšŒìˆ˜ & ì¢‹ì•„ìš”ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "keyword_stats = defaultdict(lambda: {\"view_count\": 0, \"like_count\": 0, \"ë“±ì¥ íšŸìˆ˜\": 0})\n",
    "\n",
    "# âœ… ê° ë¹„ë””ì˜¤ì—ì„œ í‚¤ì›Œë“œë³„ ì¡°íšŒìˆ˜ & ì¢‹ì•„ìš” ì ìˆ˜ í•©ì‚°\n",
    "for video_id, info in data.items():\n",
    "    text = info[\"text\"]\n",
    "    view_count = info[\"view_count\"]\n",
    "    like_count = info[\"like_count\"]\n",
    "\n",
    "    # ğŸ”¥ í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°\n",
    "    word_list = text.split()\n",
    "    word_counts = defaultdict(int)\n",
    "    for word in word_list:\n",
    "        word_counts[word] += 1\n",
    "\n",
    "    # ğŸ”¥ í‚¤ì›Œë“œë³„ ì ìˆ˜ ë¶€ì—¬ (ë¹„ë””ì˜¤ì˜ ì ìˆ˜ë¥¼ í•´ë‹¹ í‚¤ì›Œë“œì— ë¶„ë°°)\n",
    "    for word, count in word_counts.items():\n",
    "        keyword_stats[word][\"view_count\"] += (view_count * count) / len(word_list)\n",
    "        keyword_stats[word][\"like_count\"] += (like_count * count) / len(word_list)\n",
    "        keyword_stats[word][\"ë“±ì¥ íšŸìˆ˜\"] += count\n",
    "\n",
    "# âœ… í‚¤ì›Œë“œë³„ ì •ë ¬ (ì¡°íšŒìˆ˜ ê¸°ì¤€)\n",
    "sorted_keywords = sorted(keyword_stats.items(), key=lambda x: x[1][\"view_count\"], reverse=True)\n",
    "\n",
    "# âœ… ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "df_keywords = pd.DataFrame([(k, v[\"view_count\"], v[\"like_count\"], v[\"ë“±ì¥ íšŸìˆ˜\"]) for k, v in sorted_keywords],\n",
    "                           columns=[\"í‚¤ì›Œë“œ\", \"ì¡°íšŒìˆ˜\", \"ì¢‹ì•„ìš”\", \"ë“±ì¥ íšŸìˆ˜\"])\n",
    "\n",
    "# âœ… ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ ì¶œë ¥\n",
    "print(df_keywords.head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('The graph should consist of at least two nodes\\n', 'The node size of inserted graph is 0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# âœ… KRWordRank ëª¨ë¸ ì ìš©\u001b[39;00m\n\u001b[1;32m     10\u001b[0m wordrank_extractor \u001b[38;5;241m=\u001b[39m KRWordRank(min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m keywords, rank, graph \u001b[38;5;241m=\u001b[39m \u001b[43mwordrank_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.85\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(keywords)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# âœ… í‚¤ì›Œë“œë³„ ì¡°íšŒìˆ˜ & ì¢‹ì•„ìš” ë°˜ì˜í•  ë”•ì…”ë„ˆë¦¬\u001b[39;00m\n",
      "File \u001b[0;32m~/soynlp_training_data/.venv/lib/python3.12/site-packages/krwordrank/word/_word.py:211\u001b[0m, in \u001b[0;36mKRWordRank.extract\u001b[0;34m(self, docs, beta, max_iter, num_keywords, num_rset, vocabulary, bias, rset)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract\u001b[39m(\u001b[38;5;28mself\u001b[39m, docs, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_keywords\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    165\u001b[0m     num_rset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocabulary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, rset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    It constructs word graph and trains ranks of each node using HITS algorithm.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    After training it selects suitable subwords as words.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m        >>> keywords, rank, graph = wordrank_extractor.extract(texts, beta, max_iter, verbose)\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     rank, graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     lset \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint2token(idx)[\u001b[38;5;241m0\u001b[39m]:r \u001b[38;5;28;01mfor\u001b[39;00m idx, r \u001b[38;5;129;01min\u001b[39;00m rank\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint2token(idx)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rset:\n",
      "File \u001b[0;32m~/soynlp_training_data/.venv/lib/python3.12/site-packages/krwordrank/word/_word.py:326\u001b[0m, in \u001b[0;36mKRWordRank.train\u001b[0;34m(self, docs, beta, max_iter, vocabulary, bias)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_index2vocab()\n\u001b[1;32m    324\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_word_graph(docs)\n\u001b[0;32m--> 326\u001b[0m rank \u001b[38;5;241m=\u001b[39m \u001b[43mhits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m            \u001b[49m\u001b[43msum_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnumber_of_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rank, graph\n",
      "File \u001b[0;32m~/soynlp_training_data/.venv/lib/python3.12/site-packages/krwordrank/graph/_rank.py:38\u001b[0m, in \u001b[0;36mhits\u001b[0;34m(graph, beta, max_iter, bias, verbose, sum_weight, number_of_nodes, converge)\u001b[0m\n\u001b[1;32m     35\u001b[0m     number_of_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(graph), \u001b[38;5;28mlen\u001b[39m(bias))\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m number_of_nodes \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe graph should consist of at least two nodes\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe node size of inserted graph is \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m number_of_nodes\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     43\u001b[0m dw \u001b[38;5;241m=\u001b[39m sum_weight \u001b[38;5;241m/\u001b[39m number_of_nodes\n\u001b[1;32m     44\u001b[0m rank \u001b[38;5;241m=\u001b[39m {node:dw \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mkeys()}\n",
      "\u001b[0;31mValueError\u001b[0m: ('The graph should consist of at least two nodes\\n', 'The node size of inserted graph is 0')"
     ]
    }
   ],
   "source": [
    "from krwordrank.word import KRWordRank\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# âœ… KRWordRankë¥¼ ì ìš©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "texts = [info[\"text\"] for info in data.values()]\n",
    "\n",
    "print(texts)\n",
    "# âœ… KRWordRank ëª¨ë¸ ì ìš©\n",
    "wordrank_extractor = KRWordRank(min_count=1, max_length=10, verbose=False)\n",
    "keywords, rank, graph = wordrank_extractor.extract(texts, beta=0.85, max_iter=10)\n",
    "\n",
    "print(keywords)\n",
    "\n",
    "# âœ… í‚¤ì›Œë“œë³„ ì¡°íšŒìˆ˜ & ì¢‹ì•„ìš” ë°˜ì˜í•  ë”•ì…”ë„ˆë¦¬\n",
    "keyword_stats = defaultdict(lambda: {\"view_count\": 0, \"like_count\": 0, \"ì ìˆ˜\": 0})\n",
    "\n",
    "# âœ… ê° ë¹„ë””ì˜¤ì—ì„œ í‚¤ì›Œë“œë³„ ì¡°íšŒìˆ˜ & ì¢‹ì•„ìš” ì ìˆ˜ í•©ì‚°\n",
    "for video_id, info in data.items():\n",
    "    text = info[\"text\"]\n",
    "    view_count = info[\"view_count\"]\n",
    "    like_count = info[\"like_count\"]\n",
    "\n",
    "    # ğŸ”¥ KRWordRankì—ì„œ ì¶”ì¶œí•œ í‚¤ì›Œë“œê°€ í•´ë‹¹ ë¬¸ì¥ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸\n",
    "    for keyword in keywords.keys():\n",
    "        if keyword in text:\n",
    "            keyword_stats[keyword][\"view_count\"] += view_count\n",
    "            keyword_stats[keyword][\"like_count\"] += like_count\n",
    "            keyword_stats[keyword][\"ì ìˆ˜\"] += rank.get(keyword, 0) * (view_count * 0.7 + like_count * 0.3)  # âœ… KeyError ë°©ì§€\n",
    "\n",
    "# âœ… í‚¤ì›Œë“œë³„ ì •ë ¬ (ì ìˆ˜ ê¸°ì¤€)\n",
    "sorted_keywords = sorted(keyword_stats.items(), key=lambda x: x[1][\"ì ìˆ˜\"], reverse=True)\n",
    "\n",
    "# âœ… ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "df_keywords = pd.DataFrame([(k, v[\"view_count\"], v[\"like_count\"], v[\"ì ìˆ˜\"]) for k, v in sorted_keywords],\n",
    "                           columns=[\"í‚¤ì›Œë“œ\", \"ì¡°íšŒìˆ˜\", \"ì¢‹ì•„ìš”\", \"ì ìˆ˜\"])\n",
    "\n",
    "# âœ… ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ ì¶œë ¥\n",
    "print(df_keywords.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì¸ê¸°ë™ì˜ìƒì˜ í‚¤ì›Œë“œì—ëŠ” ê°€ì¤‘ì¹˜ ë¶€ì—¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kiwi_nouns_video_comments ëŒ“ê¸€ì„ 4ë“±ë¶„ í›„ ëŒ“ê¸€ í‰íƒ„í™”\n",
    "ì˜ˆì‹œ\n",
    "{\n",
    "    \"News & Politics\": [\n",
    "        {\n",
    "            \"video_id\": \"IC3iNhz02l0\",\n",
    "            \"nouns\": [\n",
    "                [\n",
    "                    \"ìœ¤ì„ì—´\",\n",
    "                    \"ë³€í˜¸ì¸\",....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„°ê°€ 'data/flattened_nouns_groups.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "file_path = \"data/kiwi_nouns_video_comments.json\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# âœ… ì¹´í…Œê³ ë¦¬ë³„ 4ë“±ë¶„ ë° í‰íƒ„í™”\n",
    "ranked_groups = {}\n",
    "\n",
    "for category, videos in data.items():\n",
    "    total_videos = len(videos)\n",
    "    quarter_size = max(1, total_videos // 4)  # ìµœì†Œ 1ê°œ ìœ ì§€\n",
    "\n",
    "    # âœ… 4ê°œ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê¸°\n",
    "    ranked_groups[category] = {\n",
    "        \"1_25\": videos[:quarter_size],  # ìƒìœ„ 25%\n",
    "        \"25_50\": videos[quarter_size : quarter_size * 2],  # 25% ~ 50%\n",
    "        \"50_75\": videos[quarter_size * 2 : quarter_size * 3],  # 50% ~ 75%\n",
    "        \"75_100\": videos[quarter_size * 3 :]  # 75% ~ 100%\n",
    "    }\n",
    "\n",
    "# âœ… ê° ê·¸ë£¹ë³„ í‰íƒ„í™” ì§„í–‰\n",
    "flattened_groups = {}\n",
    "\n",
    "\n",
    "\n",
    "for category, groups in ranked_groups.items():\n",
    "    flattened_groups[category] = {}\n",
    "\n",
    "    for group_name, group_videos in groups.items():\n",
    "        # âœ… ëª…ì‚¬(nouns) ë¦¬ìŠ¤íŠ¸ë¥¼ í‰íƒ„í™”\n",
    "        all_nouns = [\n",
    "            word for video in group_videos if isinstance(video, dict) and \"nouns\" in video\n",
    "            for noun_list in video[\"nouns\"]\n",
    "            for word in noun_list\n",
    "        ]\n",
    "        flattened_groups[category][group_name] = all_nouns\n",
    "\n",
    "# âœ… ê²°ê³¼ ì €ì¥\n",
    "output_path = \"data/flattened_nouns_groups.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(flattened_groups, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… ë°ì´í„°ê°€ '{output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í‰íƒ„í™”ëœ ê° ê·¸ë£¹ë§ˆë‹¤ nounì˜ ë¬¸ìì—´ ë§Œë“¤ê¸°.\n",
    "\n",
    "ì˜ˆì‹œ \n",
    "{\n",
    "    \"News & Politics\": {\n",
    "        \"1_25\": \"ìœ¤ì„ì—´ ë³€í˜¸ì¸ ì´ì•¼ê¸° ì“°ë ˆê¸° ëŠë‚Œ ì²˜ë¦¬ ë‚˜ë¼ ì¥ì› ì§„ìˆ  ê±°ì§“ í”„ë¡œì íŠ¸ ì´ˆë“± ì•„ì´ ì•„ì´ í‰ìƒ ì¶”ì–µ ê°€ì¡± ë‹¨ìœ„ ê³„íš ìƒê° í˜•ì œ ë‚˜ë¼ ì¼ë°˜ ìš°ì­ˆì­ˆ ìƒê° íŠ¸ëŸ¼í”„ í˜•ì œ í˜•ì œ ìƒê° ê¹€ì–´ì¤€ ìê¸° ê³¼ì‹œ í™•ì¸ ì§„ìƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê° ê·¸ë£¹ë³„ ë¬¸ìì—´ ë³€í™˜ ì™„ë£Œ! 'data/group_texts.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ë¡œë“œ\n",
    "file_path = \"data/flattened_nouns_groups.json\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    flattened_data = json.load(f)\n",
    "\n",
    "# âœ… ê° ê·¸ë£¹ë³„ ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "group_texts = {}\n",
    "\n",
    "for category, groups in flattened_data.items():\n",
    "    group_texts[category] = {}\n",
    "\n",
    "    for group_name, all_nouns in groups.items():\n",
    "        # âœ… ëª…ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ê³µë°±ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "        group_text = \" \".join(all_nouns)\n",
    "        group_texts[category][group_name] = group_text\n",
    "\n",
    "# âœ… ê²°ê³¼ ì €ì¥\n",
    "output_path = \"data/group_texts.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(group_texts, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… ê° ê·¸ë£¹ë³„ ë¬¸ìì—´ ë³€í™˜ ì™„ë£Œ! '{output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê° ì¹´í…Œê³ ë¦¬ì˜ ê° ê·¸ë£¹ì„ categories ë””ë ‰í† ë¦¬ì— jsoníŒŒì¼ë¡œ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… News_&_Politics_1_25.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… News_&_Politics_25_50.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… News_&_Politics_50_75.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… News_&_Politics_75_100.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Film_&_Animation_1_25.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Film_&_Animation_25_50.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Film_&_Animation_50_75.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Film_&_Animation_75_100.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Music_1_25.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Music_25_50.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Music_50_75.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Music_75_100.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Pets_&_Animals_1_25.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Pets_&_Animals_25_50.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Pets_&_Animals_50_75.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Pets_&_Animals_75_100.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Sports_1_25.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Sports_25_50.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Sports_50_75.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Sports_75_100.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Gaming_1_25.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Gaming_25_50.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Gaming_50_75.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Gaming_75_100.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Entertainment_1_25.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Entertainment_25_50.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Entertainment_50_75.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Entertainment_75_100.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Science_&_Technology_1_25.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Science_&_Technology_25_50.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Science_&_Technology_50_75.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… Science_&_Technology_75_100.json ì €ì¥ ì™„ë£Œ!\n",
      "âœ… ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ êµ¬ì—­ JSON íŒŒì¼ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# âœ… JSON íŒŒì¼ ë¡œë“œ\n",
    "file_path = \"data/group_texts.json\"\n",
    "output_dir = \"data/categories\"  # ì €ì¥ë  ë””ë ‰í† ë¦¬\n",
    "\n",
    "# âœ… ì €ì¥í•  ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    group_texts = json.load(f)  # ì „ì²´ ë°ì´í„° ë¡œë“œ\n",
    "\n",
    "# âœ… ê° ì¹´í…Œê³ ë¦¬ë³„ êµ¬ì—­ì„ ê°œë³„ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "for category, groups in group_texts.items():\n",
    "    for group_name, text in groups.items():\n",
    "        # âœ… íŒŒì¼ëª… ìƒì„± (ì˜ˆ: News & Politics_1_25.json)\n",
    "        safe_category = category.replace(\" \", \"_\")  # ê³µë°± ì œê±°\n",
    "        file_name = f\"{safe_category}_{group_name}.json\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "        # âœ… JSON ì €ì¥\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(text, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"âœ… {file_name} ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ êµ¬ì—­ JSON íŒŒì¼ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===============ê²°êµ­ ì‹¤íŒ¨, ì™œì¸ì§€ ëª¨ë¥´ê² ì§€ë§Œ, ìˆœìœ„ê°€ ê²¹ì¹¨=============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Film_&_Animation_75_100.json: KRWordRank ì‹¤í–‰ ì™„ë£Œ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: 221)\n",
      "âœ… Film_&_Animation_75_100_ranking.json ì €ì¥ ì™„ë£Œ!\n",
      "ğŸš¨ Film_&_Animation_1_25.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'NoneType' object has no attribute 'items'\n",
      "âœ… Entertainment_25_50.json: KRWordRank ì‹¤í–‰ ì™„ë£Œ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: 270)\n",
      "âœ… Entertainment_25_50_ranking.json ì €ì¥ ì™„ë£Œ!\n",
      "ğŸš¨ Entertainment_1_25.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'NoneType' object has no attribute 'items'\n",
      "âœ… Science_&_Technology_1_25.json: KRWordRank ì‹¤í–‰ ì™„ë£Œ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: 937)\n",
      "âœ… Science_&_Technology_1_25_ranking.json ì €ì¥ ì™„ë£Œ!\n",
      "ğŸš¨ Film_&_Animation_25_50.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'NoneType' object has no attribute 'items'\n",
      "âœ… Entertainment_50_75.json: KRWordRank ì‹¤í–‰ ì™„ë£Œ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: 18)\n",
      "âœ… Entertainment_50_75_ranking.json ì €ì¥ ì™„ë£Œ!\n",
      "ğŸš¨ News_&_Politics_75_100.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'NoneType' object has no attribute 'items'\n",
      "âœ… Music_75_100.json: KRWordRank ì‹¤í–‰ ì™„ë£Œ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: 229)\n",
      "âœ… Music_75_100_ranking.json ì €ì¥ ì™„ë£Œ!\n",
      "ğŸš¨ Gaming_75_100.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'NoneType' object has no attribute 'items'\n",
      "âœ… Film_&_Animation_50_75.json: KRWordRank ì‹¤í–‰ ì™„ë£Œ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: 30)\n",
      "âœ… Film_&_Animation_50_75_ranking.json ì €ì¥ ì™„ë£Œ!\n",
      "ğŸš¨ Science_&_Technology_50_75.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'NoneType' object has no attribute 'items'\n",
      "âœ… News_&_Politics_1_25.json: KRWordRank ì‹¤í–‰ ì™„ë£Œ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: 1597)\n",
      "âœ… News_&_Politics_1_25_ranking.json ì €ì¥ ì™„ë£Œ!\n",
      "ğŸš¨ Gaming_50_75.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'NoneType' object has no attribute 'items'\n",
      "ğŸš¨ Pets_&_Animals_1_25.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: ('The graph should consist of at least two nodes\\n', 'The node size of inserted graph is 0')\n",
      "âœ… Science_&_Technology_25_50.json: KRWordRank ì‹¤í–‰ ì™„ë£Œ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: 586)\n",
      "âœ… Science_&_Technology_25_50_ranking.json ì €ì¥ ì™„ë£Œ!\n",
      "ğŸš¨ Gaming_25_50.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'NoneType' object has no attribute 'items'\n",
      "âœ… Entertainment_75_100.json: KRWordRank ì‹¤í–‰ ì™„ë£Œ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: 420)\n",
      "âœ… Entertainment_75_100_ranking.json ì €ì¥ ì™„ë£Œ!\n",
      "ğŸš¨ Pets_&_Animals_75_100.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'NoneType' object has no attribute 'items'\n",
      "âœ… News_&_Politics_50_75.json: KRWordRank ì‹¤í–‰ ì™„ë£Œ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: 1458)\n",
      "âœ… News_&_Politics_50_75_ranking.json ì €ì¥ ì™„ë£Œ!\n",
      "ğŸš¨ Sports_1_25.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'NoneType' object has no attribute 'items'\n",
      "ğŸš¨ Pets_&_Animals_50_75.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: ('The graph should consist of at least two nodes\\n', 'The node size of inserted graph is 0')\n",
      "âœ… Music_50_75.json: KRWordRank ì‹¤í–‰ ì™„ë£Œ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: 35)\n",
      "âœ… Music_50_75_ranking.json ì €ì¥ ì™„ë£Œ!\n",
      "ğŸš¨ Sports_75_100.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'NoneType' object has no attribute 'items'\n",
      "âœ… News_&_Politics_25_50.json: KRWordRank ì‹¤í–‰ ì™„ë£Œ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: 1457)\n",
      "âœ… News_&_Politics_25_50_ranking.json ì €ì¥ ì™„ë£Œ!\n",
      "ğŸš¨ Science_&_Technology_75_100.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'NoneType' object has no attribute 'items'\n",
      "âœ… Music_25_50.json: KRWordRank ì‹¤í–‰ ì™„ë£Œ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: 196)\n",
      "âœ… Music_25_50_ranking.json ì €ì¥ ì™„ë£Œ!\n",
      "ğŸš¨ Pets_&_Animals_25_50.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'NoneType' object has no attribute 'items'\n",
      "âœ… Music_1_25.json: KRWordRank ì‹¤í–‰ ì™„ë£Œ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: 62)\n",
      "âœ… Music_1_25_ranking.json ì €ì¥ ì™„ë£Œ!\n",
      "ğŸš¨ Sports_25_50.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'NoneType' object has no attribute 'items'\n",
      "âœ… Gaming_1_25.json: KRWordRank ì‹¤í–‰ ì™„ë£Œ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: 447)\n",
      "âœ… Gaming_1_25_ranking.json ì €ì¥ ì™„ë£Œ!\n",
      "ğŸš¨ Sports_50_75.json: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'NoneType' object has no attribute 'items'\n",
      "âœ… ëª¨ë“  êµ¬ì—­ì˜ í‚¤ì›Œë“œ ë­í‚¹ ì¶”ì¶œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from krwordrank.word import KRWordRank\n",
    "\n",
    "# âœ… ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "input_dir = \"/Users/melon/soynlp_training_data/youtube/scripts/data/categories\"      # ì›ë³¸ JSON íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬\n",
    "output_dir = \"data/ranked_keywords\"  # í‚¤ì›Œë“œ ë­í‚¹ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "\n",
    "# âœ… ì €ì¥í•  ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# âœ… KRWordRank ì„¤ì •\n",
    "min_count = 5   # ìµœì†Œ ë“±ì¥ íšŸìˆ˜\n",
    "max_length = 10 # ìµœëŒ€ ë‹¨ì–´ ê¸¸ì´\n",
    "beta = 0.85     # PageRank ê°ì‡„ ê³„ìˆ˜\n",
    "max_iter = 10     # ë°˜ë³µ íšŸìˆ˜\n",
    "\n",
    "wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "\n",
    "# âœ… ë””ë ‰í† ë¦¬ ë‚´ JSON íŒŒì¼ í•˜ë‚˜ì”© ì½ì–´ì˜¤ê¸°\n",
    "json_files = [f for f in os.listdir(input_dir) if f.endswith(\".json\")]\n",
    "\n",
    "\n",
    "for file_name in json_files:\n",
    "    input_path = os.path.join(input_dir, file_name)\n",
    "    \n",
    "    # âœ… JSON íŒŒì¼ ë¡œë“œ\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = json.load(f)  # í…ìŠ¤íŠ¸ ë¡œë“œ\n",
    "\n",
    "\n",
    "    # âœ… KRWordRank ì‹¤í–‰\n",
    "    try:\n",
    "\n",
    "        keywords, _, _ = wordrank_extractor.extract([text], beta, max_iter)\n",
    "        # âœ… KRWordRank ì‹¤í–‰ í›„ í™•ì¸\n",
    "        if keywords is None:\n",
    "            print(f\"âš ï¸ {file_name}: í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨ (None ë°˜í™˜). ì›ë³¸ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\")\n",
    "            with open(f\"data/debug_{file_name}\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(text, f, indent=4, ensure_ascii=False)\n",
    "            continue\n",
    "        print(f\"âœ… {file_name}: KRWordRank ì‹¤í–‰ ì™„ë£Œ (ì¶”ì¶œëœ í‚¤ì›Œë“œ ê°œìˆ˜: {len(keywords)})\")\n",
    "\n",
    "\n",
    "        if not keywords:\n",
    "            print(f\"âš ï¸ {file_name}: í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨. ê±´ë„ˆëœ€.\")\n",
    "            continue\n",
    "\n",
    "        # âœ… ê°œë³„ í‚¤ì›Œë“œ ë­í‚¹ JSON íŒŒì¼ ì €ì¥\n",
    "        output_file_name = file_name.replace(\".json\", \"_ranking.json\")  # íŒŒì¼ëª… ë³€ê²½\n",
    "        output_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(keywords, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"âœ… {output_file_name} ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸš¨ {file_name}: KRWordRank ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"âœ… ëª¨ë“  êµ¬ì—­ì˜ í‚¤ì›Œë“œ ë­í‚¹ ì¶”ì¶œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê° êµ¬ì—­ì˜ í‚¤ì›Œë“œ ë­í‚¹ì—ì„œ ìƒìœ„ 50ê°œë§Œ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Processing: Entertainment_50_75_ranking.json (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: 18)\n",
      "âœ… Entertainment_50_75_top50.json ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ 2)\n",
      "ğŸ” Processing: Entertainment_25_50_ranking.json (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: 270)\n",
      "âœ… Entertainment_25_50_top50.json ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ 3)\n",
      "ğŸ” Processing: News_&_Politics_75_100_ranking.json (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: 1513)\n",
      "âœ… News_&_Politics_75_100_top50.json ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ 1)\n",
      "ğŸ” Processing: News_&_Politics_1_25_ranking.json (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: 1597)\n",
      "âœ… News_&_Politics_1_25_top50.json ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ 4)\n",
      "ğŸ” Processing: Science_&_Technology_1_25_ranking.json (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: 937)\n",
      "âœ… Science_&_Technology_1_25_top50.json ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ 4)\n",
      "ğŸ” Processing: Gaming_1_25_ranking.json (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: 447)\n",
      "âœ… Gaming_1_25_top50.json ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ 4)\n",
      "ğŸ” Processing: Science_&_Technology_25_50_ranking.json (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: 586)\n",
      "âœ… Science_&_Technology_25_50_top50.json ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ 3)\n",
      "ğŸ” Processing: Music_75_100_ranking.json (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: 229)\n",
      "âœ… Music_75_100_top50.json ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ 1)\n",
      "ğŸ” Processing: News_&_Politics_50_75_ranking.json (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: 1458)\n",
      "âœ… News_&_Politics_50_75_top50.json ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ 2)\n",
      "ğŸ” Processing: Film_&_Animation_50_75_ranking.json (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: 30)\n",
      "âœ… Film_&_Animation_50_75_top50.json ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ 2)\n",
      "ğŸ” Processing: News_&_Politics_25_50_ranking.json (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: 1457)\n",
      "âœ… News_&_Politics_25_50_top50.json ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ 3)\n",
      "ğŸ” Processing: Entertainment_75_100_ranking.json (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: 420)\n",
      "âœ… Entertainment_75_100_top50.json ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ 1)\n",
      "ğŸ” Processing: Music_50_75_ranking.json (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: 35)\n",
      "âœ… Music_50_75_top50.json ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ 2)\n",
      "ğŸ” Processing: Music_1_25_ranking.json (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: 62)\n",
      "âœ… Music_1_25_top50.json ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ 4)\n",
      "ğŸ” Processing: Film_&_Animation_75_100_ranking.json (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: 221)\n",
      "âœ… Film_&_Animation_75_100_top50.json ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ 1)\n",
      "ğŸ” Processing: Music_25_50_ranking.json (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: 196)\n",
      "âœ… Music_25_50_top50.json ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ 3)\n",
      "âœ… ëª¨ë“  êµ¬ì—­ì˜ ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ ì¶”ì¶œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# âœ… ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "input_dir = \"data/ranked_keywords\"  # ì›ë³¸ í‚¤ì›Œë“œ ë­í‚¹ JSON íŒŒì¼ ë””ë ‰í† ë¦¬\n",
    "output_dir = \"data/top50_keywords\"  # ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "\n",
    "# âœ… ì €ì¥í•  ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# âœ… êµ¬ì—­ë³„ ê°€ì¤‘ì¹˜ ì„¤ì •\n",
    "weight_mapping = {\n",
    "    \"1_25\": 4,\n",
    "    \"25_50\": 3,\n",
    "    \"50_75\": 2,\n",
    "    \"75_100\": 1\n",
    "}\n",
    "\n",
    "# âœ… ë””ë ‰í† ë¦¬ ë‚´ JSON íŒŒì¼ ê°€ì ¸ì˜¤ê¸°\n",
    "json_files = [f for f in os.listdir(input_dir) if f.endswith(\"_ranking.json\")]\n",
    "\n",
    "# âœ… ê° íŒŒì¼ë³„ ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ê°€ì¤‘ì¹˜ ì¶”ê°€\n",
    "for file_name in json_files:\n",
    "    input_path = os.path.join(input_dir, file_name)\n",
    "\n",
    "    # âœ… JSON íŒŒì¼ ë¡œë“œ\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        keyword_ranking = json.load(f)  # í‚¤ì›Œë“œ ë­í‚¹ ë°ì´í„° ë¡œë“œ (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)\n",
    "\n",
    "    # âœ… ë¹ˆ ë°ì´í„° ë°©ì§€\n",
    "    if not keyword_ranking:\n",
    "        print(f\"âš ï¸ {file_name}: ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ìƒìœ„ 50ê°œ ì¶”ì¶œì„ ê±´ë„ˆëœ€.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"ğŸ” Processing: {file_name} (ì´ í‚¤ì›Œë“œ ê°œìˆ˜: {len(keyword_ranking)})\")\n",
    "\n",
    "    # âœ… ìƒìœ„ 50ê°œ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ (ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ í›„ í‚¤ì›Œë“œë§Œ ê°€ì ¸ì˜´)\n",
    "    top_50_keywords = list(sorted(keyword_ranking, key=keyword_ranking.get, reverse=True)[:50])\n",
    "\n",
    "    # âœ… ê°€ì¤‘ì¹˜ ì ìš©\n",
    "    # íŒŒì¼ëª…ì—ì„œ êµ¬ì—­ëª… ì¶”ì¶œ (ì˜ˆ: \"News_&_Politics_1_25_ranking.json\" â†’ \"1_25\")\n",
    "    for zone in weight_mapping.keys():\n",
    "        if zone in file_name:\n",
    "            weight = weight_mapping[zone]\n",
    "            break\n",
    "    else:\n",
    "        print(f\"âš ï¸ {file_name}: ì•Œ ìˆ˜ ì—†ëŠ” êµ¬ì—­, ê¸°ë³¸ ê°€ì¤‘ì¹˜ 1 ì ìš©\")\n",
    "        weight = 1\n",
    "\n",
    "    # âœ… {í‚¤ì›Œë“œ: ê°€ì¤‘ì¹˜} í˜•íƒœë¡œ ë³€í™˜\n",
    "    weighted_keywords = {keyword: weight for keyword in top_50_keywords}\n",
    "\n",
    "    # âœ… ê°œë³„ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "    output_file_name = file_name.replace(\"_ranking.json\", \"_top50.json\")  # íŒŒì¼ëª… ë³€ê²½\n",
    "    output_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(weighted_keywords, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… {output_file_name} ì €ì¥ ì™„ë£Œ! (ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ + ê°€ì¤‘ì¹˜ {weight})\")\n",
    "\n",
    "print(\"âœ… ëª¨ë“  êµ¬ì—­ì˜ ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ ì¶”ì¶œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Processing: Entertainment_50_75_top50.json (í‚¤ì›Œë“œ ê°œìˆ˜: 18)\n",
      "ğŸ” Processing: Music_50_75_top50.json (í‚¤ì›Œë“œ ê°œìˆ˜: 35)\n",
      "ğŸ” Processing: Entertainment_25_50_top50.json (í‚¤ì›Œë“œ ê°œìˆ˜: 50)\n",
      "ğŸ” Processing: Music_25_50_top50.json (í‚¤ì›Œë“œ ê°œìˆ˜: 50)\n",
      "ğŸ” Processing: Science_&_Technology_25_50_top50.json (í‚¤ì›Œë“œ ê°œìˆ˜: 50)\n",
      "ğŸ” Processing: News_&_Politics_75_100_top50.json (í‚¤ì›Œë“œ ê°œìˆ˜: 50)\n",
      "ğŸ” Processing: Gaming_1_25_top50.json (í‚¤ì›Œë“œ ê°œìˆ˜: 50)\n",
      "ğŸ” Processing: Film_&_Animation_50_75_top50.json (í‚¤ì›Œë“œ ê°œìˆ˜: 30)\n",
      "ğŸ” Processing: Music_1_25_top50.json (í‚¤ì›Œë“œ ê°œìˆ˜: 50)\n",
      "ğŸ” Processing: News_&_Politics_25_50_top50.json (í‚¤ì›Œë“œ ê°œìˆ˜: 50)\n",
      "ğŸ” Processing: News_&_Politics_50_75_top50.json (í‚¤ì›Œë“œ ê°œìˆ˜: 50)\n",
      "ğŸ” Processing: Music_75_100_top50.json (í‚¤ì›Œë“œ ê°œìˆ˜: 50)\n",
      "ğŸ” Processing: Science_&_Technology_1_25_top50.json (í‚¤ì›Œë“œ ê°œìˆ˜: 50)\n",
      "ğŸ” Processing: News_&_Politics_1_25_top50.json (í‚¤ì›Œë“œ ê°œìˆ˜: 50)\n",
      "ğŸ” Processing: Film_&_Animation_75_100_top50.json (í‚¤ì›Œë“œ ê°œìˆ˜: 50)\n",
      "ğŸ” Processing: Entertainment_75_100_top50.json (í‚¤ì›Œë“œ ê°œìˆ˜: 50)\n",
      "âœ… ìµœì¢… í‚¤ì›Œë“œ ë­í‚¹ì´ 'final_keywords_ranking.json' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# âœ… ë°ì´í„° ë¡œë“œ\n",
    "ranking_file = \"data/all_keywords_ranking.json\"  # ì „ì²´ í‚¤ì›Œë“œ ë­í‚¹ íŒŒì¼\n",
    "weighted_dir = \"data/top50_keywords\"        # êµ¬ì—­ë³„ ê°€ì¤‘ì¹˜ íŒŒì¼ë“¤ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬\n",
    "output_file = \"final_keywords_ranking.json\"  # ìµœì¢… ë­í‚¹ ê²°ê³¼ ì €ì¥ íŒŒì¼\n",
    "\n",
    "# âœ… ì „ì²´ í‚¤ì›Œë“œ ë­í‚¹ ë¡œë“œ (ê¸°ë³¸ ì ìˆ˜ í¬í•¨)\n",
    "with open(ranking_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    base_ranking = dict(json.load(f))  # ë¦¬ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜\n",
    "\n",
    "# âœ… ë””ë ‰í† ë¦¬ ë‚´ ê°€ì¤‘ì¹˜ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°\n",
    "weighted_files = [f for f in os.listdir(weighted_dir) if f.endswith(\"_top50.json\")]\n",
    "\n",
    "# âœ… í‚¤ì›Œë“œë³„ ìµœì¢… ì ìˆ˜ ê³„ì‚°\n",
    "final_ranking = base_ranking.copy()  # ê¸°ì¡´ ì ìˆ˜ë¥¼ ìœ ì§€í•˜ë©° ì—…ë°ì´íŠ¸í•  ë”•ì…”ë„ˆë¦¬\n",
    "\n",
    "for file_name in weighted_files:\n",
    "    file_path = os.path.join(weighted_dir, file_name)\n",
    "\n",
    "    # âœ… êµ¬ì—­ë³„ ê°€ì¤‘ì¹˜ JSON íŒŒì¼ ë¡œë“œ\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        weighted_keywords = json.load(f)  # {í‚¤ì›Œë“œ: ê°€ì¤‘ì¹˜} í˜•íƒœì˜ ë°ì´í„°\n",
    "\n",
    "    print(f\"ğŸ” Processing: {file_name} (í‚¤ì›Œë“œ ê°œìˆ˜: {len(weighted_keywords)})\")\n",
    "\n",
    "    # âœ… ê° í‚¤ì›Œë“œì— ê°€ì¤‘ì¹˜ë¥¼ ì¶”ê°€\n",
    "    for keyword, weight in weighted_keywords.items():\n",
    "        if keyword in final_ranking:\n",
    "            final_ranking[keyword] += weight  # ê¸°ì¡´ ì ìˆ˜ + ê°€ì¤‘ì¹˜\n",
    "        else:\n",
    "            final_ranking[keyword] = weight  # ìƒˆë¡œìš´ í‚¤ì›Œë“œë©´ ê°€ì¤‘ì¹˜ë§Œ ì ìš©\n",
    "\n",
    "# âœ… ìµœì¢… í‚¤ì›Œë“œ ë­í‚¹ ì •ë ¬\n",
    "final_sorted_ranking = sorted(final_ranking.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# âœ… JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_sorted_ranking, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… ìµœì¢… í‚¤ì›Œë“œ ë­í‚¹ì´ '{output_file}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
