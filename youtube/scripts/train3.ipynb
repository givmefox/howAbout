{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "# 문선곤 - 수정 안함\n",
    "\n",
    "# JSON 데이터 저장 함수\n",
    "def save_to_json(data, filename):\n",
    "    # data 폴더가 존재하지 않으면 생성\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    \n",
    "    # 파일 경로를 data 폴더 아래로 설정\n",
    "    filepath = os.path.join('data', filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    print(f\"데이터 저장: {filepath}\")\n",
    "    \n",
    "    # JSON 데이터 로드 함수\n",
    "# JSON 파일을 읽어서 딕셔너리로 반환하는 함수\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "# value가 문자열배열의 배열일 때 한글자와 빈 문자열을 제외하고 문자열로 만드는 함수\n",
    "def merge_values(data):\n",
    "    \"\"\"\n",
    "    key: value에서 value가 문자열 배열의 배열일 때,\n",
    "    하나의 문자열로 변환하고 빈 문자열(\"\") 및 한 글자 단어는 제외하는 함수.\n",
    "\n",
    "    :param data: dict, key: list of lists (e.g., { \"key1\": [[\"문장1\", \"문장2\"], [\"문장3\"]] })\n",
    "    :return: dict, key: merged string (e.g., { \"key1\": \"문장1 문장2 문장3\" })\n",
    "    \"\"\"\n",
    "    merged_data = {}\n",
    "    for key, values in data.items():\n",
    "        if isinstance(values, list):  # 값이 리스트인지 확인\n",
    "            merged_sentence = \" \".join(\n",
    "                s for sublist in values for s in sublist if s.strip() and len(s.strip()) > 1\n",
    "            )  # 빈 문자열 & 한 글자 제외 후 공백으로 연결\n",
    "            merged_data[key] = merged_sentence  # key: 병합된 문자열 형태로 저장\n",
    "\n",
    "    return merged_data\n",
    "# title과 tags를 하나의 문장으로 합치는 함수\n",
    "def merge_title_tags(data):\n",
    "    \"\"\"\n",
    "    딕셔너리에서 'title'과 'tags'를 하나의 문장으로 변환하는 함수.\n",
    "    \n",
    "    :param data: dict (유튜브 비디오 정보)\n",
    "    :return: dict (title과 tags가 합쳐진 문장)\n",
    "    \"\"\"\n",
    "    merged_data = []\n",
    "    for key, videos in data.items():\n",
    "        for video in videos:\n",
    "            title = video.get(\"title\", \"\").strip()  # title이 없으면 빈 문자열\n",
    "            tags = video.get(\"tags\", [])  # tags가 없거나 None이면 빈 리스트로 처리\n",
    "            merged = \" \".join([title] + tags)  # title과 tags 결합\n",
    "            merged_data.append(merged)\n",
    "    \n",
    "    return merged_data\n",
    "# 텍스트 데이터를 전처리하는 함수\n",
    "def clean_text(text):   \n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\S+|#\\S+', '', text)\n",
    "    text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"[a-zA-Z]\", \"\", text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching trending videos for category: News & Politics\n",
      "Error fetching videos: Unable to find the server at youtube.googleapis.com\n",
      "Error fetching videos: Unable to find the server at youtube.googleapis.com\n",
      "Error fetching videos: Unable to find the server at youtube.googleapis.com\n",
      "비디오 44 개 카테고리: News & Politics fetch 완료.\n",
      "Fetching trending videos for category: Film & Animation\n",
      "비디오 7 개 카테고리: Film & Animation fetch 완료.\n",
      "Fetching trending videos for category: Music\n",
      "비디오 30 개 카테고리: Music fetch 완료.\n",
      "Fetching trending videos for category: Pets & Animals\n",
      "비디오 0 개 카테고리: Pets & Animals fetch 완료.\n",
      "Fetching trending videos for category: Sports\n",
      "비디오 13 개 카테고리: Sports fetch 완료.\n",
      "Fetching trending videos for category: Gaming\n",
      "비디오 77 개 카테고리: Gaming fetch 완료.\n",
      "Fetching trending videos for category: Entertainment\n",
      "비디오 7 개 카테고리: Entertainment fetch 완료.\n",
      "Fetching trending videos for category: Science & Technology\n",
      "비디오 60 개 카테고리: Science & Technology fetch 완료.\n",
      "데이터 저장: data/raw_video_data.json\n",
      "데이터 저장 : data/raw_video_data.json'\n",
      "\n",
      "Category: News & Politics\n",
      " - 김어준의 겸손은힘들다 뉴스공장 2025년 2월 19일 수요일 [윤건영, 김기표, 박은정, 겸손NSC, 김시연, 스포츠공장] (IC3iNhz02l0), 조회수: 1278696 회, 좋아요: 97711 개\n",
      " - 폭탄 봉지욱 “저는 계속 터트릴 겁니다. 버티기 힘들 거야~”｜풀버전 (cvmdNi_oBYA), 조회수: 1418688 회, 좋아요: 73932 개\n",
      " - 또 '진술 조서' 문제 삼은 尹측‥거절하자 짐 싸서 퇴장 (2025.02.18/뉴스데스크/MBC) (PaFf3jmHVS8), 조회수: 1247302 회, 좋아요: 17802 개\n",
      " - 검찰 보고서 추가 공개...'김건희 공천개입' 흔적 수두룩 〈주간 뉴스타파〉 (5V8bHfodNU8), 조회수: 1136689 회, 좋아요: 59170 개\n",
      " - 野 회유 의혹에 김현태 입 열었다 / 채널A / 뉴스TOP 10 (b3aRCbm-NhE), 조회수: 766558 회, 좋아요: 21858 개\n",
      "\n",
      "Category: Film & Animation\n",
      " - 수지의 새로운 경호원 (SmygsXbknXo), 조회수: 2222297 회, 좋아요: 78837 개\n",
      " - 배우 김새론, 마지막 SNS 게시물은?…빈소 찾은 원빈 '침통' / 연합뉴스 (Yonhapnews) (6k74FOZ7HuU), 조회수: 1989801 회, 좋아요: 9621 개\n",
      " - [🌈습짤의 기원] 안경 벗겨지고 교수님한테 끌려가는 현실 레지던트 2년 차😂 | 청춘의국 | SBS (_Auauw4MoIg), 조회수: 2220356 회, 좋아요: 38879 개\n",
      " - 달의 기원 - 파트1(쇼츠버전) (40cZ0GzbsiY), 조회수: 293341 회, 좋아요: 15387 개\n",
      " - 개욱곀ㅋㅋㅋㅋ #레전드사연 #컬투쇼 #웃긴동영상 (FezHGedgr18), 조회수: 634639 회, 좋아요: 15098 개\n",
      "\n",
      "Category: Music\n",
      " - KiiiKiii 키키 'I DO ME' MV (hAEfi_SKTEU), 조회수: 2926190 회, 좋아요: 178912 개\n",
      " - JISOO - earthquake (Official Music Video) (2V6lvCUPT8I), 조회수: 23371226 회, 좋아요: 2197051 개\n",
      " - ATTITUDE (ATTITUDE) (7sJES_4FiAs), 조회수: 3715827 회, 좋아요: 15098 개\n",
      " - REBEL HEART (REBEL HEART) (N1so5Q60Chw), 조회수: 3075804 회, 좋아요: 11559 개\n",
      " - I Miss You So Much (미치게 그리워서) (q9UUIH6xZ9U), 조회수: 1140494 회, 좋아요: 9036 개\n",
      "\n",
      "Category: Pets & Animals\n",
      "\n",
      "Category: Sports\n",
      " - [24/25 PL] 25R 토트넘 vs 맨유 H/L｜SPOTV FOOTBALL (Sz1YQ4_JZZw), 조회수: 1212172 회, 좋아요: 10076 개\n",
      " - 체대생 VS 야구선수 (DpzAVGlquE4), 조회수: 262168 회, 좋아요: 8235 개\n",
      " - 기적을 노래하라 당구스타 K (1pIsYX286is), 조회수: 1188401 회, 좋아요: 10459 개\n",
      " - [중국해설 실제 번역] 한국이 티 안내고 일부러 못 하는척 연기하다가 결승전에서 갑자기 진짜 실력 드러내며 금은동 싹쓸이하니 난리난 CCTV 중계석 반응 번역 (d0UYzYUe20c), 조회수: 1492140 회, 좋아요: 10430 개\n",
      " - ‘피겨 프린스’ 차준환 금메달! 일본 가기야마 유마에 대역전 드라마! [하얼빈 동계 아시안게임] (dEn5iPslOag), 조회수: 428696 회, 좋아요: 9026 개\n",
      "\n",
      "Category: Gaming\n",
      " - 화성 처음 가는 남자 (WA4wCIEREeM), 조회수: 349372 회, 좋아요: 3365 개\n",
      " - 포오네 이스터에그 (71KzEzcRA7o), 조회수: 262418 회, 좋아요: 2192 개\n",
      " - 생존 가능성 0.001%.. 아무것도 없는 우주에서 살아남기 (IgXcQ3fyUug), 조회수: 265130 회, 좋아요: 11000 개\n",
      " - 듀오 vs 듀오 싸움에 낑겨버린 아재 (T_uOWdI8xgo), 조회수: 174389 회, 좋아요: 0 개\n",
      " - 제자들의 진지한 개념싸움에 당황한 클리드;; (hiig0HuQib4), 조회수: 196619 회, 좋아요: 1877 개\n",
      "\n",
      "Category: Entertainment\n",
      " - [굿데이] 그리웠다 이 케미... 11년 지나도 여전한 형돈♥지용, 그리고 스타(?)가 되어 돌아온 프콘이ㅣ#GD #정형돈 #데프콘 MBC250216방송 (oFdtUKHKOZc), 조회수: 2103511 회, 좋아요: 24815 개\n",
      " - 노상원을 파도 김건희! 건진법사를 파도 김건희!｜풀버전 (TaOwNNLFb1g), 조회수: 1551079 회, 좋아요: 78557 개\n",
      " - 수지의 새로운 경호원 (SmygsXbknXo), 조회수: 2223031 회, 좋아요: 78837 개\n",
      " - 이중 언어의 문제?🔥#금쪽같은내새끼 #shorts (5pKJY1yjqvQ), 조회수: 1005976 회, 좋아요: 15645 개\n",
      " - 배우 김새론, 마지막 SNS 게시물은?…빈소 찾은 원빈 '침통' / 연합뉴스 (Yonhapnews) (6k74FOZ7HuU), 조회수: 1989801 회, 좋아요: 9621 개\n",
      "\n",
      "Category: Science & Technology\n",
      " - 아이폰 절반급 두께에 주름도 없다?;; 생태계 교란할 중국산 폴더블폰 곧 나온다. (jAV4NnFlrV0), 조회수: 594427 회, 좋아요: 5927 개\n",
      " - 아이폰 SE4, 모든 게 빗나갔다! 실물 디자인 선공개 및 최신 유출 정보 총정리 (목업, 색상 크기 두께, 다이나믹 아일랜드, 카메라, 성능, 배터리, 가격 출시일) (DKxZNTsRxqo), 조회수: 54018 회, 좋아요: 505 개\n",
      " - 지금 이 버튼 당장 끄세요!! 비밀번호/계정/이메일 전부 털어갑니다! (WVEJwFlQnPU), 조회수: 454118 회, 좋아요: 9329 개\n",
      " - OpenAI, 구글, 딥씨크 모두 압살...Grok3 | GPU 20만개의 결과물 Grok3가 왜 놀라운 것인가 (hoQwz_kQ0Xc), 조회수: 80005 회, 좋아요: 2296 개\n",
      " - 와 엄청 무턱이다 (jR4UpUPzWhk), 조회수: 515182 회, 좋아요: 5272 개\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import isodate\n",
    "import os\n",
    "\n",
    "\n",
    "# 환경 변수에서 API 키 가져오기\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API 키가 설정되지 않았습니다. 환경 변수 'YOUTUBE_API_KEY'를 설정하세요.\")\n",
    "\n",
    "# YouTube API 설정\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# 카테고리 ID 설정\n",
    "CATEGORIES = {\n",
    "    \"News & Politics\": \"25\",\n",
    "    'Film & Animation' : \"1\",\n",
    "    'Music' : \"10\",\n",
    "    'Pets & Animals' : \"15\",\n",
    "    'Sports' : \"17\",\n",
    "    'Gaming' : \"20\",\n",
    "    'Entertainment' : \"24\",\n",
    "    'Science & Technology': \"28\"\n",
    "}\n",
    "\n",
    "# 동영상 데이터 가져오기\n",
    "def fetch_trending_videos(category_id, region_code=\"KR\", max_results=200):\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(videos) < max_results:\n",
    "        try:\n",
    "            request = youtube.videos().list(\n",
    "                part=\"snippet,statistics,contentDetails\",\n",
    "                chart=\"mostPopular\",\n",
    "                regionCode=region_code,\n",
    "                videoCategoryId=category_id,\n",
    "                maxResults=min(50, max_results - len(videos)),\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                duration = isodate.parse_duration(item[\"contentDetails\"][\"duration\"])\n",
    "                duration_in_seconds = duration.total_seconds()  #초로 바꾸기기\n",
    "\n",
    "                if duration_in_seconds > 80:  # 80초 이상의 동영상만 가져오기\n",
    "                    videos.append({\n",
    "                        \"video_id\": item[\"id\"],\n",
    "                        \"title\": item[\"snippet\"][\"title\"],\n",
    "                        \"description\": item[\"snippet\"][\"description\"],\n",
    "                        \"tags\": item[\"snippet\"].get(\"tags\", []),\n",
    "                        \"duration\": str(duration),\n",
    "                        \"view_count\": int(item[\"statistics\"].get(\"viewCount\", 0)),\n",
    "                        \"like_count\": int(item[\"statistics\"].get(\"likeCount\", 0)),\n",
    "                        \"comment_count\": int(item[\"statistics\"].get(\"commentCount\", 0)),\n",
    "                        \"category_id\": category_id,\n",
    "                    })\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching videos: {e}\")\n",
    "            time.sleep(5)  # 잠시 대기 후 다시 시도\n",
    "\n",
    "    return videos\n",
    "\n",
    "\n",
    "\n",
    "# 실행\n",
    "all_videos = {}\n",
    "\n",
    "for category_name, category_id in CATEGORIES.items():\n",
    "    print(f\"Fetching trending videos for category: {category_name}\")\n",
    "    videos = fetch_trending_videos(category_id, region_code=\"KR\", max_results=200)\n",
    "    all_videos[category_name] = videos\n",
    "    print(f\"비디오 {len(videos)} 개 카테고리: {category_name} fetch 완료.\")\n",
    "\n",
    "# 결과를 하나의 JSON 파일로 저장\n",
    "output_file = \"raw_video_data.json\"\n",
    "save_to_json(all_videos, output_file)\n",
    "print(f\"데이터 저장 : data/{output_file}'\")\n",
    "\n",
    "# 결과 출력 예시\n",
    "for category, videos in all_videos.items():\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    for video in videos[:5]:\n",
    "        print(f\" - {video['title']} ({video['video_id']}), 조회수: {video['view_count']} 회, 좋아요: {video['like_count']} 개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching comments for video CfK_9iDlCSo: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=CfK_9iDlCSo&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video 7sJES_4FiAs: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=7sJES_4FiAs&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video N1so5Q60Chw: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=N1so5Q60Chw&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video q9UUIH6xZ9U: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=q9UUIH6xZ9U&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video GBm2e08CKSo: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=GBm2e08CKSo&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video Y8rHo43-LyQ: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=Y8rHo43-LyQ&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video ieWjkkZADkQ: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=ieWjkkZADkQ&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video oda74gp5RsY: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=oda74gp5RsY&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video EnsuJgsxM-8: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=EnsuJgsxM-8&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video OveWq3SnznE: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=OveWq3SnznE&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video oEBbSqUHfuI: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=oEBbSqUHfuI&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video DG_GrIsD-tg: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=DG_GrIsD-tg&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video UZ34FXPquTI: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=UZ34FXPquTI&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video 0ujged1o7zg: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=0ujged1o7zg&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video n2Lh4tvWsAg: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=n2Lh4tvWsAg&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video XEQc-2HoE1c: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=XEQc-2HoE1c&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video 529VgRXlmuI: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=529VgRXlmuI&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video xzKsXyRMrKs: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=xzKsXyRMrKs&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video xE886FlLWNA: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=xE886FlLWNA&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video rwwsLML-mSM: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=rwwsLML-mSM&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video O4-ZfIN1IM0: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=O4-ZfIN1IM0&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video 5pKJY1yjqvQ: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=5pKJY1yjqvQ&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video uYbXv0yW0wo: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=uYbXv0yW0wo&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video S1CbhpKC6SE: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=S1CbhpKC6SE&maxResults=50&key=AIzaSyC9o9YrcN8qTx33vnFA8kD8prv-mMJ0HYg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "데이터 저장: data/video_comments.json\n",
      "비디오 댓글이 'video_comments.json'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "\n",
    "# 문선곤 - 카테고리별로 정리 되도록 수정\n",
    "\n",
    "\n",
    "# 비디오 댓글 가져오기 함수\n",
    "def get_video_comments(youtube, video_id, max_results=100):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < max_results:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=min(50, max_results - len(comments)),\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                comments.append(comment)\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching comments for video {video_id}: {e}\")\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "# JSON 데이터 저장 함수\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 파일 경로 설정\n",
    "input_file_path = 'data/raw_video_data.json'\n",
    "output_file_path = 'video_comments.json'\n",
    "\n",
    "# JSON 데이터 로드\n",
    "data = load_json(input_file_path)\n",
    "\n",
    "\n",
    "# 비디오 댓글 가져오기 및 저장\n",
    "all_comments = {}\n",
    "for category, videos in data.items():\n",
    "    all_comments[category] = []  # 카테고리별 리스트 생성\n",
    "\n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        comments = get_video_comments(youtube, video_id, max_results=1000)\n",
    "        all_comments[category].append({\"video_id\": video_id, \"comments\": comments})\n",
    "\n",
    "# 결과 저장\n",
    "save_to_json(all_comments, output_file_path)\n",
    "print(f\"비디오 댓글이 '{output_file_path}'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리된 데이터가 'cleaned.txt'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "# import emoji # type: ignore\n",
    "# import json\n",
    "\n",
    "\n",
    "# # 기본 텍스트 정제 함수\n",
    "# def preprocess_text(text):\n",
    "#     \"\"\"\n",
    "#     텍스트 데이터를 전처리하고 문장을 반환합니다.\n",
    "\n",
    "#     Args:\n",
    "#         text (str): 입력 텍스트 데이터.\n",
    "\n",
    "#     Returns:\n",
    "#         str: 전처리된 텍스트.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(text, str):\n",
    "#         text = str(text)  # 문자열로 변환\n",
    "\n",
    "#     # 1. HTML 태그 제거\n",
    "#     text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "#     # 2. URL 제거\n",
    "#     text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "#     # 3. 이메일 제거\n",
    "#     text = re.sub(r\"\\S+@\\S+\\.\\S+\", \"\", text)\n",
    "\n",
    "#     # 4. 숫자 제거 (명확히 숫자만 제거)\n",
    "#     text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "#     # 5. 반복된 ㅋ, ㅎ, ㅠ, ㅜ 등 제거\n",
    "#     text = re.sub(r\"[ㅋㅎㅠㅜ]+\", \"\", text)\n",
    "\n",
    "#     # 6. 반복된 점(...) 제거\n",
    "#     text = re.sub(r\"\\.\\.+\", \".\", text)\n",
    "\n",
    "#     # 7. 반복된 문자 축소 (e.g., \"와아아\" -> \"와\")\n",
    "#     text = re.sub(r\"(.)\\1{2,}\", r\"\\1\", text)\n",
    "\n",
    "#     # 8. 이모지 제거\n",
    "#     text = emoji.replace_emoji(text, replace=\"\")\n",
    "\n",
    "#     # 9. 특수문자 및 영어 알파벳 제거\n",
    "#     text = re.sub(r\"[^\\w\\s가-힣]\", \"\", text)  # 영어 알파벳 포함 특수문자 제거\n",
    "#     text = re.sub(r\"[a-zA-Z]\", \"\", text)     # 영어 알파벳 제거\n",
    "\n",
    "#     # 10. 양쪽 공백 제거\n",
    "#     text = text.strip()\n",
    "\n",
    "#     return text\n",
    "\n",
    "\n",
    "# # 데이터 리스트 처리 함수\n",
    "# def process_text_file(input_file, output_file):\n",
    "#     \"\"\"\n",
    "#     문자열과 문자열 리스트가 섞인 데이터를 처리하여 결과를 저장합니다.\n",
    "\n",
    "#     Args:\n",
    "#         input_file (str): 입력 파일 경로.\n",
    "#         output_file (str): 출력 파일 경로.\n",
    "#     \"\"\"\n",
    "#     results = []\n",
    "\n",
    "#     # 파일 로드\n",
    "#     with open(input_file, 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "\n",
    "#     # 데이터 처리\n",
    "#     for item in data:\n",
    "#         if isinstance(item, list):  # 문자열 리스트인 경우\n",
    "#             merged_text = \" \".join(item)\n",
    "#             processed_text = preprocess_text(merged_text)\n",
    "#         elif isinstance(item, str):  # 문자열인 경우\n",
    "#             processed_text = preprocess_text(item)\n",
    "#         else:\n",
    "#             continue  # 알 수 없는 형식은 건너뜀\n",
    "\n",
    "#         results.append(processed_text)\n",
    "\n",
    "#     # 결과 저장\n",
    "#     with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# # 실행\n",
    "#     # 입력 및 출력 파일 경로\n",
    "# input_file = \"merged_texts.txt\"  # 문자열과 문자열 리스트가 섞인 txt 파일\n",
    "# output_file = \"cleaned.txt\"\n",
    "\n",
    "# # 텍스트 파일 처리\n",
    "# process_text_file(input_file, output_file)\n",
    "# print(f\"처리된 데이터가 '{output_file}'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 문장 수: 238\n",
      "데이터 저장: data/cleaned_video_comments.json\n"
     ]
    }
   ],
   "source": [
    "data = load_json('data/video_comments.json')\n",
    "\n",
    "processed_data = {}\n",
    "\n",
    "for category, videos in data.items():\n",
    "    processed_data[category] = []  # 카테고리별 리스트 생성\n",
    "    \n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        comments = video.get(\"comments\", [])\n",
    "\n",
    "        # 댓글 텍스트 정리 적용\n",
    "        cleaned_comments = [clean_text(comment) for comment in comments]\n",
    "        \n",
    "        # 저장 형식 유지\n",
    "        processed_data[category].append({\"video_id\": video_id, \"comments\": cleaned_comments})\n",
    "\n",
    "# 총 문장 수 계산\n",
    "total_sentences = sum(len(value) for value in processed_data.values())\n",
    "print(f\"총 문장 수: {total_sentences}\")\n",
    "        \n",
    "save_to_json(processed_data, 'cleaned_video_comments.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토크나이저 생성 Soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.182 Gbory 0.875 Gb\n",
      "all cohesion probabilities was computed. # words = 49846\n",
      "all branching entropies was computed # words = 83419\n",
      "all accessor variety was computed # words = 83419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Scores(cohesion_forward=np.float64(0.6554912257227551), cohesion_backward=np.float64(0.29004652062554404), left_branching_entropy=3.791747353178408, right_branching_entropy=2.8167883720666143, left_accessor_variety=86, right_accessor_variety=55, leftside_frequency=895, rightside_frequency=53)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp.word import WordExtractor\n",
    "\n",
    "# Load the cleaned video comments\n",
    "data = load_json('data/cleaned_video_comments.json')\n",
    "\n",
    "# 모든 댓글을 하나의 텍스트로 합치기 (카테고리 구조 유지)\n",
    "corpus = []\n",
    "for category, videos in data.items():\n",
    "    for video in videos:\n",
    "        comments = video.get(\"comments\", [])\n",
    "        corpus.extend(comments)  # 모든 댓글을 하나의 리스트로 추가\n",
    "\n",
    "# DoublespaceLineCorpus 객체 생성\n",
    "corpus[5]\n",
    "\n",
    "# WordExtractor 객체 생성 및 학습\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(corpus)\n",
    "word_scores = word_extractor.extract()\n",
    "\n",
    "word_scores[\"윤석열\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 215196 from 86878 sents. mem=1.190 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=643051, mem=1.455 Gb\n",
      "[Noun Extractor] batch prediction was completed for 66369 words\n",
      "[Noun Extractor] checked compounds. discovered 40896 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52429 -> 39486\n",
      "[Noun Extractor] postprocessing ignore_features : 39486 -> 39194\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39194 -> 38448\n",
      "[Noun Extractor] 38448 nouns (40896 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.294 Gb                    \n",
      "[Noun Extractor] 65.61 % eojeols are covered\n",
      "데이터 저장: data/tokenized_video_comments.json\n"
     ]
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "\n",
    "cohesion_score = {word:score.cohesion_forward for word, score in word_scores.items()}\n",
    "\n",
    "tokenizer = MaxScoreTokenizer(scores=cohesion_score)\n",
    "\n",
    "\n",
    "noun_extractor = LRNounExtractor_v2()\n",
    "nouns = noun_extractor.train_extract(corpus) # list of str like\n",
    "\n",
    "noun_scores = {noun:score.score for noun, score in nouns.items()}\n",
    "combined_scores = {noun:score + cohesion_score.get(noun, 0)\n",
    "    for noun, score in noun_scores.items()}\n",
    "combined_scores.update(\n",
    "    {subword:cohesion for subword, cohesion in cohesion_score.items()\n",
    "    if not (subword in combined_scores)}\n",
    ")\n",
    "\n",
    "\n",
    "# Load the cleaned video comments\n",
    "data = load_json('data/cleaned_video_comments.json')\n",
    "tokenizer = LTokenizer(scores=combined_scores)\n",
    "\n",
    "# Tokenize each comment\n",
    "tokenized_comments = {}\n",
    "\n",
    "for category, videos in data.items():\n",
    "    tokenized_comments[category] = []  # 카테고리별 리스트 생성\n",
    "\n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        comments = video.get(\"comments\", [])\n",
    "\n",
    "        # 댓글을 토큰화\n",
    "        tokenized = [tokenizer.tokenize(comment) for comment in comments]\n",
    "\n",
    "        # 구조 유지\n",
    "        tokenized_comments[category].append({\"video_id\": video_id, \"tokenized_comments\": tokenized})\n",
    "\n",
    "\n",
    "# Save the tokenized comments back to JSON\n",
    "save_to_json(tokenized_comments, 'tokenized_video_comments.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 215196 from 86878 sents. mem=1.132 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=643051, mem=1.184 Gb\n",
      "[Noun Extractor] batch prediction was completed for 66369 words\n",
      "[Noun Extractor] checked compounds. discovered 40896 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 52429 -> 39486\n",
      "[Noun Extractor] postprocessing ignore_features : 39486 -> 39194\n",
      "[Noun Extractor] postprocessing ignore_NJ : 39194 -> 38448\n",
      "[Noun Extractor] 38448 nouns (40896 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.269 Gb                    \n",
      "[Noun Extractor] 65.61 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "from soynlp.noun import LRNounExtractor_v2\n",
    "noun_extractor = LRNounExtractor_v2()\n",
    "nouns = noun_extractor.train_extract(corpus) # list of str like\n",
    "\n",
    "noun_scores = {noun:score.score for noun, score in nouns.items()}\n",
    "combined_scores = {noun:score + cohesion_score.get(noun, 0)\n",
    "    for noun, score in noun_scores.items()}\n",
    "combined_scores.update(\n",
    "    {subword:cohesion for subword, cohesion in cohesion_score.items()\n",
    "    if not (subword in combined_scores)}\n",
    ")\n",
    "\n",
    "tokenizer = LTokenizer(scores=combined_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 저장: data/okt_tokenized_video_comments.json\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "data = load_json('data/cleaned_video_comments.json')\n",
    "\n",
    "# Tokenize each comment using Okt\n",
    "tokenized_comments_okt = {}\n",
    "for category, videos in data.items():\n",
    "    tokenized_comments_okt[category] = []  # 카테고리별 리스트 생성\n",
    "\n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        comments = video.get(\"comments\", [])\n",
    "\n",
    "        # 댓글을 형태소 분석하여 토큰화\n",
    "        tokenized = [okt.morphs(comment) for comment in comments]\n",
    "\n",
    "        # 구조 유지\n",
    "        tokenized_comments_okt[category].append({\"video_id\": video_id, \"tokenized_comments\": tokenized})\n",
    "\n",
    "# Save the tokenized comments back to JSON\n",
    "save_to_json(tokenized_comments_okt, 'okt_tokenized_video_comments.json')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 저장: data/okt_nouns_video_comments.json\n"
     ]
    }
   ],
   "source": [
    "# Extract nouns from each comment using Okt\n",
    "nouns_comments_okt = {}\n",
    "for category, videos in data.items():\n",
    "    nouns_comments_okt[category] = []  # 카테고리별 리스트 생성\n",
    "\n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        comments = video.get(\"comments\", [])\n",
    "\n",
    "        # 댓글에서 명사만 추출\n",
    "        extracted_nouns = [okt.nouns(comment) for comment in comments]\n",
    "\n",
    "        # 구조 유지\n",
    "        nouns_comments_okt[category].append({\"video_id\": video_id, \"nouns\": extracted_nouns})\n",
    "\n",
    "\n",
    "# Save the nouns extracted comments back to JSON\n",
    "save_to_json(nouns_comments_okt, 'okt_nouns_video_comments.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kiwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('반국가세력', 0.5480024814605713, 76, -1.5345635414123535),\n",
       " ('곽종근', 0.5139729380607605, 119, -2.7739651203155518),\n",
       " ('부정선거', 0.4741281569004059, 575, -2.10939621925354),\n",
       " ('딥페이크', 0.3987986743450165, 91, -2.4564383029937744),\n",
       " ('자유대한민국', 0.3961898982524872, 179, -2.89668607711792),\n",
       " ('다대한민국', 0.3665927052497864, 49, -2.0551440715789795),\n",
       " ('탄핵반대', 0.3618530035018921, 224, -1.878380537033081),\n",
       " ('찬성집회', 0.3450433909893036, 51, -1.5257374048233032),\n",
       " ('니다대한민국', 0.33699408173561096, 26, -1.5568251609802246),\n",
       " ('란수괴', 0.33526986837387085, 170, -2.487471342086792),\n",
       " ('윤대통령', 0.32256072759628296, 325, -0.2791236937046051),\n",
       " ('온앤오프', 0.3092728555202484, 169, -2.5341715812683105),\n",
       " ('양자컴퓨터', 0.3085829019546509, 66, -0.8343412280082703),\n",
       " ('박근혜대통령', 0.2979728579521179, 41, -1.4679926633834839),\n",
       " ('태균', 0.29359349608421326, 531, -2.337681531906128),\n",
       " ('반대집회', 0.2841551899909973, 67, -1.4197840690612793),\n",
       " ('플레이브', 0.280100017786026, 183, -1.9951273202896118),\n",
       " ('박정희대통령', 0.26961392164230347, 11, -0.8941963315010071),\n",
       " ('광주시민', 0.2681012451648712, 164, -1.3160964250564575),\n",
       " ('윤석열대통령', 0.2633002996444702, 107, -0.7175803184509277),\n",
       " ('탄핵찬성', 0.26183268427848816, 79, -1.875105381011963),\n",
       " ('군사반란', 0.2608729898929596, 24, -0.6887761354446411),\n",
       " ('문형배', 0.26036399602890015, 499, -2.542275905609131),\n",
       " ('요대한민국', 0.2583264410495758, 18, -1.8522074222564697),\n",
       " ('유튜버', 0.25691354274749756, 228, -1.4269683361053467),\n",
       " ('합니다대한민국', 0.2529016435146332, 11, -1.9155044555664062),\n",
       " ('명태균게이트', 0.25219473242759705, 10, -1.4267553091049194)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# Kiwi 객체 생성\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# Load the cleaned video comments\n",
    "data = load_json('data/cleaned_video_comments.json')\n",
    "\n",
    "corpus = []\n",
    "for category, videos in data.items():\n",
    "    for video in videos:\n",
    "        comments = video.get(\"comments\", [])\n",
    "        corpus.extend(comments)  # 모든 댓글을 하나의 리스트로 추가\n",
    "\n",
    "kiwi.extract_add_words(corpus)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 저장: data/kiwi_nouns_video_comments.json\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy.utils import Stopwords\n",
    "stopwords = Stopwords()\n",
    "\n",
    "# ✅ Kiwi를 이용한 토큰화 & 명사 추출\n",
    "tokenized_nouns = {}\n",
    "stopwords = Stopwords()\n",
    "\n",
    "data = load_json('data/cleaned_video_comments.json')\n",
    "\n",
    "for category, videos in data.items():\n",
    "    tokenized_nouns[category] = []  # 카테고리별 리스트 생성\n",
    "\n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        comments = video.get(\"comments\", [])\n",
    "\n",
    "        # 댓글을 형태소 분석 후 명사만 추출\n",
    "        tokenized = []\n",
    "        for comment in comments:\n",
    "            tokens = kiwi.tokenize(comment)  # 형태소 분석\n",
    "            nouns = [\n",
    "                token.form for token in tokens\n",
    "                if token.tag in [\"NNG\", \"NNP\"]  # 명사(NNG, NNP)만 추출\n",
    "                and len(token.form) > 1  # 1글자 제외\n",
    "                and (token.form, token.tag) not in stopwords  # ✅ 수정된 필터링\n",
    "            ]\n",
    "            tokenized.append(nouns)\n",
    "\n",
    "        # 구조 유지\n",
    "        tokenized_nouns[category].append({\"video_id\": video_id, \"nouns\": tokenized})\n",
    "\n",
    "# ✅ 결과 JSON 저장\n",
    "save_to_json(tokenized_nouns, 'kiwi_nouns_video_comments.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KRWordRank(kiwi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 저장: data/kiwi_data.json\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned video comments\n",
    "data = load_json('data/kiwi_nouns_video_comments.json')\n",
    "\n",
    "# ✅ 데이터를 `merge_values` 함수에 맞게 변형\n",
    "# (카테고리 → 비디오 → 명사 리스트 구조를 `merge_values`에서 처리할 수 있도록 변형)\n",
    "transformed_data = {}\n",
    "\n",
    "for category, videos in data.items():\n",
    "    transformed_data[category] = []  # 카테고리별 리스트 생성\n",
    "\n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        nouns = video.get(\"nouns\", [])  # 명사 리스트 가져오기\n",
    "\n",
    "        # 명사 리스트를 하나의 리스트로 평탄화 (nested list 제거)\n",
    "        flat_nouns = [word for noun_list in nouns for word in noun_list]\n",
    "\n",
    "        # 병합할 데이터로 변환\n",
    "        transformed_data[category].append(flat_nouns)\n",
    "# ✅ `merge_values` 적용\n",
    "processed_data = merge_values(transformed_data)\n",
    "\n",
    "save_to_json(processed_data,'kiwi_data.json')\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.word import KRWordRank\n",
    "\n",
    "min_count = 10   # 단어의 최소 출현 빈도수 (그래프 생성 시)\n",
    "max_length = 10 # 단어의 최대 길이\n",
    "wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "\n",
    "data = load_json('data/kiwi_data.json')\n",
    "\n",
    "documents = list(data.values())  # 각 영상의 댓글을 리스트로 저장\n",
    "#save_to_json(documents, \"zzzzzz.json\")\n",
    "#save_to_json(documents,\"kkkkkkkk.json\")\n",
    "beta = 0.85    # PageRank의 감쇄 계수\n",
    "max_iter = 10  # 반복 횟수\n",
    "\n",
    "keywords, rank, graph = wordrank_extractor.extract(documents, beta, max_iter)\n",
    "\n",
    "top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('영상', 25.502771313687216),\n",
       " ('중국', 25.074480449318674),\n",
       " ('감사', 22.73052987677838),\n",
       " ('나라', 21.934993295355003),\n",
       " ('국민', 19.50593275415689),\n",
       " ('민주', 16.632856910458496),\n",
       " ('생각', 16.579941894027638),\n",
       " ('대통령', 16.24687301827613),\n",
       " ('미국', 15.288325852647821),\n",
       " ('한국', 14.855848868533247),\n",
       " ('선수', 12.069327406563033),\n",
       " ('탄핵', 11.228954217748734),\n",
       " ('언론', 10.513135664646446),\n",
       " ('응원', 10.29886196699711),\n",
       " ('대한', 10.186445433564293),\n",
       " ('사랑', 9.651541204189082),\n",
       " ('인간', 9.58221759097418),\n",
       " ('광주', 9.39334142255769),\n",
       " ('게임', 8.721073688402956),\n",
       " ('윤석열', 8.344642215758588),\n",
       " ('최고', 8.114560166445347),\n",
       " ('아이', 8.063425272705212),\n",
       " ('이상', 7.720615919002939),\n",
       " ('문제', 7.538474090864509),\n",
       " ('정도', 7.48857273174012),\n",
       " ('사용', 7.15719455600075),\n",
       " ('내란', 7.041447022052002),\n",
       " ('노래', 6.999396110327232),\n",
       " ('이재명', 6.977740407384103),\n",
       " ('정치', 6.846065123879742),\n",
       " ('자유', 6.807886469390505),\n",
       " ('방송', 6.776148208969578),\n",
       " ('국회', 6.696478355776185),\n",
       " ('트럼프', 6.694016136806612),\n",
       " ('이번', 6.667760698942341),\n",
       " ('소리', 6.548912710316956),\n",
       " ('쓰레기', 6.360723669650443),\n",
       " ('거짓', 6.222909225734387),\n",
       " ('정보', 6.17270083162189),\n",
       " ('구속', 6.104568143524099),\n",
       " ('정신', 5.923236625423641),\n",
       " ('갤럭시', 5.893123487347862),\n",
       " ('의원', 5.833160437672497),\n",
       " ('시간', 5.784321657397902),\n",
       " ('삼성', 5.712288840515198),\n",
       " ('댓글', 5.655391920290096),\n",
       " ('좌파', 5.604409410564074),\n",
       " ('국가', 5.601752529485392),\n",
       " ('기자', 5.551323749658223),\n",
       " ('계엄', 5.533998645516685),\n",
       " ('재판', 5.503263869729048),\n",
       " ('범죄', 5.4198393612246685),\n",
       " ('오늘', 5.381789025077249),\n",
       " ('헌재', 5.375934259112313),\n",
       " ('필요', 5.361171866764325),\n",
       " ('뉴스', 5.206588810349325),\n",
       " ('축하', 5.164886742117667),\n",
       " ('이유', 5.078649404227084),\n",
       " ('수준', 4.863568904719699),\n",
       " ('가능', 4.812147726290076),\n",
       " ('다음', 4.7435721677428235),\n",
       " ('부정', 4.717945612277831),\n",
       " ('지금', 4.658257556186458),\n",
       " ('공산', 4.618140989572579),\n",
       " ('음주', 4.617596251071208),\n",
       " ('구독', 4.59583392480476),\n",
       " ('목소리', 4.571508143390411),\n",
       " ('마음', 4.528108439165859),\n",
       " ('노아', 4.51867284866697),\n",
       " ('집회', 4.460205597691972),\n",
       " ('프로', 4.362759590318432),\n",
       " ('검찰', 4.358030512036283),\n",
       " ('처벌', 4.35022778262654),\n",
       " ('세계', 4.3242813273234155),\n",
       " ('문형배', 4.240315992711679),\n",
       " ('수사', 4.185971142605684),\n",
       " ('세상', 4.152698848234621),\n",
       " ('행복', 4.138329041167774),\n",
       " ('기능', 4.110538602361143),\n",
       " ('시작', 4.066593160070242),\n",
       " ('느낌', 4.0634881979108055),\n",
       " ('군인', 3.9663593913625785),\n",
       " ('기대', 3.9566605885003914),\n",
       " ('요즘', 3.9431237387590548),\n",
       " ('유튜', 3.9104243669327734),\n",
       " ('처음', 3.9036439192397725),\n",
       " ('자랑', 3.891422403200031),\n",
       " ('김현', 3.84001076218302),\n",
       " ('서울', 3.837908461181433),\n",
       " ('진실', 3.8151165276608405),\n",
       " ('디자인', 3.7705200658084657),\n",
       " ('지지', 3.7579804435461046),\n",
       " ('송가', 3.728475624895238),\n",
       " ('김건', 3.710812120487687),\n",
       " ('우리', 3.6913531829034922),\n",
       " ('이제', 3.6682065553923464),\n",
       " ('중공', 3.631746313792637),\n",
       " ('기술', 3.579770221198547),\n",
       " ('아사달', 3.573046845507894),\n",
       " ('해체', 3.5460711559967835)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# KRWordrank(konlpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 저장: data/konlpy_data.json\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned video comments\n",
    "data = load_json('data/okt_nouns_video_comments.json')\n",
    "\n",
    "    \n",
    "# ✅ 데이터를 `merge_values` 함수에 맞게 변형\n",
    "# (카테고리 → 비디오 → 명사 리스트 구조를 `merge_values`에서 처리할 수 있도록 변형)\n",
    "transformed_data = {}\n",
    "\n",
    "for category, videos in data.items():\n",
    "    transformed_data[category] = []  # 카테고리별 리스트 생성\n",
    "\n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        nouns = video.get(\"nouns\", [])  # 명사 리스트 가져오기\n",
    "\n",
    "        # 명사 리스트를 하나의 리스트로 평탄화 (nested list 제거)\n",
    "        flat_nouns = [word for noun_list in nouns for word in noun_list]\n",
    "\n",
    "        # 병합할 데이터로 변환\n",
    "        transformed_data[category].append(flat_nouns)\n",
    "# ✅ `merge_values` 적용\n",
    "processed_data = merge_values(transformed_data)\n",
    "\n",
    "save_to_json(processed_data,'konlpy_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 저장: data/all_keywords_ranking.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom krwordrank.word import KRWordRank\\n\\nmin_count = 10   # 단어의 최소 출현 빈도수 (그래프 생성 시)\\nmax_length = 10  # 단어의 최대 길이\\nbeta = 0.85      # PageRank의 감쇄 계수\\nmax_iter = 10    # 반복 횟수\\n\\ndata = load_json(\\'data/konlpy_data.json\\')\\n\\ncategory_keywords = {}\\ni = 1\\nfor category, videos in data.items():\\n    \\n    if not documents:  # 빈 리스트 방지\\n        continue\\n\\n    wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\\n    save_to_json(videos, f\"del{category}\")\\n    # 카테고리별 키워드 추출\\n    keywords, rank, graph = wordrank_extractor.extract([videos], beta, max_iter)\\n    \\n    # 키워드 랭킹 정렬 후 저장\\n    top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]\\n    category_keywords[category] = top_keywords\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_count = 10   # 단어의 최소 출현 빈도수 (그래프 생성 시)\n",
    "max_length = 10 # 단어의 최대 길이\n",
    "wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "\n",
    "data = load_json('data/konlpy_data.json')\n",
    "\n",
    "documents = list(data.values())  # 각 영상의 댓글을 리스트로 저장\n",
    "\n",
    "beta = 0.85    # PageRank의 감쇄 계수\n",
    "max_iter = 10  # 반복 횟수\n",
    "\n",
    "keywords, rank, graph = wordrank_extractor.extract(documents, beta, max_iter)\n",
    "\n",
    "top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "save_to_json(top_keywords,\"all_keywords_ranking.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_keywords   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#카테고리별 전체 랭킹을 추출하기 위한 전처리 및 정리 단계 (최종적으로 나온 결과에 가중치 부여 할거임). all_categories 디렉토리에 각 카테고리의 댓글을 json파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 각 카테고리별 JSON 파일이 'all_categories' 디렉토리에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# 데이터 로드\n",
    "data = load_json('data/konlpy_data.json')\n",
    "\n",
    "# 저장할 디렉토리 설정\n",
    "output_dir = \"all_categories\"\n",
    "\n",
    "# 디렉토리 없으면 생성\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 각 카테고리별 JSON 파일 저장\n",
    "for category, videos in data.items():\n",
    "    # 영상들의 댓글 리스트를 하나의 문자열로 변환하여 저장\n",
    "    documents = [comment for video_comments in videos for comment in video_comments]\n",
    "    category_text = \"\".join(documents)  # 모든 댓글을 하나의 문자열로 합침 (공백으로 구분)\n",
    "\n",
    "    # 카테고리별 JSON 파일 저장 (파일명은 카테고리명 그대로)\n",
    "    category_file_path = os.path.join(output_dir, f\"{category}.json\")\n",
    "    with open(category_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([category_text], f, ensure_ascii=False, indent=4)  # 문자열을 리스트에 넣어 저장\n",
    "\n",
    "print(f\"✅ 각 카테고리별 JSON 파일이 '{output_dir}' 디렉토리에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#카테고리별 키워드 랭킹 추출 (지금은 kiwi가 훈련이 안되어 있어서 쓰레기 데이터가 추출됨) ver1 (category_keywords_ranking.json 파일에 모든 카테고리의 키워드 랭킹을 저장)\n",
    "\n",
    "그리고 지훈이가 짠 코드가 추출하는 all_keywords_ranking.json 파일은 카테고리별로 분류가 안되니까, 모델 훈련 뒤에 category_keywords_ranking.json 로 하는게 나을듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Pets & Animals 카테고리에 단어가 부족하여 키워드 추출을 건너뜀\n",
      "데이터 저장: data/category_keywords_ranking.json\n",
      "✅ 각 카테고리별 키워드 랭킹이 'category_keywords_ranking.json' 파일에 저장되었습니다.\n",
      "데이터 저장: data/all_keywords_ranking.json\n",
      "✅ 전체 키워드 랭킹이 'all_keywords_ranking.json' 파일에 저장되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    \\nmin_count = 10   # 단어의 최소 출현 빈도수 (그래프 생성 시)\\nmax_length = 10 # 단어의 최대 길이\\nwordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\\n\\ndata = load_json(\\'data/konlpy_data.json\\')\\n\\ndocuments = list(data.values())  # 각 영상의 댓글을 리스트로 저장\\n\\nbeta = 0.85    # PageRank의 감쇄 계수\\nmax_iter = 10  # 반복 횟수\\n\\nkeywords, rank, graph = wordrank_extractor.extract(documents, beta, max_iter)\\n\\ntop_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]\\nsave_to_json(top_keywords,\"all_keywords_ranking.json\")    \\n    '"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from krwordrank.word import KRWordRank\n",
    "\n",
    "# KRWordRank 설정\n",
    "min_count = 10   # 단어의 최소 출현 빈도수\n",
    "max_length = 10  # 단어의 최대 길이\n",
    "beta = 0.85      # PageRank의 감쇄 계수\n",
    "max_iter = 10    # 반복 횟수\n",
    "\n",
    "# `all_categories` 폴더에서 JSON 파일 불러오기\n",
    "input_dir = \"all_categories\"\n",
    "category_keywords = {}\n",
    "all_documents = []  # 전체 데이터를 저장할 리스트\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        category_path = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # JSON 파일 로드\n",
    "        with open(category_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        category = filename.replace(\".json\", \"\")  # 파일명에서 확장자 제거하여 카테고리명으로 사용\n",
    "        text = data[0] if data else \"\"  # 리스트 내 첫 번째 요소(문자열) 가져오기\n",
    "        \n",
    "        # 텍스트를 공백 단위로 분리하여 문서 리스트 생성\n",
    "        documents = text.split(\" \")\n",
    "\n",
    "        # 전체 데이터에 추가 (전체 키워드 분석용)\n",
    "        all_documents.extend(documents)\n",
    "\n",
    "        # 문서 내 단어가 2개 이상 있어야 KRWordRank 실행 가능\n",
    "        if len(documents) < 2:\n",
    "            print(f\"⚠️  {category} 카테고리에 단어가 부족하여 키워드 추출을 건너뜀\")\n",
    "            continue\n",
    "\n",
    "        # 카테고리별 키워드 추출\n",
    "        wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "        keywords, rank, graph = wordrank_extractor.extract(documents, beta, max_iter)\n",
    "\n",
    "        # 상위 100개 키워드 추출\n",
    "        top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "        category_keywords[category] = top_keywords\n",
    "\n",
    "# ✅ 카테고리별 키워드 랭킹 저장\n",
    "save_to_json(category_keywords, \"category_keywords_ranking.json\")\n",
    "\n",
    "print(\"✅ 각 카테고리별 키워드 랭킹이 'category_keywords_ranking.json' 파일에 저장되었습니다.\")\n",
    "\n",
    "# ✅ 전체 데이터를 한 번에 분석하여 키워드 랭킹 추출\n",
    "if len(all_documents) >= 2:  # 최소 두 개의 단어가 있어야 함\n",
    "    wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "    keywords, rank, graph = wordrank_extractor.extract(all_documents, beta, max_iter)\n",
    "\n",
    "    # 상위 100개 키워드 추출\n",
    "    top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "    save_to_json(top_keywords, \"all_keywords_ranking.json\")\n",
    "\n",
    "    print(\"✅ 전체 키워드 랭킹이 'all_keywords_ranking.json' 파일에 저장되었습니다.\")\n",
    "else:\n",
    "    print(\"⚠️ 전체 데이터에 단어가 부족하여 키워드 랭킹을 생성할 수 없음.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#카테고리별 키워드 랭킹 추출 ver2 (바로 위의 코드와 비슷함) (category_ranking 디렉토리에 카테고리별로 키워드 랭킹이 json파일로 생성되는 코드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Film & Animation 카테고리 키워드 랭킹이 'category_ranking/Film & Animation_ranking.json' 파일에 저장되었습니다.\n",
      "✅ Music 카테고리 키워드 랭킹이 'category_ranking/Music_ranking.json' 파일에 저장되었습니다.\n",
      "⚠️  Pets & Animals 카테고리에 단어가 부족하여 키워드 추출을 건너뜀\n",
      "✅ Entertainment 카테고리 키워드 랭킹이 'category_ranking/Entertainment_ranking.json' 파일에 저장되었습니다.\n",
      "✅ News & Politics 카테고리 키워드 랭킹이 'category_ranking/News & Politics_ranking.json' 파일에 저장되었습니다.\n",
      "✅ Gaming 카테고리 키워드 랭킹이 'category_ranking/Gaming_ranking.json' 파일에 저장되었습니다.\n",
      "✅ Sports 카테고리 키워드 랭킹이 'category_ranking/Sports_ranking.json' 파일에 저장되었습니다.\n",
      "✅ Science & Technology 카테고리 키워드 랭킹이 'category_ranking/Science & Technology_ranking.json' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from krwordrank.word import KRWordRank\n",
    "\n",
    "# KRWordRank 설정\n",
    "min_count = 10   # 단어의 최소 출현 빈도수\n",
    "max_length = 10  # 단어의 최대 길이\n",
    "beta = 0.85      # PageRank의 감쇄 계수\n",
    "max_iter = 10    # 반복 횟수\n",
    "\n",
    "# 입력 디렉토리 (all_categories)\n",
    "input_dir = \"all_categories\"\n",
    "\n",
    "# 출력 디렉토리 (category_ranking) 생성\n",
    "output_dir = \"category_ranking\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 각 카테고리별 키워드 랭킹 추출\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        category_path = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # JSON 파일 로드\n",
    "        with open(category_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        category = filename.replace(\".json\", \"\")  # 파일명에서 확장자 제거하여 카테고리명으로 사용\n",
    "        text = data[0] if data else \"\"  # 리스트 내 첫 번째 요소(문자열) 가져오기\n",
    "        \n",
    "        # 텍스트를 공백 단위로 분리하여 문서 리스트 생성\n",
    "        documents = text.split(\" \")\n",
    "\n",
    "        # 문서 내 단어가 2개 이상 있어야 KRWordRank 실행 가능\n",
    "        if len(documents) < 2:\n",
    "            print(f\"⚠️  {category} 카테고리에 단어가 부족하여 키워드 추출을 건너뜀\")\n",
    "            continue\n",
    "\n",
    "        \"\"\"\n",
    "        ✅ 각 카테고리별 키워드 랭킹 추출\n",
    "        \"\"\"\n",
    "        wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "        keywords, rank, graph = wordrank_extractor.extract(documents, beta, max_iter)\n",
    "\n",
    "        # 상위 100개 키워드 추출\n",
    "        top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "        # 결과 저장 (각 카테고리별 JSON 파일을 `category_ranking` 폴더에 저장)\n",
    "        ranking_file = os.path.join(output_dir, f\"{category}_ranking.json\")\n",
    "        with open(ranking_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(top_keywords, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"✅ {category} 카테고리 키워드 랭킹이 '{ranking_file}' 파일에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제목, 태그 가중치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json('data/raw_video_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김어준의 겸손은힘들다 뉴스공장 2025년 2월 17일 월요일 [강기정, 노영희, 박선원, 김경호, 박범계, 여론조사]\n"
     ]
    }
   ],
   "source": [
    "title_tag = merge_title_tags(data)\n",
    "print(title_tag[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Spacing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m cleaned_texts \u001b[38;5;241m=\u001b[39m [clean_text(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m title_tag]\n\u001b[1;32m      5\u001b[0m cleaned_texts\n\u001b[0;32m----> 7\u001b[0m ko_spacing \u001b[38;5;241m=\u001b[39m \u001b[43mSpacing\u001b[49m()\n\u001b[1;32m      9\u001b[0m merged_sentences \u001b[38;5;241m=\u001b[39m [ko_spacing(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m cleaned_texts]\n\u001b[1;32m     11\u001b[0m merged_sentences\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Spacing' is not defined"
     ]
    }
   ],
   "source": [
    "cleaned_texts = []\n",
    "\n",
    "cleaned_texts = [clean_text(text) for text in title_tag]\n",
    "    \n",
    "cleaned_texts\n",
    "\n",
    "ko_spacing = Spacing()\n",
    "\n",
    "merged_sentences = [ko_spacing(text) for text in cleaned_texts]\n",
    "\n",
    "merged_sentences\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Kiwi를 이용한 명사 추출\u001b[39;00m\n\u001b[1;32m      2\u001b[0m kiwi_nouns_sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmerged_sentences\u001b[49m:\n\u001b[1;32m      5\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m kiwi\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[1;32m      6\u001b[0m     nouns \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mform \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mtag \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNNG\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNNP\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token\u001b[38;5;241m.\u001b[39mform) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "# Kiwi를 이용한 명사 추출\n",
    "kiwi_nouns_sentences = []\n",
    "\n",
    "for text in merged_sentences:\n",
    "    tokens = kiwi.tokenize(text)\n",
    "    nouns = [token.form for token in tokens if token.tag in [\"NNG\", \"NNP\"] and len(token.form) > 1]\n",
    "    kiwi_nouns_sentences.append(nouns)\n",
    "\n",
    "kiwi_nouns_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('The graph should consist of at least two nodes\\n', 'The node size of inserted graph is 0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.85\u001b[39m    \u001b[38;5;66;03m# PageRank의 감쇄 계수\u001b[39;00m\n\u001b[1;32m     13\u001b[0m max_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# 반복 횟수\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m keywords, rank, graph \u001b[38;5;241m=\u001b[39m \u001b[43mwordrank_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m top_keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(keywords\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:\u001b[38;5;241m100\u001b[39m]\n",
      "File \u001b[0;32m~/soynlp_training_data/.venv/lib/python3.12/site-packages/krwordrank/word/_word.py:211\u001b[0m, in \u001b[0;36mKRWordRank.extract\u001b[0;34m(self, docs, beta, max_iter, num_keywords, num_rset, vocabulary, bias, rset)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract\u001b[39m(\u001b[38;5;28mself\u001b[39m, docs, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_keywords\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    165\u001b[0m     num_rset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocabulary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, rset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    It constructs word graph and trains ranks of each node using HITS algorithm.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    After training it selects suitable subwords as words.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m        >>> keywords, rank, graph = wordrank_extractor.extract(texts, beta, max_iter, verbose)\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     rank, graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     lset \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint2token(idx)[\u001b[38;5;241m0\u001b[39m]:r \u001b[38;5;28;01mfor\u001b[39;00m idx, r \u001b[38;5;129;01min\u001b[39;00m rank\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint2token(idx)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rset:\n",
      "File \u001b[0;32m~/soynlp_training_data/.venv/lib/python3.12/site-packages/krwordrank/word/_word.py:326\u001b[0m, in \u001b[0;36mKRWordRank.train\u001b[0;34m(self, docs, beta, max_iter, vocabulary, bias)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_index2vocab()\n\u001b[1;32m    324\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_word_graph(docs)\n\u001b[0;32m--> 326\u001b[0m rank \u001b[38;5;241m=\u001b[39m \u001b[43mhits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m            \u001b[49m\u001b[43msum_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnumber_of_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rank, graph\n",
      "File \u001b[0;32m~/soynlp_training_data/.venv/lib/python3.12/site-packages/krwordrank/graph/_rank.py:38\u001b[0m, in \u001b[0;36mhits\u001b[0;34m(graph, beta, max_iter, bias, verbose, sum_weight, number_of_nodes, converge)\u001b[0m\n\u001b[1;32m     35\u001b[0m     number_of_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(graph), \u001b[38;5;28mlen\u001b[39m(bias))\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m number_of_nodes \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe graph should consist of at least two nodes\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe node size of inserted graph is \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m number_of_nodes\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     43\u001b[0m dw \u001b[38;5;241m=\u001b[39m sum_weight \u001b[38;5;241m/\u001b[39m number_of_nodes\n\u001b[1;32m     44\u001b[0m rank \u001b[38;5;241m=\u001b[39m {node:dw \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mkeys()}\n",
      "\u001b[0;31mValueError\u001b[0m: ('The graph should consist of at least two nodes\\n', 'The node size of inserted graph is 0')"
     ]
    }
   ],
   "source": [
    "merged_sentences = []\n",
    "\n",
    "for values in kiwi_nouns_sentences:\n",
    "    if isinstance(values, list):  # 값이 리스트인지 확인\n",
    "        merged_sentence = \" \".join(\n",
    "            s for s in values if s.strip() and len(s.strip()) > 1\n",
    "        )  # 빈 문자열 & 한 글자 제외 후 공백으로 연결\n",
    "        merged_sentences.append(merged_sentence)\n",
    "    \n",
    "\n",
    "\n",
    "beta = 0.85    # PageRank의 감쇄 계수\n",
    "max_iter = 10  # 반복 횟수\n",
    "\n",
    "keywords, rank, graph = wordrank_extractor.extract(merged_sentences, beta, max_iter)\n",
    "\n",
    "top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('광주', 21.883339764024786),\n",
       " ('대통령', 21.14930802419712),\n",
       " ('국민', 21.066524360917278),\n",
       " ('민주', 17.903521408605034),\n",
       " ('중국', 14.755495150088827),\n",
       " ('미국', 14.219984383747393),\n",
       " ('나라', 13.290342295229914),\n",
       " ('사람', 12.697506902207595),\n",
       " ('우리', 12.120483967025589),\n",
       " ('탄핵', 12.114412133709308),\n",
       " ('대한', 11.135607329155505),\n",
       " ('한동훈', 10.879634026849658),\n",
       " ('언론', 10.203989812207837),\n",
       " ('윤석열', 9.996447768514912),\n",
       " ('한국', 8.879613386251465),\n",
       " ('자유', 8.581780161186199),\n",
       " ('정치', 8.448722047613371),\n",
       " ('생각', 8.13896711204735),\n",
       " ('인간', 7.869204905101948),\n",
       " ('트럼프', 7.204134168729392),\n",
       " ('내란', 7.20330227348747),\n",
       " ('지금', 6.771218567348587),\n",
       " ('진짜', 6.766817801477181),\n",
       " ('좌파', 6.467655625686199),\n",
       " ('정말', 6.44997889860016),\n",
       " ('이재명', 6.442539967645999),\n",
       " ('계엄', 6.384078296097432),\n",
       " ('집회', 6.263202683852292),\n",
       " ('방송', 6.197539372900395),\n",
       " ('시민', 6.101414643916003),\n",
       " ('쓰레기', 5.9595224050726365),\n",
       " ('김건희', 5.930262026673098),\n",
       " ('응원', 5.789553527904718),\n",
       " ('거짓', 5.788991450295141),\n",
       " ('국회', 5.730767645232271),\n",
       " ('헌재', 5.602791647780293),\n",
       " ('검찰', 5.515897353983046),\n",
       " ('국가', 5.420032015619803),\n",
       " ('부정', 5.3511937164313235),\n",
       " ('국힘', 5.182193740831479),\n",
       " ('이제', 5.179311080305875),\n",
       " ('하나', 5.173291089689636),\n",
       " ('전라도', 5.109214575496954),\n",
       " ('의원', 4.989661887427296),\n",
       " ('변호', 4.813763403822942),\n",
       " ('뉴스', 4.604327750247186),\n",
       " ('보수', 4.4946854659144595),\n",
       " ('재판', 4.365231551029191),\n",
       " ('소리', 4.272439610165227),\n",
       " ('김현정', 4.210072597638495),\n",
       " ('때문', 4.205477126953501),\n",
       " ('북한', 4.165453711128667),\n",
       " ('장성', 4.149585108557824),\n",
       " ('구속', 4.04725063538093),\n",
       " ('대표', 4.037122898067671),\n",
       " ('기자', 3.9951583449985915),\n",
       " ('모두', 3.965344732444664),\n",
       " ('정신', 3.9436667209221494),\n",
       " ('화이팅', 3.923224476799152),\n",
       " ('지지', 3.8944086604966395),\n",
       " ('해체', 3.877620998924716),\n",
       " ('공산', 3.8724335122669467),\n",
       " ('반대', 3.8533329515000987),\n",
       " ('배신자', 3.8502368920687826),\n",
       " ('극우', 3.7600156917707923),\n",
       " ('전한길', 3.702940272882116),\n",
       " ('세력', 3.628194609242365),\n",
       " ('수사', 3.621829709523002),\n",
       " ('문제', 3.6156090371507164),\n",
       " ('최고', 3.57038756115811),\n",
       " ('김병주', 3.4942996248508282),\n",
       " ('위해', 3.4785479034772333),\n",
       " ('중공', 3.352202911442926),\n",
       " ('수준', 3.2748925913109366),\n",
       " ('그냥', 3.2408305201835974),\n",
       " ('애국', 3.2144645170912463),\n",
       " ('보고', 3.2135380902133166),\n",
       " ('명태', 3.15213680461609),\n",
       " ('우파', 3.0637982630495015),\n",
       " ('절대', 3.05675472287878),\n",
       " ('간첩', 3.0239328349102124),\n",
       " ('저런', 2.9700468412466288),\n",
       " ('호남', 2.9120874769885905),\n",
       " ('보도', 2.8111462053689302),\n",
       " ('발전', 2.7987970456592257),\n",
       " ('선동', 2.7737436147964813),\n",
       " ('역사', 2.769319172185835),\n",
       " ('복귀', 2.756237069657814),\n",
       " ('헌법', 2.7266505823349765),\n",
       " ('이유', 2.7169482036559396),\n",
       " ('조작', 2.6476384809073346),\n",
       " ('지역', 2.6327699144220365),\n",
       " ('사실', 2.62719260166544),\n",
       " ('제발', 2.6211744120405744),\n",
       " ('범죄', 2.619104760164978),\n",
       " ('무슨', 2.6140136964487115),\n",
       " ('정도', 2.604586702212233),\n",
       " ('이준석', 2.5903641331294307),\n",
       " ('일본', 2.560988416892556),\n",
       " ('진실', 2.5504779820564876)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     키워드  TF-IDF 점수\n",
      "0     국민   0.312915\n",
      "1     광주   0.268098\n",
      "2     나라   0.267983\n",
      "3    민주당   0.259020\n",
      "4    대통령   0.238910\n",
      "5     미국   0.204320\n",
      "6     중국   0.182256\n",
      "7   대한민국   0.177315\n",
      "8     감사   0.140772\n",
      "9     언론   0.134911\n",
      "10    탄핵   0.133877\n",
      "11    한국   0.130659\n",
      "12   한동훈   0.128016\n",
      "13    생각   0.124569\n",
      "14   윤석열   0.119742\n",
      "15    인간   0.115260\n",
      "16   트럼프   0.103654\n",
      "17    좌파   0.100206\n",
      "18    의원   0.098368\n",
      "19    정치   0.093197\n",
      "20   이재명   0.091128\n",
      "21    집회   0.088485\n",
      "22    내란   0.088370\n",
      "23    검찰   0.086646\n",
      "24    방송   0.085153\n",
      "25    헌재   0.084003\n",
      "26    계엄   0.081705\n",
      "27    응원   0.078487\n",
      "28    국가   0.076994\n",
      "29  부정선거   0.076879\n",
      "30    자유   0.075500\n",
      "31   전라도   0.075040\n",
      "32   쓰레기   0.073086\n",
      "33    국회   0.070558\n",
      "34    보수   0.068605\n",
      "35    수사   0.068375\n",
      "36   거짓말   0.066881\n",
      "37    정신   0.065962\n",
      "38    구속   0.063778\n",
      "39    소리   0.062054\n",
      "40    기자   0.060790\n",
      "41    장성   0.059411\n",
      "42   김현정   0.057803\n",
      "43    북한   0.057458\n",
      "44    극우   0.055734\n",
      "45  광주시민   0.055389\n",
      "46    문제   0.054815\n",
      "47   변호사   0.054240\n",
      "48   김건희   0.053321\n",
      "49    대표   0.052746\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# ✅ 예제 키워드 리스트 (제목 + 태그 조합)\n",
    "data = load_json('data/kiwi_data.json')\n",
    "\n",
    "documents = list(data.values())  # 각 영상 댓글을 리스트로 저장\n",
    "\n",
    "# ✅ TF-IDF 모델 적용\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# ✅ 키워드별 TF-IDF 점수 계산\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "word_tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "sorted_indices = word_tfidf_scores.argsort()[::-1]  # 중요도 높은 순으로 정렬\n",
    "keywords_ranked = [(feature_names[i], word_tfidf_scores[i]) for i in sorted_indices]\n",
    "\n",
    "# ✅ 상위 10개 키워드 출력\n",
    "df_keywords = pd.DataFrame(keywords_ranked, columns=[\"키워드\", \"TF-IDF 점수\"])\n",
    "print(df_keywords.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 키워드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 저장: data/view_like_counts.json\n"
     ]
    }
   ],
   "source": [
    "data = load_json('data/kiwi_data.json')\n",
    "\n",
    "# Load the raw video data\n",
    "raw_data = load_json('data/raw_video_data.json')\n",
    "\n",
    "# Extract view count and like count for each video id\n",
    "view_like_counts = {}\n",
    "for video_id, text in data.items():\n",
    "    for category, videos in raw_data.items():\n",
    "        for video in videos:\n",
    "            if video[\"video_id\"] == video_id:\n",
    "                view_like_counts[video_id] = {\n",
    "                    \"view_count\": video[\"view_count\"],\n",
    "                    \"like_count\": video[\"like_count\"],\n",
    "                    \"text\": text\n",
    "                }\n",
    "                break\n",
    "\n",
    "save_to_json(view_like_counts, 'view_like_counts.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keyword 좋아요, 조회수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [키워드, 조회수, 좋아요, 등장 횟수]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "data = load_json('data/view_like_counts.json')\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ✅ 키워드별로 조회수 & 좋아요를 저장할 딕셔너리\n",
    "keyword_stats = defaultdict(lambda: {\"view_count\": 0, \"like_count\": 0, \"등장 횟수\": 0})\n",
    "\n",
    "# ✅ 각 비디오에서 키워드별 조회수 & 좋아요 점수 합산\n",
    "for video_id, info in data.items():\n",
    "    text = info[\"text\"]\n",
    "    view_count = info[\"view_count\"]\n",
    "    like_count = info[\"like_count\"]\n",
    "\n",
    "    # 🔥 키워드 빈도 계산\n",
    "    word_list = text.split()\n",
    "    word_counts = defaultdict(int)\n",
    "    for word in word_list:\n",
    "        word_counts[word] += 1\n",
    "\n",
    "    # 🔥 키워드별 점수 부여 (비디오의 점수를 해당 키워드에 분배)\n",
    "    for word, count in word_counts.items():\n",
    "        keyword_stats[word][\"view_count\"] += (view_count * count) / len(word_list)\n",
    "        keyword_stats[word][\"like_count\"] += (like_count * count) / len(word_list)\n",
    "        keyword_stats[word][\"등장 횟수\"] += count\n",
    "\n",
    "# ✅ 키워드별 정렬 (조회수 기준)\n",
    "sorted_keywords = sorted(keyword_stats.items(), key=lambda x: x[1][\"view_count\"], reverse=True)\n",
    "\n",
    "# ✅ 데이터프레임 생성\n",
    "df_keywords = pd.DataFrame([(k, v[\"view_count\"], v[\"like_count\"], v[\"등장 횟수\"]) for k, v in sorted_keywords],\n",
    "                           columns=[\"키워드\", \"조회수\", \"좋아요\", \"등장 횟수\"])\n",
    "\n",
    "# ✅ 상위 10개 키워드 출력\n",
    "print(df_keywords.head(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('The graph should consist of at least two nodes\\n', 'The node size of inserted graph is 0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# ✅ KRWordRank 모델 적용\u001b[39;00m\n\u001b[1;32m     10\u001b[0m wordrank_extractor \u001b[38;5;241m=\u001b[39m KRWordRank(min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m keywords, rank, graph \u001b[38;5;241m=\u001b[39m \u001b[43mwordrank_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.85\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(keywords)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# ✅ 키워드별 조회수 & 좋아요 반영할 딕셔너리\u001b[39;00m\n",
      "File \u001b[0;32m~/soynlp_training_data/.venv/lib/python3.12/site-packages/krwordrank/word/_word.py:211\u001b[0m, in \u001b[0;36mKRWordRank.extract\u001b[0;34m(self, docs, beta, max_iter, num_keywords, num_rset, vocabulary, bias, rset)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract\u001b[39m(\u001b[38;5;28mself\u001b[39m, docs, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_keywords\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    165\u001b[0m     num_rset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocabulary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, rset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    It constructs word graph and trains ranks of each node using HITS algorithm.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    After training it selects suitable subwords as words.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m        >>> keywords, rank, graph = wordrank_extractor.extract(texts, beta, max_iter, verbose)\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     rank, graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     lset \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint2token(idx)[\u001b[38;5;241m0\u001b[39m]:r \u001b[38;5;28;01mfor\u001b[39;00m idx, r \u001b[38;5;129;01min\u001b[39;00m rank\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint2token(idx)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rset:\n",
      "File \u001b[0;32m~/soynlp_training_data/.venv/lib/python3.12/site-packages/krwordrank/word/_word.py:326\u001b[0m, in \u001b[0;36mKRWordRank.train\u001b[0;34m(self, docs, beta, max_iter, vocabulary, bias)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_index2vocab()\n\u001b[1;32m    324\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_word_graph(docs)\n\u001b[0;32m--> 326\u001b[0m rank \u001b[38;5;241m=\u001b[39m \u001b[43mhits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m            \u001b[49m\u001b[43msum_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnumber_of_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocabulary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rank, graph\n",
      "File \u001b[0;32m~/soynlp_training_data/.venv/lib/python3.12/site-packages/krwordrank/graph/_rank.py:38\u001b[0m, in \u001b[0;36mhits\u001b[0;34m(graph, beta, max_iter, bias, verbose, sum_weight, number_of_nodes, converge)\u001b[0m\n\u001b[1;32m     35\u001b[0m     number_of_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(graph), \u001b[38;5;28mlen\u001b[39m(bias))\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m number_of_nodes \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe graph should consist of at least two nodes\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe node size of inserted graph is \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m number_of_nodes\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     43\u001b[0m dw \u001b[38;5;241m=\u001b[39m sum_weight \u001b[38;5;241m/\u001b[39m number_of_nodes\n\u001b[1;32m     44\u001b[0m rank \u001b[38;5;241m=\u001b[39m {node:dw \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mkeys()}\n",
      "\u001b[0;31mValueError\u001b[0m: ('The graph should consist of at least two nodes\\n', 'The node size of inserted graph is 0')"
     ]
    }
   ],
   "source": [
    "from krwordrank.word import KRWordRank\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# ✅ KRWordRank를 적용할 텍스트 리스트 생성\n",
    "texts = [info[\"text\"] for info in data.values()]\n",
    "\n",
    "print(texts)\n",
    "# ✅ KRWordRank 모델 적용\n",
    "wordrank_extractor = KRWordRank(min_count=1, max_length=10, verbose=False)\n",
    "keywords, rank, graph = wordrank_extractor.extract(texts, beta=0.85, max_iter=10)\n",
    "\n",
    "print(keywords)\n",
    "\n",
    "# ✅ 키워드별 조회수 & 좋아요 반영할 딕셔너리\n",
    "keyword_stats = defaultdict(lambda: {\"view_count\": 0, \"like_count\": 0, \"점수\": 0})\n",
    "\n",
    "# ✅ 각 비디오에서 키워드별 조회수 & 좋아요 점수 합산\n",
    "for video_id, info in data.items():\n",
    "    text = info[\"text\"]\n",
    "    view_count = info[\"view_count\"]\n",
    "    like_count = info[\"like_count\"]\n",
    "\n",
    "    # 🔥 KRWordRank에서 추출한 키워드가 해당 문장에 포함되어 있는지 확인\n",
    "    for keyword in keywords.keys():\n",
    "        if keyword in text:\n",
    "            keyword_stats[keyword][\"view_count\"] += view_count\n",
    "            keyword_stats[keyword][\"like_count\"] += like_count\n",
    "            keyword_stats[keyword][\"점수\"] += rank.get(keyword, 0) * (view_count * 0.7 + like_count * 0.3)  # ✅ KeyError 방지\n",
    "\n",
    "# ✅ 키워드별 정렬 (점수 기준)\n",
    "sorted_keywords = sorted(keyword_stats.items(), key=lambda x: x[1][\"점수\"], reverse=True)\n",
    "\n",
    "# ✅ 데이터프레임 생성\n",
    "df_keywords = pd.DataFrame([(k, v[\"view_count\"], v[\"like_count\"], v[\"점수\"]) for k, v in sorted_keywords],\n",
    "                           columns=[\"키워드\", \"조회수\", \"좋아요\", \"점수\"])\n",
    "\n",
    "# ✅ 상위 10개 키워드 출력\n",
    "print(df_keywords.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인기동영상의 키워드에는 가중치 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kiwi_nouns_video_comments 댓글을 4등분 후 댓글 평탄화\n",
    "예시\n",
    "{\n",
    "    \"News & Politics\": [\n",
    "        {\n",
    "            \"video_id\": \"IC3iNhz02l0\",\n",
    "            \"nouns\": [\n",
    "                [\n",
    "                    \"윤석열\",\n",
    "                    \"변호인\",....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터가 'data/flattened_nouns_groups.json'에 저장되었습니다!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# ✅ JSON 파일 불러오기\n",
    "file_path = \"data/kiwi_nouns_video_comments.json\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# ✅ 카테고리별 4등분 및 평탄화\n",
    "ranked_groups = {}\n",
    "\n",
    "for category, videos in data.items():\n",
    "    total_videos = len(videos)\n",
    "    quarter_size = max(1, total_videos // 4)  # 최소 1개 유지\n",
    "\n",
    "    # ✅ 4개 그룹으로 나누기\n",
    "    ranked_groups[category] = {\n",
    "        \"1_25\": videos[:quarter_size],  # 상위 25%\n",
    "        \"25_50\": videos[quarter_size : quarter_size * 2],  # 25% ~ 50%\n",
    "        \"50_75\": videos[quarter_size * 2 : quarter_size * 3],  # 50% ~ 75%\n",
    "        \"75_100\": videos[quarter_size * 3 :]  # 75% ~ 100%\n",
    "    }\n",
    "\n",
    "# ✅ 각 그룹별 평탄화 진행\n",
    "flattened_groups = {}\n",
    "\n",
    "\n",
    "\n",
    "for category, groups in ranked_groups.items():\n",
    "    flattened_groups[category] = {}\n",
    "\n",
    "    for group_name, group_videos in groups.items():\n",
    "        # ✅ 명사(nouns) 리스트를 평탄화\n",
    "        all_nouns = [\n",
    "            word for video in group_videos if isinstance(video, dict) and \"nouns\" in video\n",
    "            for noun_list in video[\"nouns\"]\n",
    "            for word in noun_list\n",
    "        ]\n",
    "        flattened_groups[category][group_name] = all_nouns\n",
    "\n",
    "# ✅ 결과 저장\n",
    "output_path = \"data/flattened_nouns_groups.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(flattened_groups, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ 데이터가 '{output_path}'에 저장되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평탄화된 각 그룹마다 noun의 문자열 만들기.\n",
    "\n",
    "예시 \n",
    "{\n",
    "    \"News & Politics\": {\n",
    "        \"1_25\": \"윤석열 변호인 이야기 쓰레기 느낌 처리 나라 장원 진술 거짓 프로젝트 초등 아이 아이 평생 추억 가족 단위 계획 생각 형제 나라 일반 우쭈쭈 생각 트럼프 형제 형제 생각 김어준 자기 과시 확인 진상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 각 그룹별 문자열 변환 완료! 'data/group_texts.json'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ✅ JSON 파일 로드\n",
    "file_path = \"data/flattened_nouns_groups.json\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    flattened_data = json.load(f)\n",
    "\n",
    "# ✅ 각 그룹별 명사 리스트를 하나의 문자열로 변환\n",
    "group_texts = {}\n",
    "\n",
    "for category, groups in flattened_data.items():\n",
    "    group_texts[category] = {}\n",
    "\n",
    "    for group_name, all_nouns in groups.items():\n",
    "        # ✅ 명사 리스트를 공백으로 연결하여 문자열로 변환\n",
    "        group_text = \" \".join(all_nouns)\n",
    "        group_texts[category][group_name] = group_text\n",
    "\n",
    "# ✅ 결과 저장\n",
    "output_path = \"data/group_texts.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(group_texts, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ 각 그룹별 문자열 변환 완료! '{output_path}'에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 카테고리의 각 그룹을 categories 디렉토리에 json파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ News_&_Politics_1_25.json 저장 완료!\n",
      "✅ News_&_Politics_25_50.json 저장 완료!\n",
      "✅ News_&_Politics_50_75.json 저장 완료!\n",
      "✅ News_&_Politics_75_100.json 저장 완료!\n",
      "✅ Film_&_Animation_1_25.json 저장 완료!\n",
      "✅ Film_&_Animation_25_50.json 저장 완료!\n",
      "✅ Film_&_Animation_50_75.json 저장 완료!\n",
      "✅ Film_&_Animation_75_100.json 저장 완료!\n",
      "✅ Music_1_25.json 저장 완료!\n",
      "✅ Music_25_50.json 저장 완료!\n",
      "✅ Music_50_75.json 저장 완료!\n",
      "✅ Music_75_100.json 저장 완료!\n",
      "✅ Pets_&_Animals_1_25.json 저장 완료!\n",
      "✅ Pets_&_Animals_25_50.json 저장 완료!\n",
      "✅ Pets_&_Animals_50_75.json 저장 완료!\n",
      "✅ Pets_&_Animals_75_100.json 저장 완료!\n",
      "✅ Sports_1_25.json 저장 완료!\n",
      "✅ Sports_25_50.json 저장 완료!\n",
      "✅ Sports_50_75.json 저장 완료!\n",
      "✅ Sports_75_100.json 저장 완료!\n",
      "✅ Gaming_1_25.json 저장 완료!\n",
      "✅ Gaming_25_50.json 저장 완료!\n",
      "✅ Gaming_50_75.json 저장 완료!\n",
      "✅ Gaming_75_100.json 저장 완료!\n",
      "✅ Entertainment_1_25.json 저장 완료!\n",
      "✅ Entertainment_25_50.json 저장 완료!\n",
      "✅ Entertainment_50_75.json 저장 완료!\n",
      "✅ Entertainment_75_100.json 저장 완료!\n",
      "✅ Science_&_Technology_1_25.json 저장 완료!\n",
      "✅ Science_&_Technology_25_50.json 저장 완료!\n",
      "✅ Science_&_Technology_50_75.json 저장 완료!\n",
      "✅ Science_&_Technology_75_100.json 저장 완료!\n",
      "✅ 모든 카테고리의 구역 JSON 파일 저장이 완료되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ✅ JSON 파일 로드\n",
    "file_path = \"data/group_texts.json\"\n",
    "output_dir = \"data/categories\"  # 저장될 디렉토리\n",
    "\n",
    "# ✅ 저장할 디렉토리가 없으면 생성\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    group_texts = json.load(f)  # 전체 데이터 로드\n",
    "\n",
    "# ✅ 각 카테고리별 구역을 개별 JSON 파일로 저장\n",
    "for category, groups in group_texts.items():\n",
    "    for group_name, text in groups.items():\n",
    "        # ✅ 파일명 생성 (예: News & Politics_1_25.json)\n",
    "        safe_category = category.replace(\" \", \"_\")  # 공백 제거\n",
    "        file_name = f\"{safe_category}_{group_name}.json\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "        # ✅ JSON 저장\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(text, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"✅ {file_name} 저장 완료!\")\n",
    "\n",
    "print(\"✅ 모든 카테고리의 구역 JSON 파일 저장이 완료되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===============결국 실패, 왜인지 모르겠지만, 순위가 겹침=============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Film_&_Animation_75_100.json: KRWordRank 실행 완료 (추출된 키워드 개수: 221)\n",
      "✅ Film_&_Animation_75_100_ranking.json 저장 완료!\n",
      "🚨 Film_&_Animation_1_25.json: KRWordRank 실행 중 오류 발생: 'NoneType' object has no attribute 'items'\n",
      "✅ Entertainment_25_50.json: KRWordRank 실행 완료 (추출된 키워드 개수: 270)\n",
      "✅ Entertainment_25_50_ranking.json 저장 완료!\n",
      "🚨 Entertainment_1_25.json: KRWordRank 실행 중 오류 발생: 'NoneType' object has no attribute 'items'\n",
      "✅ Science_&_Technology_1_25.json: KRWordRank 실행 완료 (추출된 키워드 개수: 937)\n",
      "✅ Science_&_Technology_1_25_ranking.json 저장 완료!\n",
      "🚨 Film_&_Animation_25_50.json: KRWordRank 실행 중 오류 발생: 'NoneType' object has no attribute 'items'\n",
      "✅ Entertainment_50_75.json: KRWordRank 실행 완료 (추출된 키워드 개수: 18)\n",
      "✅ Entertainment_50_75_ranking.json 저장 완료!\n",
      "🚨 News_&_Politics_75_100.json: KRWordRank 실행 중 오류 발생: 'NoneType' object has no attribute 'items'\n",
      "✅ Music_75_100.json: KRWordRank 실행 완료 (추출된 키워드 개수: 229)\n",
      "✅ Music_75_100_ranking.json 저장 완료!\n",
      "🚨 Gaming_75_100.json: KRWordRank 실행 중 오류 발생: 'NoneType' object has no attribute 'items'\n",
      "✅ Film_&_Animation_50_75.json: KRWordRank 실행 완료 (추출된 키워드 개수: 30)\n",
      "✅ Film_&_Animation_50_75_ranking.json 저장 완료!\n",
      "🚨 Science_&_Technology_50_75.json: KRWordRank 실행 중 오류 발생: 'NoneType' object has no attribute 'items'\n",
      "✅ News_&_Politics_1_25.json: KRWordRank 실행 완료 (추출된 키워드 개수: 1597)\n",
      "✅ News_&_Politics_1_25_ranking.json 저장 완료!\n",
      "🚨 Gaming_50_75.json: KRWordRank 실행 중 오류 발생: 'NoneType' object has no attribute 'items'\n",
      "🚨 Pets_&_Animals_1_25.json: KRWordRank 실행 중 오류 발생: ('The graph should consist of at least two nodes\\n', 'The node size of inserted graph is 0')\n",
      "✅ Science_&_Technology_25_50.json: KRWordRank 실행 완료 (추출된 키워드 개수: 586)\n",
      "✅ Science_&_Technology_25_50_ranking.json 저장 완료!\n",
      "🚨 Gaming_25_50.json: KRWordRank 실행 중 오류 발생: 'NoneType' object has no attribute 'items'\n",
      "✅ Entertainment_75_100.json: KRWordRank 실행 완료 (추출된 키워드 개수: 420)\n",
      "✅ Entertainment_75_100_ranking.json 저장 완료!\n",
      "🚨 Pets_&_Animals_75_100.json: KRWordRank 실행 중 오류 발생: 'NoneType' object has no attribute 'items'\n",
      "✅ News_&_Politics_50_75.json: KRWordRank 실행 완료 (추출된 키워드 개수: 1458)\n",
      "✅ News_&_Politics_50_75_ranking.json 저장 완료!\n",
      "🚨 Sports_1_25.json: KRWordRank 실행 중 오류 발생: 'NoneType' object has no attribute 'items'\n",
      "🚨 Pets_&_Animals_50_75.json: KRWordRank 실행 중 오류 발생: ('The graph should consist of at least two nodes\\n', 'The node size of inserted graph is 0')\n",
      "✅ Music_50_75.json: KRWordRank 실행 완료 (추출된 키워드 개수: 35)\n",
      "✅ Music_50_75_ranking.json 저장 완료!\n",
      "🚨 Sports_75_100.json: KRWordRank 실행 중 오류 발생: 'NoneType' object has no attribute 'items'\n",
      "✅ News_&_Politics_25_50.json: KRWordRank 실행 완료 (추출된 키워드 개수: 1457)\n",
      "✅ News_&_Politics_25_50_ranking.json 저장 완료!\n",
      "🚨 Science_&_Technology_75_100.json: KRWordRank 실행 중 오류 발생: 'NoneType' object has no attribute 'items'\n",
      "✅ Music_25_50.json: KRWordRank 실행 완료 (추출된 키워드 개수: 196)\n",
      "✅ Music_25_50_ranking.json 저장 완료!\n",
      "🚨 Pets_&_Animals_25_50.json: KRWordRank 실행 중 오류 발생: 'NoneType' object has no attribute 'items'\n",
      "✅ Music_1_25.json: KRWordRank 실행 완료 (추출된 키워드 개수: 62)\n",
      "✅ Music_1_25_ranking.json 저장 완료!\n",
      "🚨 Sports_25_50.json: KRWordRank 실행 중 오류 발생: 'NoneType' object has no attribute 'items'\n",
      "✅ Gaming_1_25.json: KRWordRank 실행 완료 (추출된 키워드 개수: 447)\n",
      "✅ Gaming_1_25_ranking.json 저장 완료!\n",
      "🚨 Sports_50_75.json: KRWordRank 실행 중 오류 발생: 'NoneType' object has no attribute 'items'\n",
      "✅ 모든 구역의 키워드 랭킹 추출이 완료되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from krwordrank.word import KRWordRank\n",
    "\n",
    "# ✅ 데이터 디렉토리 설정\n",
    "input_dir = \"/Users/melon/soynlp_training_data/youtube/scripts/data/categories\"      # 원본 JSON 파일이 있는 디렉토리\n",
    "output_dir = \"data/ranked_keywords\"  # 키워드 랭킹 결과 저장 디렉토리\n",
    "\n",
    "# ✅ 저장할 디렉토리가 없으면 생성\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ✅ KRWordRank 설정\n",
    "min_count = 5   # 최소 등장 횟수\n",
    "max_length = 10 # 최대 단어 길이\n",
    "beta = 0.85     # PageRank 감쇄 계수\n",
    "max_iter = 10     # 반복 횟수\n",
    "\n",
    "wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "\n",
    "# ✅ 디렉토리 내 JSON 파일 하나씩 읽어오기\n",
    "json_files = [f for f in os.listdir(input_dir) if f.endswith(\".json\")]\n",
    "\n",
    "\n",
    "for file_name in json_files:\n",
    "    input_path = os.path.join(input_dir, file_name)\n",
    "    \n",
    "    # ✅ JSON 파일 로드\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = json.load(f)  # 텍스트 로드\n",
    "\n",
    "\n",
    "    # ✅ KRWordRank 실행\n",
    "    try:\n",
    "\n",
    "        keywords, _, _ = wordrank_extractor.extract([text], beta, max_iter)\n",
    "        # ✅ KRWordRank 실행 후 확인\n",
    "        if keywords is None:\n",
    "            print(f\"⚠️ {file_name}: 키워드 추출 실패 (None 반환). 원본 데이터를 저장합니다.\")\n",
    "            with open(f\"data/debug_{file_name}\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(text, f, indent=4, ensure_ascii=False)\n",
    "            continue\n",
    "        print(f\"✅ {file_name}: KRWordRank 실행 완료 (추출된 키워드 개수: {len(keywords)})\")\n",
    "\n",
    "\n",
    "        if not keywords:\n",
    "            print(f\"⚠️ {file_name}: 키워드 추출 실패. 건너뜀.\")\n",
    "            continue\n",
    "\n",
    "        # ✅ 개별 키워드 랭킹 JSON 파일 저장\n",
    "        output_file_name = file_name.replace(\".json\", \"_ranking.json\")  # 파일명 변경\n",
    "        output_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(keywords, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"✅ {output_file_name} 저장 완료!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"🚨 {file_name}: KRWordRank 실행 중 오류 발생: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"✅ 모든 구역의 키워드 랭킹 추출이 완료되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 구역의 키워드 랭킹에서 상위 50개만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Processing: Entertainment_50_75_ranking.json (총 키워드 개수: 18)\n",
      "✅ Entertainment_50_75_top50.json 저장 완료! (상위 50개 키워드 + 가중치 2)\n",
      "🔍 Processing: Entertainment_25_50_ranking.json (총 키워드 개수: 270)\n",
      "✅ Entertainment_25_50_top50.json 저장 완료! (상위 50개 키워드 + 가중치 3)\n",
      "🔍 Processing: News_&_Politics_75_100_ranking.json (총 키워드 개수: 1513)\n",
      "✅ News_&_Politics_75_100_top50.json 저장 완료! (상위 50개 키워드 + 가중치 1)\n",
      "🔍 Processing: News_&_Politics_1_25_ranking.json (총 키워드 개수: 1597)\n",
      "✅ News_&_Politics_1_25_top50.json 저장 완료! (상위 50개 키워드 + 가중치 4)\n",
      "🔍 Processing: Science_&_Technology_1_25_ranking.json (총 키워드 개수: 937)\n",
      "✅ Science_&_Technology_1_25_top50.json 저장 완료! (상위 50개 키워드 + 가중치 4)\n",
      "🔍 Processing: Gaming_1_25_ranking.json (총 키워드 개수: 447)\n",
      "✅ Gaming_1_25_top50.json 저장 완료! (상위 50개 키워드 + 가중치 4)\n",
      "🔍 Processing: Science_&_Technology_25_50_ranking.json (총 키워드 개수: 586)\n",
      "✅ Science_&_Technology_25_50_top50.json 저장 완료! (상위 50개 키워드 + 가중치 3)\n",
      "🔍 Processing: Music_75_100_ranking.json (총 키워드 개수: 229)\n",
      "✅ Music_75_100_top50.json 저장 완료! (상위 50개 키워드 + 가중치 1)\n",
      "🔍 Processing: News_&_Politics_50_75_ranking.json (총 키워드 개수: 1458)\n",
      "✅ News_&_Politics_50_75_top50.json 저장 완료! (상위 50개 키워드 + 가중치 2)\n",
      "🔍 Processing: Film_&_Animation_50_75_ranking.json (총 키워드 개수: 30)\n",
      "✅ Film_&_Animation_50_75_top50.json 저장 완료! (상위 50개 키워드 + 가중치 2)\n",
      "🔍 Processing: News_&_Politics_25_50_ranking.json (총 키워드 개수: 1457)\n",
      "✅ News_&_Politics_25_50_top50.json 저장 완료! (상위 50개 키워드 + 가중치 3)\n",
      "🔍 Processing: Entertainment_75_100_ranking.json (총 키워드 개수: 420)\n",
      "✅ Entertainment_75_100_top50.json 저장 완료! (상위 50개 키워드 + 가중치 1)\n",
      "🔍 Processing: Music_50_75_ranking.json (총 키워드 개수: 35)\n",
      "✅ Music_50_75_top50.json 저장 완료! (상위 50개 키워드 + 가중치 2)\n",
      "🔍 Processing: Music_1_25_ranking.json (총 키워드 개수: 62)\n",
      "✅ Music_1_25_top50.json 저장 완료! (상위 50개 키워드 + 가중치 4)\n",
      "🔍 Processing: Film_&_Animation_75_100_ranking.json (총 키워드 개수: 221)\n",
      "✅ Film_&_Animation_75_100_top50.json 저장 완료! (상위 50개 키워드 + 가중치 1)\n",
      "🔍 Processing: Music_25_50_ranking.json (총 키워드 개수: 196)\n",
      "✅ Music_25_50_top50.json 저장 완료! (상위 50개 키워드 + 가중치 3)\n",
      "✅ 모든 구역의 상위 50개 키워드 추출이 완료되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ✅ 데이터 디렉토리 설정\n",
    "input_dir = \"data/ranked_keywords\"  # 원본 키워드 랭킹 JSON 파일 디렉토리\n",
    "output_dir = \"data/top50_keywords\"  # 상위 50개 키워드 저장 디렉토리\n",
    "\n",
    "# ✅ 저장할 디렉토리가 없으면 생성\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ✅ 구역별 가중치 설정\n",
    "weight_mapping = {\n",
    "    \"1_25\": 4,\n",
    "    \"25_50\": 3,\n",
    "    \"50_75\": 2,\n",
    "    \"75_100\": 1\n",
    "}\n",
    "\n",
    "# ✅ 디렉토리 내 JSON 파일 가져오기\n",
    "json_files = [f for f in os.listdir(input_dir) if f.endswith(\"_ranking.json\")]\n",
    "\n",
    "# ✅ 각 파일별 상위 50개 키워드 추출 및 가중치 추가\n",
    "for file_name in json_files:\n",
    "    input_path = os.path.join(input_dir, file_name)\n",
    "\n",
    "    # ✅ JSON 파일 로드\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        keyword_ranking = json.load(f)  # 키워드 랭킹 데이터 로드 (딕셔너리 형태)\n",
    "\n",
    "    # ✅ 빈 데이터 방지\n",
    "    if not keyword_ranking:\n",
    "        print(f\"⚠️ {file_name}: 데이터가 부족하여 상위 50개 추출을 건너뜀.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"🔍 Processing: {file_name} (총 키워드 개수: {len(keyword_ranking)})\")\n",
    "\n",
    "    # ✅ 상위 50개 키워드만 추출 (점수 기준 정렬 후 키워드만 가져옴)\n",
    "    top_50_keywords = list(sorted(keyword_ranking, key=keyword_ranking.get, reverse=True)[:50])\n",
    "\n",
    "    # ✅ 가중치 적용\n",
    "    # 파일명에서 구역명 추출 (예: \"News_&_Politics_1_25_ranking.json\" → \"1_25\")\n",
    "    for zone in weight_mapping.keys():\n",
    "        if zone in file_name:\n",
    "            weight = weight_mapping[zone]\n",
    "            break\n",
    "    else:\n",
    "        print(f\"⚠️ {file_name}: 알 수 없는 구역, 기본 가중치 1 적용\")\n",
    "        weight = 1\n",
    "\n",
    "    # ✅ {키워드: 가중치} 형태로 변환\n",
    "    weighted_keywords = {keyword: weight for keyword in top_50_keywords}\n",
    "\n",
    "    # ✅ 개별 JSON 파일로 저장\n",
    "    output_file_name = file_name.replace(\"_ranking.json\", \"_top50.json\")  # 파일명 변경\n",
    "    output_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(weighted_keywords, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ {output_file_name} 저장 완료! (상위 50개 키워드 + 가중치 {weight})\")\n",
    "\n",
    "print(\"✅ 모든 구역의 상위 50개 키워드 추출이 완료되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Processing: Entertainment_50_75_top50.json (키워드 개수: 18)\n",
      "🔍 Processing: Music_50_75_top50.json (키워드 개수: 35)\n",
      "🔍 Processing: Entertainment_25_50_top50.json (키워드 개수: 50)\n",
      "🔍 Processing: Music_25_50_top50.json (키워드 개수: 50)\n",
      "🔍 Processing: Science_&_Technology_25_50_top50.json (키워드 개수: 50)\n",
      "🔍 Processing: News_&_Politics_75_100_top50.json (키워드 개수: 50)\n",
      "🔍 Processing: Gaming_1_25_top50.json (키워드 개수: 50)\n",
      "🔍 Processing: Film_&_Animation_50_75_top50.json (키워드 개수: 30)\n",
      "🔍 Processing: Music_1_25_top50.json (키워드 개수: 50)\n",
      "🔍 Processing: News_&_Politics_25_50_top50.json (키워드 개수: 50)\n",
      "🔍 Processing: News_&_Politics_50_75_top50.json (키워드 개수: 50)\n",
      "🔍 Processing: Music_75_100_top50.json (키워드 개수: 50)\n",
      "🔍 Processing: Science_&_Technology_1_25_top50.json (키워드 개수: 50)\n",
      "🔍 Processing: News_&_Politics_1_25_top50.json (키워드 개수: 50)\n",
      "🔍 Processing: Film_&_Animation_75_100_top50.json (키워드 개수: 50)\n",
      "🔍 Processing: Entertainment_75_100_top50.json (키워드 개수: 50)\n",
      "✅ 최종 키워드 랭킹이 'final_keywords_ranking.json' 파일로 저장되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ✅ 데이터 로드\n",
    "ranking_file = \"data/all_keywords_ranking.json\"  # 전체 키워드 랭킹 파일\n",
    "weighted_dir = \"data/top50_keywords\"        # 구역별 가중치 파일들이 저장된 디렉토리\n",
    "output_file = \"final_keywords_ranking.json\"  # 최종 랭킹 결과 저장 파일\n",
    "\n",
    "# ✅ 전체 키워드 랭킹 로드 (기본 점수 포함)\n",
    "with open(ranking_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    base_ranking = dict(json.load(f))  # 리스트를 딕셔너리 형태로 변환\n",
    "\n",
    "# ✅ 디렉토리 내 가중치 파일 가져오기\n",
    "weighted_files = [f for f in os.listdir(weighted_dir) if f.endswith(\"_top50.json\")]\n",
    "\n",
    "# ✅ 키워드별 최종 점수 계산\n",
    "final_ranking = base_ranking.copy()  # 기존 점수를 유지하며 업데이트할 딕셔너리\n",
    "\n",
    "for file_name in weighted_files:\n",
    "    file_path = os.path.join(weighted_dir, file_name)\n",
    "\n",
    "    # ✅ 구역별 가중치 JSON 파일 로드\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        weighted_keywords = json.load(f)  # {키워드: 가중치} 형태의 데이터\n",
    "\n",
    "    print(f\"🔍 Processing: {file_name} (키워드 개수: {len(weighted_keywords)})\")\n",
    "\n",
    "    # ✅ 각 키워드에 가중치를 추가\n",
    "    for keyword, weight in weighted_keywords.items():\n",
    "        if keyword in final_ranking:\n",
    "            final_ranking[keyword] += weight  # 기존 점수 + 가중치\n",
    "        else:\n",
    "            final_ranking[keyword] = weight  # 새로운 키워드면 가중치만 적용\n",
    "\n",
    "# ✅ 최종 키워드 랭킹 정렬\n",
    "final_sorted_ranking = sorted(final_ranking.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# ✅ JSON 파일로 저장\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_sorted_ranking, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ 최종 키워드 랭킹이 '{output_file}' 파일로 저장되었습니다!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
