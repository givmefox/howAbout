{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "def save_to_json(data, filename):\n",
    "    # data 폴더가 존재하지 않으면 생성\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    \n",
    "    # 파일 경로를 data 폴더 아래로 설정\n",
    "    filepath = os.path.join('data', filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    print(f\"데이터 저장: {filepath}\")\n",
    "    \n",
    "    # JSON 데이터 로드 함수\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "def merge_values(data):\n",
    "    \"\"\"\n",
    "    key: value에서 value가 문자열 배열의 배열일 때,\n",
    "    하나의 문자열로 변환하고 빈 문자열(\"\")은 제외하는 함수.\n",
    "\n",
    "    :param data: dict, key: list of lists (e.g., { \"key1\": [[\"문장1\", \"문장2\"], [\"문장3\"]] })\n",
    "    :return: dict, key: merged string (e.g., { \"key1\": \"문장1 문장2 문장3\" })\n",
    "    \"\"\"\n",
    "    merged_data = {}\n",
    "    for key, values in data.items():\n",
    "        if isinstance(values, list):  # 리스트인지 확인\n",
    "            merged_sentence = \" \".join(\n",
    "                s for sublist in values for s in sublist if s.strip()\n",
    "            )  # 빈 문자열 제거 후 공백으로 연결\n",
    "            merged_data[key] = merged_sentence  # key: 병합된 문자열 형태로 저장\n",
    "            \n",
    "            \n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching trending videos for category: News & Politics\n",
      "비디오 44 개 카테고리: News & Politics fetch 완료.\n",
      "데이터 저장: data\\raw_video_data.json\n",
      "데이터 저장 : data/raw_video_data.json'\n",
      "\n",
      "Category: News & Politics\n",
      " - 김어준의 겸손은힘들다 뉴스공장 2025년 2월 11일 화요일 [이재명, 신장식, 박범계, 박시동, 이광수, 패션공장] (merQIjiKU3s), 조회수: 1683911 회, 좋아요: 152676 개\n",
      " - \"그날의 진짜 이야기 담긴 녹취록을!\" \"아니 잠깐, 채택 안 됐잖아요\"...야심차게 꺼냈지만 재판관이 끊은 이유가  / SBS / 바로 이 뉴스 (U8KJo_RKeXs), 조회수: 2878389 회, 좋아요: 29013 개\n",
      " - [오늘 이 뉴스] \"그때 법무실장이 안 말렸으면..\" 재판관 앞 '무시무시한' 증언 (2025.02.06/MBC뉴스) (zYsXdEKSmaA), 조회수: 1905473 회, 좋아요: 44445 개\n",
      " - 대구 찾은 전한길, 헌재 겨냥 “제2의 을사오적” / 채널A / 뉴스 TOP10 (Oqv0A6dyxw0), 조회수: 651235 회, 좋아요: 33556 개\n",
      " - 범여권 지지율 1위 김문수 장관... 이재명 대표연설 듣고 일침 작렬 (7Clx1bGNyX0), 조회수: 521968 회, 좋아요: 27753 개\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "import isodate\n",
    "\n",
    "\n",
    "# 환경 변수에서 API 키 가져오기\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API 키가 설정되지 않았습니다. 환경 변수 'YOUTUBE_API_KEY'를 설정하세요.\")\n",
    "\n",
    "# YouTube API 설정\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# 카테고리 ID 설정\n",
    "CATEGORIES = {\n",
    "    \"News & Politics\": \"25\"\n",
    "}\n",
    "\n",
    "# 동영상 데이터 가져오기\n",
    "def fetch_trending_videos(category_id, region_code=\"KR\", max_results=200):\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(videos) < max_results:\n",
    "        try:\n",
    "            request = youtube.videos().list(\n",
    "                part=\"snippet,statistics,contentDetails\",\n",
    "                chart=\"mostPopular\",\n",
    "                regionCode=region_code,\n",
    "                videoCategoryId=category_id,\n",
    "                maxResults=min(50, max_results - len(videos)),\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                duration = isodate.parse_duration(item[\"contentDetails\"][\"duration\"])\n",
    "                duration_in_seconds = duration.total_seconds()  #초로 바꾸기기\n",
    "\n",
    "                if duration_in_seconds > 80:  # 80초 이상의 동영상만 가져오기\n",
    "                    videos.append({\n",
    "                        \"video_id\": item[\"id\"],\n",
    "                        \"title\": item[\"snippet\"][\"title\"],\n",
    "                        \"description\": item[\"snippet\"][\"description\"],\n",
    "                        \"tags\": item[\"snippet\"].get(\"tags\", []),\n",
    "                        \"duration\": str(duration),\n",
    "                        \"view_count\": int(item[\"statistics\"].get(\"viewCount\", 0)),\n",
    "                        \"like_count\": int(item[\"statistics\"].get(\"likeCount\", 0)),\n",
    "                        \"comment_count\": int(item[\"statistics\"].get(\"commentCount\", 0)),\n",
    "                        \"category_id\": category_id,\n",
    "                    })\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching videos: {e}\")\n",
    "            time.sleep(5)  # 잠시 대기 후 다시 시도\n",
    "\n",
    "    return videos\n",
    "\n",
    "\n",
    "\n",
    "# 실행\n",
    "all_videos = {}\n",
    "\n",
    "for category_name, category_id in CATEGORIES.items():\n",
    "    print(f\"Fetching trending videos for category: {category_name}\")\n",
    "    videos = fetch_trending_videos(category_id, region_code=\"KR\", max_results=200)\n",
    "    all_videos[category_name] = videos\n",
    "    print(f\"비디오 {len(videos)} 개 카테고리: {category_name} fetch 완료.\")\n",
    "\n",
    "# 결과를 하나의 JSON 파일로 저장\n",
    "output_file = \"raw_video_data.json\"\n",
    "save_to_json(all_videos, output_file)\n",
    "print(f\"데이터 저장 : data/{output_file}'\")\n",
    "\n",
    "# 결과 출력 예시\n",
    "for category, videos in all_videos.items():\n",
    "    print(f\"\\nCategory: {category}\")\n",
    "    for video in videos[:5]:\n",
    "        print(f\" - {video['title']} ({video['video_id']}), 조회수: {video['view_count']} 회, 좋아요: {video['like_count']} 개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "\n",
    "# 비디오 댓글 가져오기 함수\n",
    "def get_video_comments(youtube, video_id, max_results=100):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < max_results:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=min(50, max_results - len(comments)),\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                comments.append(comment)\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching comments for video {video_id}: {e}\")\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "# JSON 데이터 저장 함수\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 파일 경로 설정\n",
    "input_file_path = 'data/raw_video_data.json'\n",
    "output_file_path = 'video_comments.json'\n",
    "\n",
    "# JSON 데이터 로드\n",
    "data = load_json(input_file_path)\n",
    "\n",
    "\n",
    "# 비디오 댓글 가져오기 및 저장\n",
    "all_comments = {}\n",
    "for category, videos in data.items():\n",
    "    for video in videos:\n",
    "        video_id = video[\"video_id\"]\n",
    "        comments = get_video_comments(youtube, video_id, max_results=1000)\n",
    "        all_comments[video_id] = comments\n",
    "\n",
    "# 결과 저장\n",
    "save_to_json(all_comments, output_file_path)\n",
    "print(f\"비디오 댓글이 '{output_file_path}'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 문장 수: 30147\n",
      "데이터 저장: data\\cleaned_video_comments.json\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pykospacing import Spacing\n",
    "\n",
    "def clean_text(text):   \n",
    "    # text = re.sub(r'\\s+', '', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\S+|#\\S+', '', text)\n",
    "    text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", text)\n",
    "    # text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r\"[a-zA-Z]\", \"\", text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "data = load_json('data/video_comments.json')\n",
    "\n",
    "processed_data = {}\n",
    "\n",
    "\n",
    "for key, values in data.items():\n",
    "    if isinstance(values, list):  # 값이 리스트인지 확인\n",
    "        processed_data[key] = [clean_text(value) for value in values]\n",
    "        \n",
    "# 총 문장 수 계산\n",
    "total_sentences = sum(len(value) for value in processed_data.values())\n",
    "print(f\"총 문장 수: {total_sentences}\")\n",
    "        \n",
    "save_to_json(processed_data, 'cleaned_video_comments.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토크나이저 생성 Soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.557 Gbory 0.549 Gb\n",
      "all cohesion probabilities was computed. # words = 27413\n",
      "all branching entropies was computed # words = 37145\n",
      "all accessor variety was computed # words = 37145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Scores(cohesion_forward=np.float64(0.3958368792415143), cohesion_backward=np.float64(0.22787631744074252), left_branching_entropy=3.6319164965071917, right_branching_entropy=3.3525386724289588, left_accessor_variety=115, right_accessor_variety=103, leftside_frequency=1502, rightside_frequency=66)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp.word import WordExtractor\n",
    "\n",
    "# Load the cleaned video comments\n",
    "data = load_json('data/cleaned_video_comments.json')\n",
    "\n",
    "# 모든 댓글을 하나의 텍스트로 합치기\n",
    "corpus = []\n",
    "for key, values in data.items():\n",
    "    if isinstance(values, list):  # Check if the value is a list\n",
    "        corpus.extend(values)\n",
    "\n",
    "# DoublespaceLineCorpus 객체 생성\n",
    "corpus[5]\n",
    "\n",
    "# WordExtractor 객체 생성 및 학습\n",
    "word_extractor = WordExtractor()\n",
    "word_extractor.train(corpus)\n",
    "word_scores = word_extractor.extract()\n",
    "\n",
    "word_scores[\"이재명\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 124921 from 30147 sents. mem=0.580 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=311011, mem=0.764 Gb\n",
      "[Noun Extractor] batch prediction was completed for 41747 words\n",
      "[Noun Extractor] checked compounds. discovered 22581 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30190 -> 23169\n",
      "[Noun Extractor] postprocessing ignore_features : 23169 -> 22967\n",
      "[Noun Extractor] postprocessing ignore_NJ : 22967 -> 22535\n",
      "[Noun Extractor] 22535 nouns (22581 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.820 Gb                    \n",
      "[Noun Extractor] 66.60 % eojeols are covered\n",
      "그날 대한민국을 구한 시민들에게 경의를 표합니다 감사드립니다\n",
      "이새끼는 희기종이네\n",
      "한국군 제1군인곽종근\n",
      "전한길 선생님은 애국자이십니다 감사합니다\n",
      "남한내 중국인 전원미국처럼 추방조치 해야 한다 중국인 입국 불허조치 해야 한다\n",
      "미국이 산업기반을 동맹국과는 다르게 해왔는데 그것을 모두 독식하기 위해서 동맹을 쥐어짜는 것은 안된다 세계가 서로 연결되어 주고받는 관계에서 동맹에 더 혜택을 주는 것이 당연하고 그 영향력을 세계에 알게하여 힘을 확장하는 것이다 자유민주주의 동맹이 세계를 이끌고 나가는데 힘을 합치는 것을 달리 보는 것은 어리석다 국익에는 여러가지 영역이 있으므로 손익계산을 잘 해야 맞다\n",
      "용혜인 제발 티비 못나오게 치워주세요 거것들 보고나면 종일재수가 없다 자질없는것들 버럭 소리지를줄만 알고 머리에 똥만 들었나\n",
      "제발 우덜식 멈춰라\n",
      "국힘 자갈뱃고 재 수개표 하자 하라낙선된 인간들 자갈 뱃고 말하라\n",
      "박지원의원님 내란동조자들과의 포용은 절대 하면 안되죠\n",
      "잔망스러운 놈들 뉴스타파 끌어내라이 빨갱이들을 어짜노\n",
      "김종대씨의 분석지지합니다\n",
      "다필요없고 미국 기축통화 국가에서 비트코인 제제하고 쓸모없다는식으로 나오면 그냥 휴지조각진짜 디지털 금이될수도정보쪼가리가 될수도 있음\n",
      "우두머리이런 개같은 표현을 하냐이런 쓰레기 같은\n",
      "헌법을 배우게된 효과라 뭐 맞는말이기도 한데 그 결과가 불법계엄을 그럴수도 있다고 말하는게 효과인가여 그게 40프로가 넘는다는데 정상인가요 이게 논리적으로 맞아요\n",
      "속이후련합니다문죄인 정말진상이지요\n",
      "우린 끝까지 계속 갈겁니다 묵묵히 계속 가겠습니다\n",
      "이죄명부부문죄인부부구속하라정신차려\n",
      "트럼프가 날뛸 수록 시진핑이 좋아하는 이유\n",
      "2심유죄나오면 나오지마라\n",
      "어떻게해야 저렇게 주차를할까 머리속에 뭐가들어있는지 참 대단하네\n",
      "결국 또 쥐 새 끼 네\n",
      "봤다고 해도 죄는 안되는데 조치를 할려고 회의를 두번이나 진행했다가 계엄이 실패해서 안보았다고 모른다고 한것임\n",
      "좌파 선전선동에 빠져 있던 주위 사람들이 하나둘 조금씩 변한 모습들이 보여집니다\n",
      "김계리 이인간 말만하면 재수없어 개짜증\n",
      "무시하세요\n",
      "곽종근은시받은대로 표현한거다 연석열 김용현씨지시감춰주지마라 지시 덮을려하지마라 곽종근사실을 표현한거다\n",
      "민주당 짜놓고온 문장으로 하네\n",
      "홍 많이갔어 요단강 건너간거지 어떻게 다시 돌아올꺼야\n",
      "안녕하세요 반갑습니다 늘 고맙습니다\n",
      "오세훈은 아닙니다 착각마세요윤대통령님 뿐\n",
      "드럼프가 딥스들 청소한다니깐 맞대결 하겟지요그러려니 하지요싸울준비항상해야\n",
      "거니와 선라이즈사건 및 용산 대통령실 연관가능성 농후반드시 밝혀야 합니다\n",
      "어째 이런일이아름다운 사랑인데너무 안타까워 가슴이 저리네요고인의명복을빕니다구준엽씨 잘이겨내 주세요\n",
      "\n",
      "중학교 선생 천박하네\n",
      "중국말 듣기 싫다 병의 근원 너무 싫다\n",
      "김형두판사는 탄핵쪽일 수 있다저들은 살아남기위해 대통령을 탄핵시켜야만 하고 그러기 위해 말도안되는 재판을 진행할 것이다선관위의 부정선거가 쟁점인데 선관위 출신판사가 말이 되나전주시 선거관리위원장 출신 문형배 판사공주시 선거관리위원장 출신 정정미 판사제주시 선거관리위원장 출신 조한창 판사강릉시 선거관리위원장 출신 김형두 판사선거관리위원회의 부정부패로 인한 사건을 판결하는데 사건과 직간접적으로 연관이 있는선거관리위원장 출신 판사가 공정한 판결을 할것이라고 믿을 수 있겠는가결국 우리법출신과 선거관리위원장 출신이 합하면 62로 탄핵된다는 결론\n",
      "진짜냐\n",
      "장성철 장윤선 나오면 무조건 패스\n",
      "파나마 대통령 와 내인생 되기 직전임\n",
      "안녕하시렵니까\n",
      "전과2범부터는 인권 줄필요없음\n",
      "국짐34라는게 놀랍네 하긴 나라 팔아먹어도 국짐이라는 동네도 있으니 그쪽은 방송이 조선민 되나 식당가면 전부 조선 야 이제 그쪽은 보지도 않지만\n",
      "데이터 저장: data\\tokenized_video_comments.json\n"
     ]
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "\n",
    "cohesion_score = {word:score.cohesion_forward for word, score in word_scores.items()}\n",
    "\n",
    "tokenizer = MaxScoreTokenizer(scores=cohesion_score)\n",
    "\n",
    "\n",
    "noun_extractor = LRNounExtractor_v2()\n",
    "nouns = noun_extractor.train_extract(corpus) # list of str like\n",
    "\n",
    "noun_scores = {noun:score.score for noun, score in nouns.items()}\n",
    "combined_scores = {noun:score + cohesion_score.get(noun, 0)\n",
    "    for noun, score in noun_scores.items()}\n",
    "combined_scores.update(\n",
    "    {subword:cohesion for subword, cohesion in cohesion_score.items()\n",
    "    if not (subword in combined_scores)}\n",
    ")\n",
    "\n",
    "\n",
    "# Load the cleaned video comments\n",
    "data = load_json('data/cleaned_video_comments.json')\n",
    "tokenizer = LTokenizer(scores=combined_scores)\n",
    "\n",
    "# Tokenize each comment\n",
    "tokenized_comments = {}\n",
    "for key, values in data.items():\n",
    "    print(values[1])\n",
    "    if isinstance(values, list):  # Check if the value is a list\n",
    "        tokenized_comments[key] = [tokenizer.tokenize(value) for value in values]\n",
    "\n",
    "# Save the tokenized comments back to JSON\n",
    "save_to_json(tokenized_comments, 'tokenized_video_comments.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 124921 from 30147 sents. mem=0.753 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=311011, mem=0.780 Gb\n",
      "[Noun Extractor] batch prediction was completed for 41747 words\n",
      "[Noun Extractor] checked compounds. discovered 22581 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 30190 -> 23169\n",
      "[Noun Extractor] postprocessing ignore_features : 23169 -> 22967\n",
      "[Noun Extractor] postprocessing ignore_NJ : 22967 -> 22535\n",
      "[Noun Extractor] 22535 nouns (22581 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.830 Gb                    \n",
      "[Noun Extractor] 66.60 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "from soynlp.noun import LRNounExtractor_v2\n",
    "noun_extractor = LRNounExtractor_v2()\n",
    "nouns = noun_extractor.train_extract(corpus) # list of str like\n",
    "\n",
    "noun_scores = {noun:score.score for noun, score in nouns.items()}\n",
    "combined_scores = {noun:score + cohesion_score.get(noun, 0)\n",
    "    for noun, score in noun_scores.items()}\n",
    "combined_scores.update(\n",
    "    {subword:cohesion for subword, cohesion in cohesion_score.items()\n",
    "    if not (subword in combined_scores)}\n",
    ")\n",
    "\n",
    "tokenizer = LTokenizer(scores=combined_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 저장: data\\okt_tokenized_video_comments.json\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "data = load_json('data/cleaned_video_comments.json')\n",
    "\n",
    "# Tokenize each comment using Okt\n",
    "tokenized_comments_okt = {}\n",
    "for key, values in data.items():\n",
    "    if isinstance(values, list):  # Check if the value is a list\n",
    "        tokenized_comments_okt[key] = [okt.morphs(value) for value in values]\n",
    "\n",
    "# Save the tokenized comments back to JSON\n",
    "save_to_json(tokenized_comments_okt, 'okt_tokenized_video_comments.json')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 저장: data\\okt_nouns_video_comments.json\n"
     ]
    }
   ],
   "source": [
    "# Extract nouns from each comment using Okt\n",
    "nouns_comments_okt = {}\n",
    "for key, values in data.items():\n",
    "    if isinstance(values, list):  # Check if the value is a list\n",
    "        nouns_comments_okt[key] = [okt.nouns(value) for value in values]\n",
    "\n",
    "# Save the nouns extracted comments back to JSON\n",
    "save_to_json(nouns_comments_okt, 'okt_nouns_video_comments.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kiwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('부정선거', 0.7329921722412109, 713, -2.150365114212036),\n",
       " ('곽종근', 0.6221180558204651, 571, -2.5893051624298096),\n",
       " ('내란수괴', 0.41759631037712097, 161, -2.3843839168548584),\n",
       " ('문형배', 0.40476036071777344, 578, -2.8553996086120605),\n",
       " ('윤석열대통령', 0.40355342626571655, 139, 0.11226491630077362),\n",
       " ('우리법연구회', 0.40248537063598633, 36, -2.949073553085327),\n",
       " ('사전투표', 0.40150853991508484, 91, -1.8540213108062744),\n",
       " ('계엄선포', 0.37189987301826477, 38, -0.5901609063148499),\n",
       " ('석열', 0.36449238657951355, 1481, -2.452401876449585),\n",
       " ('윤대통령', 0.3614285886287689, 469, 0.24186985194683075),\n",
       " ('그라운드씨', 0.34617775678634644, 40, -1.5572603940963745),\n",
       " ('렉스턴', 0.34515896439552307, 25, -2.0683038234710693),\n",
       " ('사법리스크', 0.34044161438941956, 13, -1.316566824913025),\n",
       " ('대통령탄핵', 0.333201140165329, 28, -1.916110873222351),\n",
       " ('더불어공산당', 0.33175933361053467, 21, -2.059748649597168),\n",
       " ('국민들', 0.31844037771224976, 1041, -2.1782732009887695),\n",
       " ('구준엽씨', 0.3088982105255127, 51, -2.8865554332733154),\n",
       " ('홍장원', 0.3052136301994324, 368, -2.796747922897339),\n",
       " ('자유대한민국', 0.30401360988616943, 82, -2.790320634841919),\n",
       " ('내란우두머리', 0.29165536165237427, 27, -2.5659449100494385),\n",
       " ('다대한민국', 0.27813470363616943, 32, -2.6729960441589355),\n",
       " ('다윤석열대통령', 0.2756351828575134, 15, -0.40993720293045044),\n",
       " ('반국가세력', 0.26237890124320984, 77, -1.8978217840194702)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "\n",
    "# Kiwi 객체 생성\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# Load the cleaned video comments\n",
    "data = load_json('data/cleaned_video_comments.json')\n",
    "\n",
    "corpus = []\n",
    "for key, values in data.items():\n",
    "    if isinstance(values, list):  # Check if the value is a list\n",
    "        corpus.extend(values)\n",
    "        \n",
    "\n",
    "kiwi.extract_add_words(corpus)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1630096044.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[15], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    ] if if values values else else [] []\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ✅ Kiwi를 이용한 토큰화 & 명사 추출\n",
    "tokenized_nouns = {}\n",
    "\n",
    "\n",
    "for key, values in data.items():\n",
    "    if isinstance(values, list):  # 리스트인지 확인\n",
    "        tokenized_nouns[key] = [\n",
    "            [token.form for token in kiwi.tokenize(value) if token.tag in [\"NNG\", \"NNP\"] and len(token.form) > 1]  # 명사만 추출, 1글자 제외\n",
    "            for value in values if value  # 빈 문자열 제외\n",
    "        ]\n",
    "\n",
    "# ✅ 결과 JSON 저장\n",
    "save_to_json(tokenized_nouns, 'kiwi_nouns_video_comments.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KRWordRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 저장: data\\kiwi_data.json\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned video comments\n",
    "data = load_json('data/kiwi_nouns_video_comments.json')\n",
    "\n",
    "    \n",
    "processed_data = merge_values(data)\n",
    "\n",
    "save_to_json(processed_data,'kiwi_data.json')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.word import KRWordRank\n",
    "\n",
    "min_count = 10   # 단어의 최소 출현 빈도수 (그래프 생성 시)\n",
    "max_length = 10 # 단어의 최대 길이\n",
    "wordrank_extractor = KRWordRank(min_count=min_count, max_length=max_length)\n",
    "\n",
    "data = load_json('data/kiwi_data.json')\n",
    "\n",
    "documents = list(data.values())  # 각 영상의 댓글을 리스트로 저장\n",
    "\n",
    "beta = 0.85    # PageRank의 감쇄 계수\n",
    "max_iter = 10  # 반복 횟수\n",
    "\n",
    "keywords, rank, graph = wordrank_extractor.extract(documents, beta, max_iter)\n",
    "\n",
    "top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords, rank, graph = wordrank_extractor.extract(documents, beta, max_iter)\n",
    "\n",
    "top_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('국민', 23.804733480548844),\n",
       " ('대통령', 23.177865811936364),\n",
       " ('나라', 17.927918897117337),\n",
       " ('사람', 17.014688347002217),\n",
       " ('이재명', 14.159581424001649),\n",
       " ('민주당', 13.578475480755616),\n",
       " ('탄핵', 11.904426635750083),\n",
       " ('윤석열', 11.296851125747969),\n",
       " ('중국', 11.097538875608626),\n",
       " ('내란', 10.85458190281492)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned video comments\n",
    "data = load_json('data/merged_kiwi_video_comments.json')\n",
    "\n",
    "    \n",
    "processed_data = {}\n",
    "for key, values in data.items():\n",
    "    if isinstance(values, list):  # 값이 리스트인지 확인\n",
    "        merged_sentence = \" \".join([s for s in values if s.strip()])  # 빈 문자열 제거 후 공백으로 연결        \n",
    "        processed_data[key] = merged_sentence  # key: value 형태로 저장\n",
    "\n",
    "save_to_json(processed_data,'kiwi_data.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
